- en: 'Chapter 1. XEN: A HIGH-LEVEL OVERVIEW'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。XEN：高级概述
- en: '![image with no caption](httpatomoreillycomsourcenostarchimages333191.png.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![无标题图片](httpatomoreillycomsourcenostarchimages333191.png.jpg)'
- en: We'll start by explaining what makes Xen different from other virtualization
    techniques and then provide some low-level detail on how Xen works and how its
    components fit together.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先解释Xen与其他虚拟化技术不同的地方，然后提供一些关于Xen如何工作以及其组件如何组合的底层细节。
- en: Virtualization Principles
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化原理
- en: First, we might want to mention that computers, even new and fast ones with
    modern multitasking operating systems, can perform only one instruction at a time.^([[8](#ftn.CHP-1-FNOTE-1)])
    Now, you say, "But my computer is performing many tasks at once. Even now, I can
    see a clock running, hear music playing, download files, and chat with friends,
    all at the same time." And this is true. However, what is *actually* happening
    is that the computer is switching between these different tasks so quickly that
    the delays become imperceptible. Just as a movie is a succession of still images
    that give the illusion of movement, a computer performs tasks that are so seamlessly
    interweaved as to appear simultaneous.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可能需要提到，即使是配备现代多任务操作系统的最新、最快的计算机，一次也只能执行一条指令.^([[8](#ftn.CHP-1-FNOTE-1)])
    现在，你说，“但是我的计算机正在同时执行许多任务。甚至现在，我可以看到时钟在运行，听到音乐在播放，下载文件，和朋友们聊天，所有这些都在同一时间进行。”这是真的。然而，实际上发生的情况是，计算机在这些不同任务之间切换得如此之快，以至于延迟变得难以察觉。就像电影是由一系列静态图像组成，给人以运动错觉一样，计算机执行的任务交织得如此紧密，以至于看起来是同时进行的。
- en: Virtualization just extends this metaphor a bit. Ordinarily, this multiplexing
    takes place under the direction of the operating system, which acts to supervise
    tasks and make sure that each receives its fair share of CPU time. Because the
    operating system must therefore *schedule* tasks to run on the CPU, this aspect
    of the operating system is called a *scheduler*. With Xen virtualization, the
    same process occurs, with entire operating systems taking the place of tasks.
    The scheduling aspect is handled by the Xen kernel, which runs on a level superior
    to the "supervising" guest operating systems, and which we thus call the *hypervisor*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化只是将这个比喻扩展了一点点。通常，这种多路复用是在操作系统的指导下进行的，操作系统的作用是监督任务并确保每个任务都能获得其公平的CPU时间。因为操作系统必须因此对任务进行*调度*以在CPU上运行，所以操作系统的这个方面被称为*调度器*。在Xen虚拟化中，发生的过程相同，整个操作系统取代了任务的位置。调度方面由Xen内核处理，该内核在“监督”客户操作系统之上运行，因此我们称之为*管理程序*。
- en: Of course, it's not quite so simple—operating systems, even ones that have been
    modified to be Xen-friendly, use a different, more comprehensive, set of assumptions
    than applications, and switching between them is almost by definition going to
    involve more complexity.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，事情并不那么简单——操作系统，即使是经过修改以适应Xen的操作系统，也比应用程序使用更不同、更全面的假设集，并且在这些假设之间切换几乎必然涉及更多的复杂性。
- en: So let's look at an overview of how virtualization is traditionally done and
    how Xen's design is new and different. A traditional virtual machine is designed
    to mimic a real machine in every way, such that it's impossible to tell from within
    the virtual machine that it isn't real. To preserve that illusion, fully virtualized
    machines intercept attempts to access hardware and emulate that hardware's functionality
    in software—thus maintaining perfect compatibility with the applications inside
    the virtual machine. This layer of indirection makes the virtual machine very
    slow.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看传统上是如何进行虚拟化的，以及Xen的设计是如何新颖和不同的。传统的虚拟机旨在以任何方式模仿真实机器，以至于在虚拟机内部无法判断它不是真实的。为了保持这种错觉，完全虚拟化的机器会拦截尝试访问硬件的尝试，并在软件中模拟该硬件的功能——从而与虚拟机内的应用程序保持完美的兼容性。这一层间接性使得虚拟机非常慢。
- en: Xen bypasses this slowdown using an approach called *paravirtualization*—*para*
    as a prefix means *similar to* or *alongside*. As the name suggests, it's not
    "real" virtualization in the traditional sense because it doesn't try to provide
    a seamless illusion of a machine. Xen presents only a partial abstraction of the
    underlying hardware to the hosted operating system, exposing some aspects of the
    machine as *limitations* on the guest OS, which needs to know that it's running
    on Xen and should handle certain hardware interactions accordingly.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Xen通过一种称为*Para虚拟化*的方法绕过这种减速——*Para*作为前缀意味着*类似*或*并行*。正如其名所示，它不是传统意义上的“真实”虚拟化，因为它不试图提供一个无缝的机器幻觉。Xen仅向托管操作系统呈现底层硬件的部分抽象，将机器的一些方面作为*限制*暴露给客户操作系统，客户操作系统需要知道它在Xen上运行，并相应地处理某些硬件交互。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Newer processors incorporate support for hardware virtualization, allowing
    unmodified operating systems to run under Xen. See [Chapter 12](ch12.html "Chapter 12. HVM:
    BEYOND PARAVIRTUALIZATION") for details*.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*较新的处理器集成了对硬件虚拟化的支持，允许未经修改的操作系统在Xen下运行。有关详细信息，请参阅[第12章](ch12.html "第12章。HVM：超越Para虚拟化")*。'
- en: Most of these limitations—by design—aren't noticeable to the system's users.
    To run under Xen, the guest OS kernel needs to be modified so that, for example,
    it asks Xen for memory rather than allocating it directly. One of the design goals
    for Xen was to have these changes occur in the hardware-dependent bits of the
    guest operating system, without changing the interface between the kernel and
    user-level software.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制中的大多数——出于设计考虑——对系统用户来说并不明显。为了在Xen下运行，客户操作系统内核需要修改，以便例如，它向Xen请求内存而不是直接分配。Xen的设计目标之一是在客户操作系统的硬件相关部分进行这些更改，而不改变内核与用户级软件之间的接口。
- en: This design goal reduces the difficulty of moving to Xen by ensuring that existing
    binaries will work unmodified on the Xen guest OS and that the virtual machine
    will, in most regards, act exactly like a real one, at least from the perspective
    of the system's end users.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计目标通过确保现有的二进制文件在Xen客户操作系统上无需修改即可运行，并且虚拟机在大多数方面将表现得与真实机器完全一样，至少从系统最终用户的角度来看是这样。
- en: Xen therefore trades seamless virtualization for a high-performance paravirtualized
    environment. The paper in which the original Xen developers initially presented
    this project, "Xen and the Art of Virtualization,"^([[9](#ftn.CHP-1-FNOTE-2)])
    puts this in strong terms, saying "Paravirtualization is necessary to attain high
    performance and strong resource isolation on uncooperative machine architectures
    such as x86." It's not quite as simple as "paravirtualization makes a computer
    fast"—I/O, for example, can lead to expensive context switches—but it is generally
    faster than other approaches. We generally assume that a Xen guest will run at
    about 95 percent of its native speed on physical hardware, assuming that other
    guests on the machine are idle.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Xen以高性能的Para虚拟化环境为代价，换取了无缝虚拟化。原始Xen开发者最初提出这个项目的论文，“Xen与虚拟化艺术”，^([[9](#ftn.CHP-1-FNOTE-2)])
    强调了这一点，说“Para虚拟化对于在x86等不合作的机器架构上实现高性能和强资源隔离是必要的。”这并不像“Para虚拟化使计算机变快”那么简单——例如，I/O可能会导致昂贵的上下文切换，但它通常比其他方法快。我们通常假设Xen客户在物理硬件上的运行速度大约是其原生速度的95%，假设机器上的其他客户空闲。
- en: 'However, paravirtualization isn''t the only way to run a virtual machine. There
    are two competing approaches: full virtualization and OS-level virtualization.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虚拟化并非仅通过Para虚拟化一种方式来运行虚拟机。存在两种竞争性的方法：全虚拟化和操作系统级别的虚拟化。
- en: '* * *'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[8](#CHP-1-FNOTE-1)]) Of course, SMP and multicore CPUs make this not entirely
    true, and we are drastically simplifying pipelining, superscalar execution, and
    so forth, but the principle still holds—at any instant, each core is only doing
    one thing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[8](#CHP-1-FNOTE-1)]) 当然，SMP和多核CPU使得这一点并不完全成立，我们在流水线、超标量执行等方面做了极大的简化，但原则仍然适用——在任何时刻，每个核心只做一件事。
- en: ^([[9](#CHP-1-FNOTE-2)]) See [http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf](http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[9](#CHP-1-FNOTE-2)]) 请参阅[http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf](http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf)。
- en: 'Virtualization Techniques: Full Virtualization'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化技术：全虚拟化
- en: Not all virtualization methods use Xen's approach. Virtualization software come
    in three flavors. At one extreme you have *full virtualization*, or emulation,
    in which the virtual machine is a software simulation of hardware, real or fictional—as
    long as there's a driver, it doesn't matter much. Products in this category include
    VMware and QEMU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有虚拟化方法都使用Xen的方法。虚拟化软件有三种类型。在一种极端情况下，你有*完全虚拟化*，或仿真，其中虚拟机是硬件的软件模拟，无论是真实的还是虚构的——只要存在驱动程序，这并不重要。这类产品包括VMware和QEMU。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*And what, you ask, is this fictional hardware? Apart from the obvious "not
    real" answer, one good example is the VTPM driver. TPM (Trusted Platform Module)
    hardware is relatively uncommon, but it has some potential applications with signing
    code—for example, making sure that the running kernel is the correct one, rather
    than a fake put on by a rootkit or virus. Xen therefore makes a virtual TPM available
    to the domUs*.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可能会问，这种虚构的硬件是什么？除了明显的“不真实”答案之外，一个很好的例子是VTPM驱动程序。TPM（可信平台模块）硬件相对不常见，但它有一些潜在的代码签名应用——例如，确保正在运行的内核是正确的，而不是由rootkit或病毒放置的假内核。因此，Xen为domUs提供了一个虚拟TPM*。'
- en: With full virtualization, an unmodified^([[10](#ftn.CHP-1-FNOTE-3)]) OS "hosts"
    a userspace program that emulates a machine on which the "guest" OS runs. This
    is a popular approach because it doesn't require the guest OS to be changed in
    any way. It also has the advantage that the virtualized architecture can be completely
    different from the host architecture—for example, QEMU can simulate a MIPS processor
    on an IA-32 host and a startling array of other chips.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全虚拟化的情况下，未经修改的^([[10](#ftn.CHP-1-FNOTE-3)]) 操作系统“托管”一个用户空间程序，该程序模拟一个“客户”操作系统运行的机器。这是一种流行的方法，因为它不需要以任何方式修改客户操作系统。它还有这样的优势，即虚拟化架构可以与主机架构完全不同——例如，QEMU可以在IA-32主机上模拟MIPS处理器以及一系列其他芯片。
- en: However, this level of hardware independence comes at the cost of an enormous
    speed penalty. Unaccelerated QEMU is an order of magnitude slower than native
    execution, and accelerated QEMU or VMware ESX server can only accelerate the emulated
    machine if it's the same architecture as the underlying hardware. In this context,
    for normal usage, the increased hardware versatility of a full emulator isn't
    a significant advantage over Xen.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种程度的硬件独立性是以巨大的速度惩罚为代价的。未经加速的QEMU比本地执行慢一个数量级，而加速的QEMU或VMware ESX服务器只能在模拟机器与底层硬件相同架构的情况下加速。在这种情况下，对于正常使用来说，全仿真器的硬件多功能性增加并不比Xen有显著优势。
- en: VMware is currently the best-known vendor of full-virtualization products, with
    a robust set of tools, broad support, and a strong brand. Recent versions of VMware
    address the speed problem by running instructions in place where possible and
    dynamically translating code when necessary. Although this approach is elegant
    and doesn't require guest OS modification, it's not as fast as Xen, making it
    less desirable for production setups or for a full-time work environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: VMware是目前最知名的完全虚拟化产品供应商，拥有强大的工具集、广泛的支持和强大的品牌。VMware的最近版本通过尽可能在原地运行指令并在必要时动态转换代码来解决速度问题。尽管这种方法很优雅，并且不需要修改客户操作系统，但它不如Xen快，因此在生产环境或全职工作环境中不太受欢迎。
- en: '* * *'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[10](#CHP-1-FNOTE-3)]) Or a slightly modified OS—QEMU, for example, has the
    KQEMU kernel module, which speeds up the emulated code by allowing it to run directly
    on the processor when possible.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[10](#CHP-1-FNOTE-3)]) 或者稍微修改过的操作系统——例如，QEMU具有KQEMU内核模块，它通过允许在可能的情况下直接在处理器上运行来加速模拟代码。
- en: 'Virtualization Techniques: OS Virtualization'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化技术：操作系统虚拟化
- en: On the other extreme is *OS-level virtualization*, where what's being virtualized
    is the operating environment, rather than the complete machine. FreeBSD jails
    and Solaris Containers take this approach.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种极端情况下是*操作系统级别的虚拟化*，其中被虚拟化的是操作系统环境，而不是完整的机器。FreeBSD监狱和Solaris容器采用这种方法。
- en: OS virtualization takes the position that the operating system already provides,
    or at least can be made to provide, sufficient isolation to do everything that
    a normal VM user expects—install software systemwide, upgrade system libraries
    in the guest without affecting those in the host, and so forth. Thus, rather than
    emulating physical hardware, OS virtualization emulates a complete OS userspace
    using operating system facilities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统虚拟化认为操作系统已经提供，或者至少可以被配置为提供足够的隔离来执行正常虚拟机用户期望的一切——在全局范围内安装软件、在客户机中升级系统库而不影响主机中的库，等等。因此，操作系统虚拟化不是模拟物理硬件，而是使用操作系统功能模拟一个完整的操作系统用户空间。
- en: FreeBSD jails and Solaris Containers (or Zones) are two popular implementations
    of OS-level virtualization. Both derive from the classic Unix `chroot` jail. The
    idea is that the jailed process can only access parts of the filesystem that reside
    under a certain directory—the rest of the filesystem, as far as this process can
    tell, simply doesn't exist. If we install an OS into that directory, it can be
    considered a complete virtual environment. Jails and Zones expand on the concept
    by also restricting certain system calls and providing a virtual network interface
    to enhance isolation between virtual machines. Although this is incredibly useful,
    it's neither as useful or as versatile as a full-fledged virtual machine would
    be. Because the jails share a kernel, for example, a kernel panic will bring down
    all the VMs on the hardware.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: FreeBSD监狱和Solaris容器（或区域）是两种流行的操作系统级别虚拟化实现。两者都源自经典的Unix `chroot`监狱。其想法是，被监禁的进程只能访问位于某个目录下的文件系统的一部分——对于这个进程来说，文件系统的其余部分，简单地讲，是不存在的。如果我们把操作系统安装到那个目录中，它就可以被认为是一个完整的虚拟环境。监狱和区域通过限制某些系统调用并提供虚拟网络接口来扩展这个概念，以增强虚拟机之间的隔离。尽管这非常有用，但它既不如完整的虚拟机有用，也不如灵活。例如，由于监狱共享内核，因此内核恐慌将导致硬件上的所有虚拟机崩溃。
- en: However, because they bypass the overhead of virtualizing hardware, virtualized
    machines can be about as fast as native execution—in fact, they are native.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于它们绕过了虚拟化硬件的开销，虚拟化机器可以与本地执行一样快——事实上，它们就是本地的。
- en: OS virtualization and Xen complement each other, each being useful in different
    situations, possibly even simultaneously. One can readily imagine, for example,
    giving a user a single Xen VM, which he then partitions into multiple Zones for
    his own use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统虚拟化和Xen相互补充，在不同的情境下都很有用，甚至可能同时使用。例如，可以想象给用户分配一个单独的Xen虚拟机（VM），然后他可以将它分区成多个区域（Zones）供自己使用。
- en: 'Paravirtualization: Xen''s Approach'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化：Xen的方法
- en: Finally, somewhere between the two, there's *paravirtualization*, which relies
    on the operating system being modified to work in concert with a sort of "super
    operating system," which we call the *hypervisor*. This is the approach Xen uses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在两者之间，有一种叫做*半虚拟化*的技术，它依赖于操作系统被修改以与一种“超级操作系统”协同工作，我们称之为*虚拟管理程序*。这正是Xen所采用的方法。
- en: How Paravirtualization Works
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半虚拟化的工作原理
- en: Xen works by introducing a very small, very compact and focused piece of software
    that runs directly on the hardware and provides services to the virtualized operating
    systems.^([[11](#ftn.CHP-1-FNOTE-4)])
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Xen通过引入一个非常小、非常紧凑且专注于硬件的软件片段来工作，该软件直接在硬件上运行并为虚拟化操作系统提供服务。[^11](#ftn.CHP-1-FNOTE-4)
- en: Xen's approach to virtualization does away with most of the split between host
    OS and guest OS. Full virtualization and OS-level virtualization have a clear
    distinction—the host OS is the one that runs with full privileges. With Xen, only
    the hypervisor has full privileges, and it's designed to be as small and limited
    as possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Xen对虚拟化的方法消除了主机操作系统和客户操作系统之间的大部分分裂。全虚拟化和操作系统级别的虚拟化有一个明显的区别——主机操作系统是拥有完全权限的那个。在Xen中，只有虚拟管理程序（hypervisor）拥有完全权限，并且它被设计得尽可能小和有限。
- en: Instead of this "host/guest" split, the hypervisor relies on a trusted guest
    OS (domain 0, the *driver domain*, or more informally, *dom0*) to provide hardware
    drivers, a kernel, and a userland. This privileged domain is uniquely distinguished
    as the domain that the hypervisor allows to access devices and perform control
    functions. By doing this, the Xen developers ensure that the hypervisor remains
    small and maintainable and that it occupies as little memory as possible. [Figure 1-1](ch01s04.html#shown_here_is_the_hypervisor_with_domain
    "Figure 1-1. Shown here is the hypervisor with domains. Note that the hypervisor
    runs directly on the hardware but doesn't itself mediate access to disk and network
    devices. Instead, dom0 interacts directly with disk and network devices, servicing
    requests from the other domains. In this diagram, domU 1 also acts as a driver
    domain for an unnamed PCI device.") shows this relationship.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种“主机/客人”的划分不同，虚拟机管理程序依赖于一个受信任的客人操作系统（domain 0，*驱动域*，或者更非正式地，*dom0*）来提供硬件驱动程序、内核和用户空间。这个特权域被独特地区分开来，是虚拟机管理程序允许访问设备并执行控制功能的域。通过这样做，Xen的开发者确保虚拟机管理程序保持小巧且易于维护，并且尽可能占用最少的内存。[图1-1](ch01s04.html#shown_here_is_the_hypervisor_with_domain
    "图1-1. 这里展示了带有域的虚拟机管理程序。注意虚拟机管理程序直接在硬件上运行，但它本身并不中介对磁盘和网络设备的访问。相反，dom0直接与磁盘和网络设备交互，为其他域提供服务。在这个图中，domU
    1还充当一个未命名的PCI设备的驱动域。")展示了这种关系。
- en: '![Shown here is the hypervisor with domains. Note that the hypervisor runs
    directly on the hardware but doesn''t itself mediate access to disk and network
    devices. Instead, dom0 interacts directly with disk and network devices, servicing
    requests from the other domains. In this diagram, domU 1 also acts as a driver
    domain for an unnamed PCI device.](httpatomoreillycomsourcenostarchimages333193.png.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![这里展示了带有域的虚拟机管理程序。注意虚拟机管理程序直接在硬件上运行，但它本身并不中介对磁盘和网络设备的访问。相反，dom0直接与磁盘和网络设备交互，为其他域提供服务。在这个图中，domU
    1还充当一个未命名的PCI设备的驱动域。](httpatomoreillycomsourcenostarchimages333193.png.jpg)'
- en: Figure 1-1. Shown here is the hypervisor with domains. Note that the hypervisor
    runs directly on the hardware but doesn't itself mediate access to disk and network
    devices. Instead, dom0 interacts directly with disk and network devices, servicing
    requests from the other domains. In this diagram, domU 1 also acts as a driver
    domain for an unnamed PCI device.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 1-1. 这里展示了带有域的虚拟机管理程序。注意虚拟机管理程序直接在硬件上运行，但它本身并不中介对磁盘和网络设备的访问。相反，dom0直接与磁盘和网络设备交互，为其他域提供服务。在这个图中，domU
    1还充当一个未命名的PCI设备的驱动域。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*See also "Safe Hardware Access with the Xen Virtual Machine Monitor," Fraser
    et al.^([[12](#ftn.CHP-1-FNOTE-5)]) Also, non-dom0 driver domains can exist—however,
    they''re not recommended on current hardware in the absence of an IOMMU (I/O Memory
    Management Unit) and therefore will not be covered here. For more on IOMMU development,
    see [Chapter 12](ch12.html "Chapter 12. HVM: BEYOND PARAVIRTUALIZATION")*.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*另请参阅“使用Xen虚拟机监控器安全访问硬件”，Fraser等人.^([[12](#ftn.CHP-1-FNOTE-5)]) 此外，还可以存在非-dom0的驱动域——然而，在没有IOMMU（I/O内存管理单元）的情况下，它们在当前硬件上不被推荐，因此这里不会涉及。有关IOMMU开发的更多信息，请参阅[第12章](ch12.html
    "第12章。HVM：超越Para虚拟化")*。'
- en: Domain 0's privileged operations broadly fall into two categories. First, dom0
    functions as an area from which to administer Xen. From the dom0, the administrator
    can control the other domains running on the machine—create, destroy, save, restore,
    etc. Network and storage devices can also be manipulated—created, presented to
    the kernel, assigned to domUs, etc.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Domain 0的特权操作大致分为两类。首先，dom0作为一个区域，用于管理Xen。从dom0出发，管理员可以控制机器上运行的其它域——创建、销毁、保存、恢复等。网络和存储设备也可以被操作——创建、呈现给内核、分配给domUs等。
- en: Second, dom0 has uniquely privileged access to hardware. The domain 0 kernel
    has the usual hardware drivers and uses them to export abstractions of hardware
    devices to the hypervisor and thence to virtual machines. Think of the machine
    as a car, with the dom0 as driver. He's also a passenger but has privileges and
    responsibilities that the other passengers don't.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，dom0对硬件具有独特的特权访问权限。domain 0的内核拥有通常的硬件驱动程序，并使用它们将硬件设备的抽象导出给虚拟机管理程序，然后传递给虚拟机。想象一下这台机器就像一辆车，dom0就像是司机。他也是一个乘客，但拥有其他乘客没有的特权和责任。
- en: '* * *'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[11](#CHP-1-FNOTE-4)]) Some would call the Xen hypervisor a microkernel.
    Others wouldn't.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[11](#CHP-1-FNOTE-4)]) 有些人可能会称Xen虚拟机管理程序为微内核，而有些人则不会。
- en: ^([[12](#CHP-1-FNOTE-5)]) See [http://www.cl.cam.ac.uk/research/srg/netos/papers/2004-oasis-ngio.pdf](http://www.cl.cam.ac.uk/research/srg/netos/papers/2004-oasis-ngio.pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[12](#CHP-1-FNOTE-5)]) 查看 [http://www.cl.cam.ac.uk/research/srg/netos/papers/2004-oasis-ngio.pdf](http://www.cl.cam.ac.uk/research/srg/netos/papers/2004-oasis-ngio.pdf)。
- en: 'Xen''s Underpinnings: The Gory Details'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Xen的底层：详细情况
- en: 'So, with this concept of virtual devices firmly in mind, the question becomes:
    What does a computer need to provide at the most basic level? The Xen developers
    considered this question at length and concluded that Xen would have to manage
    *CPU time, interrupts, memory, block devices*, and *network*.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，牢记这个虚拟设备的概念，问题变成了：计算机在最低级别需要提供什么？Xen的开发者深入考虑了这个问题，并得出结论，Xen必须管理**CPU时间、中断、内存、块设备**和**网络**。
- en: The hypervisor operates much like the very core of a traditional operating system,
    parceling out CPU time and resources to the operating systems that run under it,
    which in turn allocate resources to their individual processes. Just as modern
    operating systems can transparently pause a process, the Xen hypervisor can pause
    an operating system, hand control to another for a while, and then seamlessly
    restart the paused system.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机管理程序的工作方式与传统操作系统的核心非常相似，将CPU时间和资源分发给在其下运行的操作系统，这些操作系统再将其分配给各自的进程。正如现代操作系统可以透明地暂停一个进程一样，Xen虚拟机管理程序可以暂停一个操作系统，暂时将控制权交给另一个操作系统，然后无缝地重新启动暂停的系统。
- en: Because Xen is designed to be small and simple, the hypervisor interacts with
    the OSs that run under it using a very few well-defined interfaces, which the
    Xen team refers to as *hypercalls*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Xen被设计成小巧简单，虚拟机管理程序与其下运行的操作系统使用非常少数量定义良好的接口进行交互，Xen团队将这些接口称为**超调用**。
- en: These hypercalls take the place of a standard operating system's system calls,
    with a similar interface. In effect, they have the same function—to allow user
    code to execute privileged operations in a way that can be controlled and managed
    by trusted code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超调用取代了标准操作系统的系统调用，具有类似的接口。实际上，它们具有相同的功能——允许用户代码以受信任代码可以控制和管理的的方式进行特权操作。
- en: The hypercalls have several design goals and requirements. First, they are *asynchronous*
    so that hypercalls don't block other processes or other OSs—while one domain waits
    for a hypercall to finish, another domain can get some CPU time. Second, they
    are small, simple, and clearly defined—Xen has only about 50 hypercalls, in contrast
    with over 300 syscalls for Linux. Finally, the hypercalls use a common system
    of notifications to interact with the Xen hypervisor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 超调用有几个设计目标和要求。首先，它们是**异步的**，这样超调用就不会阻塞其他进程或其他操作系统——当一个域等待超调用完成时，另一个域可以获取一些CPU时间。其次，它们很小、简单且定义明确——与Linux的300多个系统调用相比，Xen只有大约50个超调用。最后，超调用使用一个通用的通知系统与Xen虚拟机管理程序进行交互。
- en: Scheduling
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度
- en: The CPU, regardless of Xen virtualization, is still a physical object, subject
    to all the messy and intractable laws of physical reality. It can perform only
    one instruction at a time, and so the various demands on its attention have to
    be scheduled. Xen schedules processes to run on the CPU in response to instructions
    from the guest OSs, subject to its own accounting of which guest should have access
    to the CPU at any given time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否进行Xen虚拟化，CPU仍然是一个物理对象，受所有混乱和难以处理的物理现实法则的约束。它一次只能执行一条指令，因此对它注意力的各种需求必须进行调度。Xen根据客户操作系统发出的指令在CPU上调度进程运行，同时根据其自己的计算，确定在任何给定时间哪个客户机应该有权访问CPU。
- en: Each guest maintains its own internal queues of which instructions to run next—which
    process gets a CPU time slice, essentially. In an ordinary machine, the OS would
    run the process at the head of a queue on the physical CPU. (Under Linux, the
    run queue.) On a virtual machine, it instead notifies Xen to run that process
    for a certain amount of time, expressed in domain-virtual terms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户机维护自己的内部队列，指定下一个要运行的指令——本质上就是哪个进程获得CPU时间片。在一个普通机器上，操作系统会在物理CPU上运行队列头部的进程。（在Linux中，是运行队列。）在虚拟机上，它反而会通知Xen运行该进程一定时间，以域虚拟术语表示。
- en: The guest can also "make an appointment" with Xen, requesting an interrupt and
    CPU time at a later time, based on either a domain-virtual timer or system timer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 客户机还可以与Xen“预约”时间，根据域虚拟定时器或系统定时器，在稍后请求中断和CPU时间。
- en: The domain-virtual timer is used mostly for internal scheduling between processes—the
    domU kernel can request that the hypervisor preempt a task and run another one
    after a certain amount of virtual time has passed. Note that the domain doesn't
    actually schedule processes directly on the CPU—that sort of hardware interaction
    has to be handled by the hypervisor.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 域虚拟定时器主要用于进程之间的内部调度——domU 内核可以请求虚拟机管理程序在经过一定量的虚拟时间后抢占一个任务并运行另一个任务。请注意，域实际上并不直接在
    CPU 上调度进程——这种硬件交互必须由虚拟机管理程序处理。
- en: The system timer is used for events that are sensitive to real-world time, such
    as networking. Using the system timer, the domain can give up the CPU for a while
    and request to be woken back up in time to refill the network buffer or send out
    the next ping.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 系统定时器用于对现实世界时间敏感的事件，例如网络。使用系统定时器，域可以暂时放弃 CPU 并请求在适当的时间醒来以填充网络缓冲区或发送下一个 ping。
- en: 'The administrator can also tune the scheduling parameters that Xen uses to
    allocate resources to domains. There are a number of different algorithms, with
    varying degrees of usefulness. See [Chapter 7](ch07.html "Chapter 7. HOSTING UNTRUSTED
    USERS UNDER XEN: LESSONS FROM THE TRENCHES") for more details on scheduling.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员还可以调整 Xen 用于向域分配资源的调度参数。有几种不同的算法，其有用程度各不相同。有关调度的更多详细信息，请参阅[第 7 章](ch07.html
    "第 7 章。在 XEN 下托管不受信任的用户：来自战壕的经验教训")。
- en: Interrupts
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中断
- en: In computing terms, an *interrupt* is a request for attention. An interrupt
    usually occurs when some piece of hardware needs to interact with its control
    software (that is, drivers). Traditionally, interrupts must be handled immediately,
    and all other processes have to wait until the interrupt handler has finished.
    In the context of virtualization, this is patently unacceptable.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机术语中，*中断*是请求注意的信号。中断通常发生在某些硬件需要与其控制软件（即驱动程序）交互时（即，驱动程序）。传统上，中断必须立即处理，并且所有其他进程都必须等待直到中断处理程序完成。在虚拟化的上下文中，这是明显不可接受的。
- en: Xen therefore intercepts interrupts, rather than passing them directly through
    to guest domains. This allows Xen to retain control of the hardware, *scheduling*
    interrupt servicing, rather than merely reacting. Domains can register interrupt
    handlers with the hypervisor in advance. Then, when an interrupt comes in, Xen
    notifies the appropriate guest domain and schedules it for execution. Interrupts
    that occur while the domain is waiting to execute are coalesced into a nice package,
    avoiding unnecessary notifications. This also contributes to Xen's performance
    because context switches between domains are expensive.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Xen 会拦截中断，而不是直接将它们传递给客户域。这允许 Xen 保持对硬件的控制，*调度*中断服务，而不是仅仅做出反应。域可以提前向虚拟机管理程序注册中断处理程序。然后，当中断到来时，Xen
    通知适当的客户域并为其安排执行。在域等待执行时发生的中断会被合并成一个漂亮的包，避免不必要的通知。这也为 Xen 的性能做出了贡献，因为域之间的上下文切换是昂贵的。
- en: Memory
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存
- en: The hypervisor has authority over memory that is both localized and absolute.
    It must allocate all memory used by the domains, but it only deals with physical
    memory and the page table—the guest OSs handle all other memory management functions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机管理程序对本地化和绝对化的内存都拥有权限。它必须为所有域分配内存，但它只处理物理内存和页表——客户操作系统处理所有其他内存管理功能。
- en: This, as it turns out, is quite as much as any sensible implementor could desire.
    Memory, under x86, is difficult and arcane. The Xen authors point out, in a classic
    understatement, that "the x86 processors use a complex hybrid memory management
    scheme." [Figure 1-2](ch01s05.html#lets_take_the_example_of_translating_an_ "Figure 1-2. Let's
    take the example of translating an address given by an application. First, at
    the left, we have the address as given. This consists of a segment selector and
    offset. The MMU looks up the segment selector in the GDT (Global Descriptor Table)
    to find that segment's location in the linear address space, which is the complete
    address space accessible to the process (usually 4GB). The offset then acts as
    an address within that segment. This gives the processor a linear address relative
    to the process's address space. The MMU then decomposes that address into two
    indices and an offset—first it looks through the page directory to find the correct
    page table, then it finds the correct page in the page table, and finally it uses
    the offset to return a machine address—actual, physical memory.") shows an overview
    of address translation on the x86.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，这几乎满足了任何明智的实现者的需求。在x86架构下，内存管理既困难又晦涩。Xen的作者以一种经典的低调方式指出，“x86处理器使用了一种复杂的混合内存管理方案。”[图1-2](ch01s05.html#lets_take_the_example_of_translating_an_
    "图1-2. 举例说明如何翻译应用程序给出的地址。首先，在左侧，我们看到给出的地址。这由段选择器和偏移量组成。MMU（内存管理单元）在GDT（全局描述符表）中查找段选择器，以找到该段在线性地址空间中的位置，这是进程可访问的完整地址空间（通常为4GB）。偏移量随后在该段内充当地址。这为处理器提供了一个相对于进程地址空间的线性地址。然后MMU将此地址分解为两个索引和一个偏移量——首先它通过页面目录找到正确的页面表，然后它在页面表中找到正确的页面，最后它使用偏移量返回一个机器地址——实际的物理内存。”]展示了x86架构上地址转换的概述。
- en: On the most fundamental, hardware-dependent level, or at least the lowest level
    we're willing to mention here, we have the machine memory. This can be accessed
    one word at a time, via numbered addresses. That's the final product, shown on
    the right in [Figure 1-2](ch01s05.html#lets_take_the_example_of_translating_an_
    "Figure 1-2. Let's take the example of translating an address given by an application.
    First, at the left, we have the address as given. This consists of a segment selector
    and offset. The MMU looks up the segment selector in the GDT (Global Descriptor
    Table) to find that segment's location in the linear address space, which is the
    complete address space accessible to the process (usually 4GB). The offset then
    acts as an address within that segment. This gives the processor a linear address
    relative to the process's address space. The MMU then decomposes that address
    into two indices and an offset—first it looks through the page directory to find
    the correct page table, then it finds the correct page in the page table, and
    finally it uses the offset to return a machine address—actual, physical memory.").
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基础的、硬件依赖的层面上，或者至少是我们愿意在这里提到的最低层面上，我们有机器内存。这个内存可以逐字访问，通过编号的地址。这就是最终产品，如[图1-2](ch01s05.html#lets_take_the_example_of_translating_an_
    "图1-2. 举例说明如何翻译应用程序给出的地址。首先，在左侧，我们看到给出的地址。这由段选择器和偏移量组成。MMU（内存管理单元）在GDT（全局描述符表）中查找段选择器，以找到该段在线性地址空间中的位置，这是进程可访问的完整地址空间（通常为4GB）。偏移量随后在该段内充当地址。这为处理器提供了一个相对于进程地址空间的线性地址。然后MMU将此地址分解为两个索引和一个偏移量——首先它通过页面目录找到正确的页面表，然后它在页面表中找到正确的页面，最后它使用偏移量返回一个机器地址——实际的物理内存。”]中所示。
- en: However, this approach is too hardware dependent for a modern computer, which
    needs to be able to swap to disk, memory map I/O, use DMA, and so on. The processor
    therefore implements *virtual memory*, which provides two advantages for the programmer.
    First, it allows each process to access its own memory as if it were the only
    thing running on the computer—that is, as if it had the entirety of physical memory
    to itself. Second, virtual memory enables a process to access much more memory
    than is physically available, swapping to disk as necessary.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法对于现代计算机来说过于依赖硬件，因为现代计算机需要能够交换到磁盘、内存映射I/O、使用DMA等。因此，处理器实现了*虚拟内存*，这为程序员提供了两个优势。首先，它允许每个进程像它是计算机上唯一运行的东西一样访问其自己的内存——也就是说，就像它拥有全部物理内存一样。其次，虚拟内存使进程能够访问比物理上可用的内存多得多的内存，并在必要时交换到磁盘。
- en: '![Let''s take the example of translating an address given by an application.
    First, at the left, we have the address as given. This consists of a segment selector
    and offset. The MMU looks up the segment selector in the GDT (Global Descriptor
    Table) to find that segment''s location in the linear address space, which is
    the complete address space accessible to the process (usually 4GB). The offset
    then acts as an address within that segment. This gives the processor a linear
    address relative to the process''s address space. The MMU then decomposes that
    address into two indices and an offset—first it looks through the page directory
    to find the correct page table, then it finds the correct page in the page table,
    and finally it uses the offset to return a machine address—actual, physical memory.](httpatomoreillycomsourcenostarchimages333195.png.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![让我们以翻译应用程序给出的地址为例。首先，在左侧，我们有给出的地址。这由段选择器和偏移量组成。MMU（内存管理单元）在 GDT（全局描述符表）中查找段选择器，以找到该段在线性地址空间中的位置，这是进程可访问的完整地址空间（通常是
    4GB）。然后，偏移量在该段内充当地址。这给处理器提供了一个相对于进程地址空间的线性地址。然后 MMU 将该地址分解为两个索引和一个偏移量——首先它通过页目录找到正确的页表，然后它在页表中找到正确的页，最后它使用偏移量返回一个机器地址——实际的物理内存。](httpatomoreillycomsourcenostarchimages333195.png.jpg)'
- en: Figure 1-2. Let's take the example of translating an address given by an application.
    First, at the left, we have the address as given. This consists of a segment selector
    and offset. The MMU looks up the segment selector in the GDT (Global Descriptor
    Table) to find that segment's location in the linear address space, which is the
    complete address space accessible to the process (usually 4GB). The offset then
    acts as an address within that segment. This gives the processor a linear address
    relative to the process's address space. The MMU then decomposes that address
    into two indices and an offset—first it looks through the page directory to find
    the correct page table, then it finds the correct page in the page table, and
    finally it uses the offset to return a machine address—actual, physical memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-2. 让我们以翻译应用程序给出的地址为例。首先，在左侧，我们有给出的地址。这由段选择器和偏移量组成。MMU 在 GDT（全局描述符表）中查找段选择器，以找到该段在线性地址空间中的位置，这是进程可访问的完整地址空间（通常是
    4GB）。然后，偏移量在该段内充当地址。这给处理器提供了一个相对于进程地址空间的线性地址。然后 MMU 将该地址分解为两个索引和一个偏移量——首先它通过页目录找到正确的页表，然后它在页表中找到正确的页，最后它使用偏移量返回一个机器地址——实际的物理内存。
- en: Like physical memory, virtual memory is accessed one word at a time, via numbered
    addresses. The mapping between physical addresses and virtual addresses is handled
    by *page tables*, which associate chunks of physical memory with pages of virtual
    memory.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 与物理内存一样，虚拟内存是逐字访问的，通过编号的地址。物理地址和虚拟地址之间的映射由 *页表* 处理，它们将物理内存块与虚拟内存页关联起来。
- en: This level of abstraction applies even when there's only one operating system
    running on the machine. It's one of the basic forms of virtualization, so ubiquitous
    as to go unnoticed by most non-programmers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象级别甚至适用于机器上只运行一个操作系统的情况。它是虚拟化的基本形式之一，如此普遍以至于大多数非程序员都未注意到。
- en: Xen interposes itself at this point, acting as the sole gatekeeper of the page
    tables. Because applications have to go through Xen to update their mappings between
    virtual and physical memory, the hypervisor can ensure that domains only access
    memory within their reservation—memory that a domain doesn't have access to isn't
    mapped to any of its pages and therefore doesn't exist from the domain's perspective.
    [Figure 1-3](ch01s05.html#the_hypervisors_main_role_is_to_validate "Figure 1-3. The
    hypervisor's main role is to validate the domU's updates to the page tables, ensuring
    that domU only maps memory allocated to it. The domU handles memory using physical
    pages directly, generating pseudophysical addresses where necessary.") shows the
    relationship between the hypervisor, physical memory, and pseudophysical mappings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Xen 在这一点上介入，作为页表的唯一守护者。因为应用程序必须通过 Xen 来更新它们在虚拟内存和物理内存之间的映射，因此虚拟机管理程序可以确保域只能访问其预留的内存——域无法访问的内存不会被映射到任何页面上，因此从域的角度来看，这些内存不存在。[图 1-3](ch01s05.html#the_hypervisors_main_role_is_to_validate
    "图 1-3. 虚拟机管理程序的主要作用是验证 domU 对页表的更新，确保 domU 只映射分配给它的内存。domU 直接使用物理页处理内存，在必要时生成伪物理地址。")
    展示了虚拟机管理程序、物理内存和伪物理映射之间的关系。
- en: So far so good. x86 handles this partially in hardware, using an area of the
    processor called the *MMU*, or *Memory Management Unit*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利。x86通过硬件部分处理这个问题，使用处理器中的一个区域，称为*MMU*，或*内存管理单元*。
- en: Although this mapping should be sufficient to provide memory protection and
    the *illusion* of contiguous virtual memory, the x86 architecture also uses segmentation
    to protect memory and increase the amount of addressable memory.^([[13](#ftn.CHP-1-FNOTE-6)])
    Application-level addresses are *logical addresses*, each of which includes a
    16-bit segment selector and a 32-bit segment offset, which the processor then
    maps to virtual (or *linear*) addresses, which are eventually turned into physical
    addresses.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种映射应该足以提供内存保护和连续虚拟内存的*错觉*，但x86架构也使用分段来保护内存并增加可寻址内存的数量.^([[13](#ftn.CHP-1-FNOTE-6)])
    应用级地址是*逻辑地址*，每个地址包括一个16位的段选择器和32位的段偏移量，处理器然后将这些地址映射到虚拟（或*线性*）地址，这些地址最终被转换为物理地址。
- en: '![The hypervisor''s main role is to validate the domU''s updates to the page
    tables, ensuring that domU only maps memory allocated to it. The domU handles
    memory using physical pages directly, generating pseudophysical addresses where
    necessary.](httpatomoreillycomsourcenostarchimages333197.png.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![虚拟机管理程序的主要作用是验证domU对页表的更新，确保domU只映射分配给它的内存。domU直接使用物理页来处理内存，在必要时生成伪物理地址。](httpatomoreillycomsourcenostarchimages333197.png.jpg)'
- en: Figure 1-3. The hypervisor's main role is to validate the domU's updates to
    the page tables, ensuring that domU only maps memory allocated to it. The domU
    handles memory using physical pages directly, generating pseudophysical addresses
    where necessary.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-3. 虚拟机管理程序的主要作用是验证domU对页表的更新，确保domU只映射分配给它的内存。domU直接使用物理页来处理内存，在必要时生成伪物理地址。
- en: In practice, however, modern software usually avoids the segment registers as
    much as possible—the segments are simply made equivalent to the entire address
    space, which has the practical effect of allowing processes to ignore their existence.
    However, the unused segmentation model provides a perfect way for Xen to protect
    its own memory reservation. The Xen hypervisor reserves a small amount of memory
    at the beginning of each domain's allocation and arranges the domain's segments
    so that they don't include the hypervisor's memory region.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，现代软件通常尽可能避免使用段寄存器——段被简单地等同于整个地址空间，这在实际效果上允许进程忽略它们的实际存在。然而，未使用的分段模型为Xen提供了保护其内存预留的完美方式。Xen虚拟机管理程序在每个域的分配开始处预留一小块内存，并安排域的段，使它们不包含虚拟机管理程序的内存区域。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*This leads to the common* /lib/tls *problem. See [Chapter 15](ch15.html "Chapter 15. TROUBLESHOOTING")
    for more information*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*这导致了常见的* /lib/tls *问题。更多信息请见[第15章](ch15.html "第15章. 故障排除")*。'
- en: But wait! There's more. Each memory segment can also be protected by the system
    of *rings*, which specify the privilege levels that allow access to the memory
    on a per-process basis. Xen protects the hypervisor by allowing it to run in the
    privileged ring 0, while the guest OS uses privilege rings 1 through 3\. This
    way, the processor can trap access attempts to the protected beginning of the
    segment.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！还有更多。每个内存段也可以通过*环*系统来保护，该系统指定了允许按进程访问内存的特权级别。Xen通过允许虚拟机管理程序在特权环0中运行来保护虚拟机管理程序，而客户操作系统使用特权环1到3。这样，处理器可以捕获对保护段开始的访问尝试。
- en: Finally, Xen adds another layer to this memory-management tower of cards. Because
    the physical memory allocated to a domain is likely to be fragmented, and because
    most guest OSs don't expect to have to deal with this sort of thing, they must
    be modified to build a mapping between the hardware and the virtual machine, or
    *real physical* and *pseudophysical* addresses. This mapping is used for all other
    components of the guest OS so that they have the illusion of operating in a contiguous
    address space.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Xen给这个内存管理塔又增加了一层。因为分配给域的物理内存很可能会碎片化，而且大多数客户操作系统都不期望必须处理这类事情，因此它们必须修改以在硬件和虚拟机之间建立映射，即*真实物理*和*伪物理*地址之间的映射。这个映射用于客户操作系统的所有其他组件，以便它们有在连续地址空间中操作的错觉。
- en: Thus, guest OS page tables still contain real machine addresses, which the guest
    itself translates to pseudophysical addresses for the benefit of applications.
    This helps Xen to remain fast, but it means that the guests cannot be trusted
    to manipulate page tables directly.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，客户操作系统页表仍然包含真实机器地址，客户本身将其转换为伪物理地址以供应用程序使用。这有助于Xen保持快速，但这也意味着客户不能被信任直接操作页表。
- en: The internal update mechanisms are replaced by two hypercalls that request Xen
    to manipulate the page tables on the domain's behalf.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 内部更新机制被两个超调用所取代，这些超调用请求Xen代表域操作页表。
- en: I/O Devices
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I/O设备
- en: Obviously, the domUs cannot be trusted to handle devices by themselves. Part
    of Xen's model is that even actively malicious guest domains should be unable
    to interfere with the hardware or other domains. All device access is through
    the hypervisor, with the aid of the dom0.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，domUs不能被信任独立处理设备。Xen模型的一部分是，即使是积极恶意的主机域也不应该能够干扰硬件或其他域。所有设备访问都通过虚拟机管理程序进行，dom0提供辅助。
- en: Xen handles domain I/O by using *device channels* and *virtual devices*. These
    are point-to-point links between a frontend device in the domU and a backend device
    in dom0, implemented as *ring buffers*, as shown in [Figure 1-4](ch01s05.html#a_ring_buffer_is_a_simple_data_structure
    "Figure 1-4. A ring buffer is a simple data structure that consists of preallocated
    memory regions, each tagged with a descriptor. As one party writes to the ring,
    the other reads from it, each updating the descriptors along the way. If the writer
    reaches a "written" block, the ring is full, and it needs to wait for the reader
    to mark some blocks empty."). (Note that these are distinct from x86 privilege
    rings.)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Xen通过使用*设备通道*和*虚拟设备*来处理域I/O。这些是在domU的前端设备与dom0的后端设备之间建立的点对点链接，实现为*环形缓冲区*，如图[图1-4](ch01s05.html#a_ring_buffer_is_a_simple_data_structure
    "图1-4。环形缓冲区是一种简单的数据结构，由预分配的内存区域组成，每个区域都带有描述符。当一个实体向环形缓冲区写入时，另一个实体从其中读取，每个实体在过程中更新描述符。如果写入者达到“已写入”块，环形缓冲区已满，它需要等待读取者标记一些块为空。")所示。（注意，这些与x86特权环不同。）
- en: '![A ring buffer is a simple data structure that consists of preallocated memory
    regions, each tagged with a descriptor. As one party writes to the ring, the other
    reads from it, each updating the descriptors along the way. If the writer reaches
    a "written" block, the ring is full, and it needs to wait for the reader to mark
    some blocks empty.](httpatomoreillycomsourcenostarchimages333199.png.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![环形缓冲区是一种简单的数据结构，由预分配的内存区域组成，每个区域都带有描述符。当一个实体向环形缓冲区写入时，另一个实体从其中读取，每个实体在过程中更新描述符。如果写入者达到“已写入”块，环形缓冲区已满，它需要等待读取者标记一些块为空。](httpatomoreillycomsourcenostarchimages333199.png.jpg)'
- en: Figure 1-4. A ring buffer is a simple data structure that consists of preallocated
    memory regions, each tagged with a descriptor. As one party writes to the ring,
    the other reads from it, each updating the descriptors along the way. If the writer
    reaches a "written" block, the ring is full, and it needs to wait for the reader
    to mark some blocks empty.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-4。环形缓冲区是一种简单的数据结构，由预分配的内存区域组成，每个区域都带有描述符。当一个实体向环形缓冲区写入时，另一个实体从其中读取，每个实体在过程中更新描述符。如果写入者达到“已写入”块，环形缓冲区已满，它需要等待读取者标记一些块为空。
- en: The important qualities of these rings is that they're fixed size and lightweight—the
    domain operates directly on physical memory, without the need for constant hypervisor
    intervention. At opportune times, the virtual machine notifies the hypervisor
    that it's updated the ring, and the hypervisor then takes appropriate action (sending
    packets, replying with data, etc.).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些环形缓冲区的重要特性是它们具有固定大小和轻量级——域直接在物理内存上操作，无需虚拟机管理程序的不断干预。在适当的时机，虚拟机会通知虚拟机管理程序它已更新了环形缓冲区，然后虚拟机管理程序采取适当的行动（发送数据包，用数据回复等）。
- en: For performance reasons, the rings generally contain I/O descriptors rather
    than actual data. The data is kept in separate buffers accessed through DMA, which
    Xen maintains control of using principles similar to those for memory allocation.
    The hypervisor also locks the pages in question, ensuring that the application
    doesn't try to give them away or use them incorrectly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于性能原因，环形缓冲区通常包含I/O描述符而不是实际数据。数据保存在通过DMA访问的单独缓冲区中，Xen使用类似于内存分配的原则来维护对这些缓冲区的控制。虚拟机管理程序还锁定相关页面，确保应用程序不会尝试将它们赠送或错误使用。
- en: As the contents of a ring buffer are read, they're replaced by empty descriptors,
    indicating that the buffer has space for more data. Meanwhile, the reading process
    moves on to the next buffer entry. At the end of the buffer, it simply wraps around.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当读取环形缓冲区的内容时，它们会被空描述符替换，这表明缓冲区有空间存储更多数据。同时，读取过程继续到下一个缓冲区条目。在缓冲区末尾，它简单地循环回来。
- en: When a ring fills up, the backend device silently drops data intended for it.
    This is analogous to a network card or disk filling its buffer and usually results
    in a re-request for the data at a more convenient time.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当环形缓冲区填满时，后端设备会静默地丢弃分配给它的数据。这类似于网络卡或磁盘填满其缓冲区，通常会导致在更方便的时间重新请求数据。
- en: Networking
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络
- en: The networking architecture (shown in [Figure 1-5](ch01s05.html#the_domu_uses_the_netfront_or_network_fr
    "Figure 1-5. The domU uses the netfront or network frontend driver as its network
    device, which then transparently flips packets to the netback driver in the dom0\.
    The packets then go through the Linux software bridge, traverse Linux's network
    stack (including interaction with iptables and friends), and finally go to the
    network via Linux's network driver.")) of Xen is designed to reuse as much code
    as possible. Xen provides virtual network interfaces to domains and functions,
    via device channels, as a medium by which packets can move from a virtual interface
    in a guest domain to a virtual interface in the driver domain. Other functions
    are left to standard networking tools.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Xen 的网络架构（如图 [1-5](ch01s05.html#the_domu_uses_the_netfront_or_network_fr "图
    1-5。domU 使用 netfront 或网络前端驱动程序作为其网络设备，该设备然后将数据包透明地转发到 dom0 中的 netback 驱动程序。数据包随后通过
    Linux 软件桥，穿越 Linux 的网络堆栈（包括与 iptables 和其他工具的交互），并最终通过 Linux 的网络驱动程序进入网络。")）设计得尽可能重用代码。Xen
    通过设备通道为域和功能提供虚拟网络接口，作为数据包可以从虚拟机域的虚拟接口移动到驱动程序域的虚拟接口的媒介。其他功能留给标准网络工具处理。
- en: '![The domU uses the netfront or network frontend driver as its network device,
    which then transparently flips packets to the netback driver in the dom0\. The
    packets then go through the Linux software bridge, traverse Linux''s network stack
    (including interaction with iptables and friends), and finally go to the network
    via Linux''s network driver.](httpatomoreillycomsourcenostarchimages333201.png.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![domU 使用 netfront 或网络前端驱动程序作为其网络设备，该设备然后将数据包透明地转发到 dom0 中的 netback 驱动程序。数据包随后通过
    Linux 软件桥，穿越 Linux 的网络堆栈（包括与 iptables 和其他工具的交互），并最终通过 Linux 的网络驱动程序进入网络。](httpatomoreillycomsourcenostarchimages333201.png.jpg)'
- en: Figure 1-5. The domU uses the netfront or network frontend driver as its network
    device, which then transparently flips packets to the netback driver in the dom0\.
    The packets then go through the Linux software bridge, traverse Linux's network
    stack (including interaction with iptables and friends), and finally go to the
    network via Linux's network driver.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1-5。domU 使用 netfront 或网络前端驱动程序作为其网络设备，该设备然后将数据包透明地转发到 dom0 中的 netback 驱动程序。数据包随后通过
    Linux 软件桥，穿越 Linux 的网络堆栈（包括与 iptables 和其他工具的交互），并最终通过 Linux 的网络驱动程序进入网络。
- en: The hypervisor functions solely as a data channel by which packets can move
    from the physical network interface to the domU's virtual interface. It mediates
    access between domains, but it doesn't validate packets or perform accounting—these
    are handled by iptables rules in dom0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机管理程序仅作为数据通道，通过该通道数据包可以从物理网络接口移动到 domU 的虚拟接口。它调解域之间的访问，但不验证数据包或执行会计——这些由 dom0
    中的 iptables 规则处理。
- en: Accordingly, the virtual network interface is relatively simple—a buffer to
    receive packets, a buffer to send them, and a hypercall to notify the hypervisor
    that something has changed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，虚拟网络接口相对简单——一个用于接收数据包的缓冲区，一个用于发送数据包的缓冲区，以及一个超调用来通知虚拟机管理程序有变化。
- en: The other side of this is that there's a lot of configurability in Xen's networking
    because you can act on the virtual interfaces using all the standard Linux tools.
    For more information on networking and suggestions on how to use this nigh-unlimited
    power, see [Chapter 5](ch05.html "Chapter 5. NETWORKING").
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Xen 的网络配置有很多可定制性，因为你可以使用所有标准的 Linux 工具对虚拟接口进行操作。有关网络信息和如何使用这种几乎无限的力量的建议，请参阅[第
    5 章](ch05.html "第 5 章。网络")。
- en: Block Devices
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块设备
- en: In practical terms, *block devices* are disks or disklike devices. MD arrays,
    filesystem images, and physical disks all fall under the general category of block
    devices.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，*块设备*是指磁盘或类似磁盘的设备。MD 数组、文件系统镜像和物理磁盘都属于块设备的总类别。
- en: Xen handles block devices in much the same way as network devices. The hypervisor
    exports *virtual block devices* (often referred to as VBDs) to the domUs and relies
    on the dom0 to provide backend drivers that map the functionality of the real
    block device to the VBD. The system of rings and limited hypercalls is also similar,
    as shown in [Figure 1-6](ch01s05.html#a_domus_request_for_a_block_device_begin
    "Figure 1-6. A domU's request for a block device begins with the blkfront or block
    frontend driver, which uses a buffer in the hypervisor to interact with the block
    backend driver in domain 0\. Blkback then reads or writes the requested blocks
    through dom0's block device driver (which can be a SCSI driver, IDE, fibre channel,
    etc.).").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Xen处理块设备的方式与网络设备非常相似。虚拟机管理程序导出*虚拟块设备*（通常称为VBDs）到domUs，并依赖于dom0提供后端驱动程序，将真实块设备的函数映射到VBD。环系统和有限的超调用系统也类似，如图1-6所示[图1-6](ch01s05.html#a_domus_request_for_a_block_device_begin
    "图1-6。domU请求块设备从blkfront或块前端驱动程序开始，该驱动程序在虚拟机管理程序中使用缓冲区与域0中的块后端驱动程序交互。然后Blkback通过dom0的块设备驱动程序（可以是SCSI驱动程序、IDE、光纤通道等）读取或写入请求的块。")。
- en: '![A domU''s request for a block device begins with the blkfront or block frontend
    driver, which uses a buffer in the hypervisor to interact with the block backend
    driver in domain 0\. Blkback then reads or writes the requested blocks through
    dom0''s block device driver (which can be a SCSI driver, IDE, fibre channel, etc.).](httpatomoreillycomsourcenostarchimages333203.png.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![domU请求块设备从blkfront或块前端驱动程序开始，该驱动程序在虚拟机管理程序中使用缓冲区与域0中的块后端驱动程序交互。然后Blkback通过dom0的块设备驱动程序（可以是SCSI驱动程序、IDE、光纤通道等）读取或写入请求的块。](httpatomoreillycomsourcenostarchimages333203.png.jpg)'
- en: Figure 1-6. A domU's request for a block device begins with the blkfront or
    block frontend driver, which uses a buffer in the hypervisor to interact with
    the block backend driver in domain 0\. Blkback then reads or writes the requested
    blocks through dom0's block device driver (which can be a SCSI driver, IDE, fibre
    channel, etc.).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图1-6。domU请求块设备从blkfront或块前端驱动程序开始，该驱动程序在虚拟机管理程序中使用缓冲区与域0中的块后端驱动程序交互。然后Blkback通过dom0的块设备驱动程序（可以是SCSI驱动程序、IDE、光纤通道等）读取或写入请求的块。
- en: Xen relies on the dom0 to create block devices and provide device drivers that
    map physical devices to Xen virtual devices.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Xen依赖于dom0创建块设备并提供将物理设备映射到Xen虚拟设备的设备驱动程序。
- en: For more information about this, see [Chapter 4](ch04.html "Chapter 4. STORAGE
    WITH XEN").
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于这方面的信息，请参阅[第4章](ch04.html "第4章。XEN的存储")。
- en: '* * *'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[13](#CHP-1-FNOTE-6)]) This is untrue for AMD64, which does away with segmentation
    entirely. Instead, Xen on x86_64 uses page-level protection for its memory regions.
    Stranger things in Heaven and Earth, Horatio.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[13](#CHP-1-FNOTE-6)]) 对于AMD64来说，这是不真实的，因为它完全取消了分段。相反，Xen在x86_64上使用页级保护来保护其内存区域。天地间还有更奇怪的事情，哈罗特。
- en: Putting It Together
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合起来
- en: In general, all of these implementation details demonstrate Xen's focus on simplicity
    and code reuse. Where possible, the Xen developers have chosen to focus on providing
    and managing channels between physical devices and virtual devices, letting Linux
    userspace tools and kernel mechanisms handle arbitration and device access. Also,
    the actual work is offloaded as much as possible to the dom0 so as to reduce the
    complexity of the hypervisor and maximize device support.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，所有这些实现细节都表明Xen关注简单性和代码重用。在可能的情况下，Xen开发者选择专注于提供和管理物理设备与虚拟设备之间的通道，让Linux用户空间工具和内核机制处理仲裁和设备访问。此外，实际工作尽可能多地卸载到dom0，以减少虚拟机管理程序的复杂性并最大化设备支持。
- en: For the administrator, this means that Xen can be administered and monitored,
    by and large, with standard tools, and that most interactions with Xen take place
    at the level of the dom0\. When Xen is installed and domains are running, the
    Xen domains act like normal, physical machines, running unmodified userspace programs,
    with some caveats. Let's move on to the next chapter to see how to set this up
    in practice.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于管理员来说，这意味着Xen可以通过标准工具进行管理和监控，大多数与Xen的交互都发生在dom0级别。当Xen安装并运行域时，Xen域就像正常的物理机器一样运行，运行未经修改的用户空间程序，但有一些注意事项。让我们继续下一章，看看如何在实践中设置它。
