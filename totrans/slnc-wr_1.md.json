["```\n00000000 00000000 11001010 11111110     Multiplicand (P)\n* 00000000 00000000 00000000 00000110     Multiplier (R)\n -------------------------------------\n  00000000 00000000 00000000 00000000     P * R[0] = P * 0\n  00000000 00000001 10010101 1111110      P * R[1] = P * 1\n  00000000 00000011 00101011 111110       P * R[2] = P * 1\n  00000000 00000000 00000000 00000        P * R[3] = P * 0\n  00000000 00000000 00000000 0000         P * R[4] = P * 0\n  00000000 00000000 00000000 000          P * R[5] = P * 0\n  ...\n+ 0                                       P * R[31] = P * 0\n -------------------------------------\n  00000000 00000100 11000001 11110100\n```", "```\n00000000 00000000 11001010 11111110      Multiplicand (P)\n* 00000000 00000000 00000000 00000110      Multiplier (R) - optimizing\n -------------------------------------\n  00000000 00000000 00000000 00000000      P * R[0] = P * 0\n  00000000 00000001 10010101 1111110       P * R[1] = P * 1\n+ 00000000 00000011 00101011 111110        P * R[2] = P * 1\n  ...Bail out âˆ’ ignore leading zeros of R!\n -------------------------------------\n  00000000 00000100 11000001 11110100\n```", "```\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/time.h>\n#include <limits.h>\n\nint main(int argc,char** argv) {\n\n  int shortest = INT_MAX;\n  int i,p,r;\n\n  if (argc != 3) {\n    printf(\"Usage: %s multiplicand multiplier\\n\",argv[0]);\n    exit(1);\n  }\n\n  p=atoi(argv[1]);\n  r=atoi(argv[2]);\n\n  for (i=0;i<256;i++) {\n    int ct;\n    struct timeval s;\n    struct timeval e;\n\n    gettimeofday(&s,NULL);\n\n    asm(\n\n      \"  movl $500,%%ecx    \\n\"  /* Loop repetition counter (R) */\n      \"imul_loop:           \\n\"\n      \"  movl %%esi,%%eax   \\n\"\n      \"  movl %%edi,%%edx   \\n\"\n      \"  imul %%edx,%%eax   \\n\"        /* Comment out for first run */\n      \"  loop imul_loop     \\n\"\n        :\n        : \"S\" (p), \"D\" (r)\n        : \"ax\", \"cx\", \"dx\", \"cc\");\n\n    gettimeofday(&e,NULL);\n\n    ct = ( e.tv_usec - s.tv_usec ) +\n         ( e.tv_sec - s.tv_sec ) * 1000000;\n\n    if (ct < shortest) shortest = ct;\n\n  }\n\n  printf(\"T[%d,%d] = %d usec\\n\",p,r,shortest);\n  return 0;\n}\n```", "```\n==Phrack Inc.==\n                  Volume 0x0b, Issue 0x39, Phile #0x0a of 0x12\n|=---------------=[ Against the System: Rise of the Robots ]=----------------=|\n|=---------------------------------------------------------------------------=|\n|=---=[ (C)Copyright 2001 by Michal Zalewski <lcamtuf@bos.bindview.com> ]=---=|\n\n-- [1] Introduction -----------------------------------------------------------\n\n\" . . . [the] big difference between the Web and traditional well-controlled\n collections is that there is virtually no control over what people can put on\n the Web. Couple this flexibility to publish anything with the enormous\n influence of search engines to route traffic, and companies that deliberately\n manipulate [sic] search engines for profit become a serious problem.\"\n\n                                    -- Sergey Brin, Lawrence Page [A]\n\nConsider a remote attacker who can compromise a remote system without sending\n any traffic to his victim. Consider an attack that relies on simply creating a\n file to compromise thousands of computers and that does not require any local\n resources to carry it out. Welcome to the world of zero-effort exploit\n techniques, automation, and anonymous as well as virtually unstoppable attacks\n that result from the ever-increasing complexity of the Internet.\n\nZero-effort exploits create their wish list and leave it somewhere in\n cyberspace where others can find it. The utility workers of the Internet [B] --\n hundreds of tireless, never-sleeping robots, information browsers, search\n engines, intelligent agents -- come to pick up the information and,\n unknowingly, become a tool in the hands of the attacker. You can stop one of\n them, but you cannot stop them all. You can find out what their orders are, but\n you cannot guess what these orders will be tomorrow, lurking somewhere in the\n abyss of not-yet-indexed cyberspace.\n\nYour private army, close at hand, is picking up the orders you left for them on\n their way. You exploit them without having to compromise them. They do what\n they are designed to do the best they can. Welcome to the new reality, in which\n our AI machines can rise against us.\n\nConsider a worm. Consider a worm that does nothing. It is carried and injected by\n others, but does not infect them. This worm creates a list of 10,000 random\n addresses with specific orders. And waits. Intelligent agents pick up this\nlist, and with their united forces they try to attack the targets. Imagine that\n they are not too lucky and achieve a 0.1% success ratio. Ten new hosts are now\n infected. On every single one of them, the worm does exactly the same thing-\nprepares a list. Now the agents come back to infect 100 new hosts. And so the\n story goes (or crawls, if you wish).\n\nAgents are virtually unnoticeable, as people are now accustomed to their\n presence and persistence. Agents just slowly move ahead in a never-ending loop.\n They work systematically. They do not choke connections with excessive data,\n and there are no network meltdowns, traffic spikes, or telltale signs of\n disease. Week after week they try new hosts, carefully, and their exploration\n never ends. Is it possible to notice that they carry a worm? Possibly . . .\n\n-- [2] An example -------------------------------------------------------------\n\nWhen this idea came to mind, I tried to use the simplest test just to see if I\n was right. I targeted, if that is the correct word, several general-purpose\n web-indexing crawlers. I created a very short HTML document and put it\n somewhere on my home page and then waited for a couple of weeks. And they came\n -- AltaVista, Lycos, and dozens of others. They found new links, picked them up\n enthusiastically, and then disappeared for days.\n\n  bigip1-snat.sv.av.com:\n    GET /indexme.html HTTP/1.0\n\n  sjc-fe5-1.sjc.lycos.com:\n    GET /indexme.html HTTP/1.0\n\n  [...]\n\nThey came back later to see what I had given them to parse.\n\n    http://somehost/cgi-bin/script.pl?p1=../../../../attack\n    http://somehost/cgi-bin/script.pl?p1=;attack\n    http://somehost/cgi-bin/script.pl?p1=|attack\n    http://somehost/cgi-bin/script.pl?p1=`attack`\n    http://somehost/cgi-bin/script.pl?p1=$(attack)\n    http://somehost:54321/attack?`id`\n    http://somehost/AAAAAAAAAAAAAAAAAAAAA...\n\nThe bots followed the links, each of the links simulating vulnerabilities.\n Although these exploits did not affect my server, they could easily compromise\n specific scripts or the entire web server on a remote system by causing the\n script to execute arbitrary commands, to write to arbitrary files, or, better\n yet, to suffer a buffer overflow problem:\n\n  sjc-fe6-1.sjc.lycos.com:\n    GET /cgi-bin/script.pl?p1=;attack HTTP/1.0\n\n  212.135.14.10:\n    GET /cgi-bin/script.pl?p1=$(attack) HTTP/1.0\nbigip1-snat.sv.av.com:\n    GET /cgi-bin/script.pl?p1=../../../../attack HTTP/1.0\n\n  [...]\n\nBots also happily connected to the non-HTTP ports I prepared for them and\n started a conversation by sending the data I supplied in URLs, thus making it\n possible to attack even services other than just web servers:\n\n  GET /attack?`id` HTTP/1.0\n  Host: somehost\n  Pragma: no-cache\n  Accept: text/*\n  User-Agent: Scooter/1.0\n  From: scooter@pa.dec.com\n\n  GET /attack?`id` HTTP/1.0\n  User-agent: Lycos_Spider_(T-Rex)\n  From: spider@lycos.com\n  Accept: */*\n  Connection: close\n  Host: somehost:54321\n\n  GET /attack?`id` HTTP/1.0\n  Host: somehost:54321\n  From: crawler@fast.no\n  Accept: */*\n  User-Agent: FAST-WebCrawler/2.2.6 (crawler@fast.no; [...])\n  Connection: close\n\n  [...]\n\nOther than the well-known set of web search engines, a bunch of other, private,\n crawl bots and agents run by specific organizations and companies also\n responded. Bots from ecn.purdue.edu, visual.com, poly.edu, inria.fr,\n powerinter.net, xyleme.com, and even more unidentified engines found this page\n and enjoyed it. Although some robots did not pick all addresses (some crawlers\n do not index CGI scripts at all, while others would not use nonstandard ports),\n the majority of the most powerful bots did attack virtually all vectors I\n supplied; and even those that were more careful always got tricked into\n performing at least some.\n\nThe experiment could be modified to use a set of real vulnerabilities in the\n form of thousands and thousands of web server overflows, Unicode problems in\n servers such as Microsoft IIS, or script problems. Instead of pointing to my\n own server, the bots could point to a list of randomly generated IP addresses\n or a random selection of .com, .org, or .net servers. Or, you could point the\n bots to a service that could be attacked by supplying a specific input string.\nThere is an army of robots encompassing a wide range of species, functions, and\n levels of intelligence. And these robots will do whatever you tell them to do.\n\n-- [3] Social considerations --------------------------------------------------\n\nWho is guilty if a \"possessed\" web crawler compromises your system? The most\n obvious answer is: the author of the original web page the crawler visited. But\n web page authors are hard to trace, and a web crawler indexing cycle takes\n weeks. It is hard to determine when a specific page was put on the Net because\n pages can be delivered in so many ways or even produced by other robots. There\n is no tracking mechanism for the Web that provides functionality similar to\n that implemented in the SMTP protocol. Moreover, many crawlers do not remember\n where they \"learned\" new URLs. Additional problems are caused by indexing\n flags, such as \"noindex\" without the \"nofollow\" option. In many cases, an\n author's identity and attack origin can never be fully determined.\n\nBy analogy to other cases, it is reasonable to expect that intelligent bot\n developers would be forced to implement specific filters or to pay enormous\n compensation to victims suffering from bot abuse, should this kind of attack\n become a reality. On the other hand, when you consider the number and wide\n variety of known vulnerabilities, it seems almost impossible to successfully\n filter contents to eliminate malicious code. And so the problem persists. (An\n additional issue is that not all crawler bots are under U.S. jurisdiction,\n which differs significantly from some of their counterparts when it comes to\n computer abuse regulations.)\n\n-- [4] Defense ----------------------------------------------------------------\n\nAs mentioned earlier, web crawlers themselves have limited defense and\n avoidance possibilities, due to a wide variety of web-based vulnerabilities. It\n is impossible to simply ban all malicious sequences, and heuristic\n investigation is risky: input that is valid and expected for one script may be\n enough to attack another. One reasonable defense tactic is for all potential\n victims to use secure and up-to-date software, but this concept is extremely\n unpopular for some reason. (A quick and nonscientific test: A search at http://\nwww.google.com with the unique documents filter enabled returns 62,100 matches\n for \"CGI vulnerability\" query [C].) Another line of defense against infected\n bots is to use the standard /robots.txt exclusion mechanism [D]. The price you\n pay, though, is the partial or complete exclusion of your site from search\n engines, which in most cases is undesirable and unacceptable. Also, some robots\n are broken or intentionally designed to ignore /robots.txt when following a\n direct link to new websites.\n\n-- [5] References -------------------------------------------------------------\n\n[A] \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\"\n    Googlebot concept, Sergey Brin, Lawrence Page, Stanford University\n    URL: http://www7.scu.edu.au/programme/fullpapers/1921/com1921.htm\n\n[B] \"The Web Robots Database\"\n    URL: http://www.robotstxt.org/wc/active.html\n\n[C] \"Web Security FAQ\", Lincoln D. Stein\n    URL: http://www.w3.org/Security/Faq/www-security-faq.html\n\n[D] \"A Standard for Robot Exclusion\", Martijn Koster\n    URL: http://info.webcrawler.com/mak/projects/robots/norobots.html\n\n|=[ EOF ]=-------------------------------------------------------------------=|\n```"]