- en: Chapter 2. COLLECTORS AND SENSORS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![image with no caption](httpatomoreillycomsourcenostarchimages651574.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The collector and sensors are the irreplaceable components of any flow system.
    Why? You can analyze data in innumerable ways, but before you do, you must gather
    and store the data, and you do that with sensors and the collector.
  prefs: []
  type: TYPE_NORMAL
- en: Because these pieces of your flow system are so critical, you'll begin your
    implementation with the collector and proceed to the first sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Collector Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *collector* is the host that receives records from network equipment and
    is where you'll perform most of your work. You must practice good systems administration
    on your collector, but you have a lot of flexibility on your hardware, operating
    system, and software.
  prefs: []
  type: TYPE_NORMAL
- en: Operating System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The collector runs on a Unix-like operating system. Almost any modern Unix-like
    operating system will suffice. I recommend a BSD operating system. (If you've
    never worked with BSD, permit me to suggest you buy one of my BSD books, such
    as *Absolute FreeBSD* or *Absolute OpenBSD*.)
  prefs: []
  type: TYPE_NORMAL
- en: That said, Linux, OpenSolaris, or any standard 32-bit or 64-bit Unix-like system
    with a recent GCC compiler and libraries will work just fine. The more unusual
    your operating system, however, the more trouble you will have with your collector
    and reporting system. Some commercial Unix-like operating systems are noted for
    behavior that technically complies with the standards but differs from every other
    Unix-like operating system in bizarre or downright unspeakable ways.^([[3](#ftn.CHP-2-FN-1)])
    Pick something known for playing well with others.
  prefs: []
  type: TYPE_NORMAL
- en: Whichever system you choose, it must be secure. Every active Unix-like operating
    system has a current security and hardening guide. Get it. Read it. Use it. Your
    collector should provide no other services except for those related to flow management.
    This will let you harden the system much more than a multipurpose machine permits.
  prefs: []
  type: TYPE_NORMAL
- en: System Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flow collection uses very few system resources other than disk space. (I've
    run flow collectors on Pentium Pro servers.)
  prefs: []
  type: TYPE_NORMAL
- en: The amount of disk space needed depends on the type of traffic on your network
    and how long you want to retain your records. In my environment, a data flow averaging
    5Mbps uses about 2GB of disk a month. Today, disk capacity expands faster than
    network utilization. I've found that it's possible to retain all data for the
    indefinite future if you keep buying larger disks.
  prefs: []
  type: TYPE_NORMAL
- en: Additional memory and CPU resources will accelerate flow reporting. When provisioning
    a machine for flow collection, I suggest using a slower but larger hard drive
    (like one of the "green" drives) and adding memory. The slower hard drive will
    save you money and power, and additional memory will improve the system's buffer
    cache and let you analyze data more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Get at least 4GB or more of RAM for your collector. Any modern Unix-like OS
    caches recently accessed disk files in RAM. Large amounts of memory will accelerate
    running multiple reports on the same data more than anything else.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[3](#CHP-2-FN-1)]) I would name names, but IBM has better lawyers than I
    do.
  prefs: []
  type: TYPE_NORMAL
- en: Sensor Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sensor is the device or program that captures flow data from your network
    and forwards it to the collector. Flow sensors are perhaps the most difficult
    portion of a flow-based management system to implement, especially on a geographically
    large network. You don't want to drive across the continent just to install a
    flow sensor!
  prefs: []
  type: TYPE_NORMAL
- en: The good news is, you don't need to worry about making those road trips. In
    fact, you probably already have flow sensors installed that you just haven't configured
    yet. Do you have Internet border routers? Have them act as sensors. Got a high-end
    Cisco switch? Great, use that.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have hardware that can do the job, you can implement a flow sensor
    in software.
  prefs: []
  type: TYPE_NORMAL
- en: Location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have even a medium-sized network, you'll quickly realize that you need
    to be selective about your sensor locations. Perhaps you have a couple dozen small
    switches and a hefty central core switch at your headquarters, half a dozen DMZs,
    an Internet border, and several remote facilities connected via VPN or MPLS. You
    could conceivably have sensors at each of these locations. Which are worth the
    effort?
  prefs: []
  type: TYPE_NORMAL
- en: Internet Border
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start with your Internet border. Almost all modern commercial-grade routers
    can export flow records. Analyzing those flows will tell you how you're using
    your Internet access. Knowing how much of your traffic is web surfing versus how
    much of your traffic is accessing your VPN will immediately help you make better
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Ethernet Core
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Look at your network's Ethernet core next. The internal LAN will have much more
    traffic than the wide-area Internet connection. Analyzing flow data from your
    internal network will quickly expose problems, misconfigurations, and performance
    issues. Your network architecture dictates sensor placement.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a single large core switch, such as a Cisco 4000 or 7000, the switch
    itself can probably export flow information.
  prefs: []
  type: TYPE_NORMAL
- en: If you have multiple switches in your Ethernet core, you might think you need
    flow export on every one of them, but that's overly ambitious. You do not need
    a complete record of every last packet that passes through every switch on your
    office network.
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering where to capture data, think about how traffic flows from
    device to device, and configure flow export only from central "choke" points.
    For example, my main data center has a configuration common in large enterprises:
    a large Cisco switch at its core and client switches in wiring closets on each
    floor. Every closet switch and every server is attached directly to the core switch.
    Any traffic that leaves a local switch must pass through the core switch.'
  prefs: []
  type: TYPE_NORMAL
- en: I collect flow data only from the central switch. This means that I'm blind
    to any traffic that remains entirely on a closet switch, but I capture all client
    broadcast traffic and anything that goes to servers or leaves the network. Even
    if a client uses a printer attached to the local switch, the print job traverses
    the print server attached to the core switch. One single flow export point offers
    adequate visibility into the entire network.
  prefs: []
  type: TYPE_NORMAL
- en: If your Ethernet core is only one or two small switches and none of the switches
    can export flow information, you can still implement flow-based network management
    if one of the switches has a "sniffer" or "monitor" port. One of the switches
    is effectively the network core. If you haven't designated a particular switch
    as the core, use the switch that serves the highest number of servers. Attach
    a software flow sensor to the monitor port (see [Configuring Software Flow Sensors](ch02s10.html
    "Configuring Software Flow Sensors") in [Configuring Software Flow Sensors](ch02s10.html
    "Configuring Software Flow Sensors")).
  prefs: []
  type: TYPE_NORMAL
- en: From Remote Facilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apply similar reasoning to remote facilities. Each remote facility has at least
    one router connecting it to the global network. It might be connected to an MPLS
    cloud or the Internet, but it's still your link into the outside world. Capture
    flows from that device.
  prefs: []
  type: TYPE_NORMAL
- en: If a remote facility has a export-capable core switch, use it as well. If the
    site reports many problems and the central switch cannot export flows, consider
    implementing a software sensor or upgrading the core switch.
  prefs: []
  type: TYPE_NORMAL
- en: Have remote sites export their flows to your central collector. Maintaining
    multiple collectors increases your workload with very little gain.
  prefs: []
  type: TYPE_NORMAL
- en: From Private Network Segments/DMZs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tracking flows from your core network, and your Internet border provides insight
    into your entire network, including servers on isolated or private network segments
    such as DMZs. You can see the traffic DMZ servers exchange with your core network
    and the Internet. What you cannot see is the traffic among DMZ servers.
  prefs: []
  type: TYPE_NORMAL
- en: If you have only one or two servers on a DMZ, you probably don't need flow export
    on that network segment. If you have several servers, you'll want flow export.
    You don't need to decide right away, however. Fine-tune your flow management installation
    on your core and border networks, and then implement flow export on your DMZs.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Collector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enough theory, enough planning. Let's install something!
  prefs: []
  type: TYPE_NORMAL
- en: An Internet search reveals any number of freely available flow collectors. The
    big three are cflowd, flowd, and flow-tools. Cflowd is obsolete, is unsupported,
    and doesn't compile on 64-bit systems. I have high hopes for flowd, but it's a
    comparatively new tool and doesn't yet have broad support among users or third-party
    software. That leaves us with flow-tools, the most commonly used flow management
    tool kit.
  prefs: []
  type: TYPE_NORMAL
- en: Flow-tools is older but has a very broad user community. The original author
    (Mark Fullmer) released version 0.68 in 2005 and went on to other projects. Any
    unmaintained software slowly becomes obsolete, but a group of users assumed responsibility
    for flow-tools in 2007\. These users collect improvements and bug fixes, and they
    release updated versions as needed. Although you'll still find occasional bugs,
    as with any software, flow-tools has a broad base of users.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Do not use version 0.68 of flow-tools. It has many small problems and does not
    function correctly on 64-bit systems. (It seems to work on 64-bit systems, but
    actually corrupts data, which is even worse than completely failing to work!)
    Take the time to install the newer version, 0.68.5 as of this writing.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Flow-tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can install flow-tools from an operating system package or from source.
    Before installing it, visit the flow-tools website at [http://code.google.com/p/flow-tools/](http://code.google.com/p/flow-tools/)
    to download the most recent version.
  prefs: []
  type: TYPE_NORMAL
- en: Installing from Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most Unix-like operating systems offer prepackaged versions of freely available
    software. If you can use precompiled, prepackaged software, do so.
  prefs: []
  type: TYPE_NORMAL
- en: However, many operating systems include only flow-tools version 0.68 rather
    than the newer version with its the bug fixes. Some operating systems, such as
    FreeBSD, include the newer software as a package called *flow-tools-ng*.
  prefs: []
  type: TYPE_NORMAL
- en: Fine details in the flow-tools package names are important. For example, flow-tools
    was recently at version 0.68.4\. A search for a CentOS flow-tools RPM revealed
    version 0.68-4, which is actually revision 4 of flow-tools package 0.68\. At first
    glance, this might look like the correct package, but it's not.
  prefs: []
  type: TYPE_NORMAL
- en: By the time this book reaches print, ideally major OS vendors will provide an
    updated package. If not, you get to install from source.
  prefs: []
  type: TYPE_NORMAL
- en: Installing from Source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To install the latest version of flow-tools from source, download the source
    code from [http://code.google.com/p/flow-tools/](http://code.google.com/p/flow-tools/),
    and extract it. You'll need GNU `make` and GCC, as well as the libraries and header
    files for your operating system.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now go into the *flow-tools* directory, and read the *INSTALL* file for the
    current compiling instructions. The process probably includes the steps `configure`,
    `make`, and `make install`.
  prefs: []
  type: TYPE_NORMAL
- en: Before installing, run **`./configure —help`** to list the build and installation
    options. The option I find most useful when installing is `prefix`, which allows
    you to specify where you want the software installed.
  prefs: []
  type: TYPE_NORMAL
- en: Most Unix-like operating systems install prepackaged software in */usr/local/bin*,
    */usr/local/sbin*, and so on. When you build a package such as flow-tools from
    source, however, your installation is not tightly integrated with the system's
    package management system. I suggest installing it in a location not used by the
    rest of the system because you don't want an operating system update to overwrite
    part of flow-tools or, worse, revert it to the obsolete version shipped with your
    OS! (Don't forget to update your PATH and MANPATH environment variables to include
    your new directories.)
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, you'll install flow-tools under */usr/local/flow*.
    I'll use this directory for the examples throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If your system is missing any prerequisites, the installation process will produce
    an error after the first command. After a successful install, you will find commands
    under */usr/local/flow/bin*, manual pages under */usr/local/flow/share/man*, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: DON'T CLEAN UP
  prefs: []
  type: TYPE_NORMAL
- en: Do not run make clean after building flow-tools from source. I find that I occasionally
    need to return to the source code for troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Running flow-capture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `flow-capture` program listens on a specified UDP port for incoming flow
    exports. It then captures the data and writes flow records to disk. `flow-capture`
    must know where to write the files, how often to start a new file, who to accept
    flow information from, and so on. The `flow-capture` manual page offers many options,
    but the following example suffices for many situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `-p` argument tells `flow-capture` where to store its process ID (PID) file.
    The location */var/run/flow-capture.pid* is a common default for most Unix-like
    operating systems.
  prefs: []
  type: TYPE_NORMAL
- en: The `-n` option tells `flow-capture` how many times it should rotate its log
    files in a 24-hour period. `287` tells `flow-capture` to create a new log file
    every five minutes. (The astute among you will notice that a day contains 288
    five-minute periods. Flow-capture creates one file and then rotates to a new one
    287 times in a day, for a total of 288 log files per day.) Many of the add-on
    reporting programs you'll use expect log files in five-minute increments.
  prefs: []
  type: TYPE_NORMAL
- en: Tell `flow-capture` where to write its files with `-w`. The directory */var/db/flows*
    is a common choice, though some prefer */var/log/flows*. Either way, each collector
    needs its own directory, so you might want to use something like */var/db/flows/internet*
    or */var/log/internet_flows*.
  prefs: []
  type: TYPE_NORMAL
- en: The `-S` *`5`* option tells `flow-capture` to log messages to syslog, telling
    how many flows it has processed, how many packets it has received, and how many
    flows it believes it has dropped. The argument (`5`) is the number of minutes
    between log messages.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`flow-capture` uses the syslog facility LOCAL6\. (Check a syslog tutorial to
    learn how to manage syslog messages.) You cannot change this facility without
    delving deep into the `flow-capture` source code.'
  prefs: []
  type: TYPE_NORMAL
- en: The last command line argument (`192.0.2.10/192.0.2.1/5678`) specifies `flow-capture`'s
    network configuration. The first address is the IP address on the local machine
    that `flow-capture` listens to. As you can see in the listing above, our sample
    collector runs on the IP address 192.0.2.10\. If you put a 0 in this space, the
    collector will accept traffic on all IP addresses on the machine. Even if your
    collector has only one IP address, I recommend explicitly assigning that address
    to your collector. You might add more IP addresses to your collector at a later
    date, and you probably don't want `flow-capture` attaching to those addresses
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: The second IP, 192.0.2.1, is the address of the sensor that is permitted to
    send data to this collector. If you were to put 0 here (or eliminate it) instead
    of an IP address, `flow-capture` would accept flow data from any address. Doing
    so increases the risk that an intruder will send bogus data to your collector
    but also permits you to accept flows from multiple sources simultaneously. (Almost
    all Unix-like operating systems have packet filter functions that would let you
    protect this port from all but sensors you specify, however.)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `5678` is the UDP port `flow-capture` listens to. Because no authority
    has formally assigned a UDP port for Cisco NetFlow, you should use any high-numbered
    port not reserved by any other service. Port 6434 is assigned to sFlow, and ports
    4739–4740 have been assigned to IPFIX, so you might want to use one of those ports.
    Also, many Cisco NetFlow products use port 2055, which was assigned to Cisco for
    a product never publicly released.
  prefs: []
  type: TYPE_NORMAL
- en: Try to start `flow-capture` on your system with options appropriate for your
    system. Confirm that it continues to run for a few minutes. This validates that
    you haven't made any fundamental mistakes in building or installing the software
    and that your command line is basically correct.
  prefs: []
  type: TYPE_NORMAL
- en: Starting flow-capture at Boot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your operating system must start `flow-capture` at boot time, just as it does
    any other critical service. The process for doing so varies widely with operating
    systems. If you installed flow-tools from a package provided by your operating
    system vendor, it almost certainly includes a startup script. For example, the
    Red Hat Linux RPM installs a startup script in */etc/init.d*. The FreeBSD package
    includes a startup script configured in */etc/rc.local*. You'll need to tell the
    script where to store the captured flow files, how often to rotate them, and what
    hosts to accept flow data from—in fact, all of the things you set in the `flow-capture`
    command in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the package for your chosen operating system doesn't include a startup script,
    add your `flow-capture` command into the computer's startup system appropriately.
    Check your operating system documentation. Sometimes this is as simple as copying
    the command into */etc/rc.local*. `flow-capture` should start only after the network
    and local storage are started.
  prefs: []
  type: TYPE_NORMAL
- en: Reboot your system to verify that `flow-capture` starts on boot.
  prefs: []
  type: TYPE_NORMAL
- en: How Many Collectors?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have one instance of `flow-capture` running, it's time to decide
    how to handle incoming data. You can choose to have all your sensors feed data
    to a single collector or have each sensor feed data to its own collector instance.
  prefs: []
  type: TYPE_NORMAL
- en: Having all sensors feed records to one collector is simple. Configure one and
    only one collector, and do not restrict the addresses that can send to it. Configure
    all your sensors to use that single collector. The collector will intermingle
    all the flow records from all sensors into one common log file. But how do you
    tell whether flows are from one part of the network or another? You can differentiate
    flows by the sensor IP address, but this adds steps to the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the absence of compelling reasons otherwise, I recommend running a separate
    collector for each flow sensor to help you keep your data separate. All the `flow-capture`
    instances can run on the same server and use the same IP address, and you can
    assign each `flow-capture` process its own UDP port and data directory so that
    you can analyze traffic from each network segment separately. Combining separated
    data is much easier than separating combined data.
  prefs: []
  type: TYPE_NORMAL
- en: Collector Log Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Your collector won''t record anything until a sensor starts sending data. Once
    data reaches the collector, however, the collector creates a log file of the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *tmp-v05.2009-11-15.134501-0500* |'
  prefs: []
  type: TYPE_TB
- en: The leading *tmp* in this filename indicates that this is a temporary file.
    `flow-capture` is still writing to this file. The year, month, and day on which
    the flow file was created comes next, followed by the timestamp in 24-hour format.
    The time given is the time the flow file was created, not the time the log file
    was completed and closed off. This example flow file was created on November 15,
    2009 (*2009-11-15*), at 13:45:01 or 1:45:01 **pm** (*134501*). The last number
    (*−0500*) is the time zone offset from UTC. My collector is running in Eastern
    standard time, negative five hours east of UTC. If your collector is in a time
    zone west of UTC, the time zone offset will have a *+* in front of it. If you
    have multiple collectors in multiple time zones, I recommend having them all use
    the same time zone, such as UTC.
  prefs: []
  type: TYPE_NORMAL
- en: When it's time to create a new file, `flow-capture` renames the current temporary
    file to begin with *ft-* and creates a new temporary file. The name otherwise
    remains the same, allowing you to easily identify and sort flow files by creation
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Collector Troubleshooting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you configure a sensor to send data to your collector but `flow-capture`
    isn't generating any log files within a few minutes, start troubleshooting. Either
    the sensor is not transmitting data, flow-capture is not writing the data to disk,
    or a firewall between the sensor and collector is blocking that port.
  prefs: []
  type: TYPE_NORMAL
- en: To begin troubleshooting, first verify that sensor data is reaching your collector
    with `tcpdump` in order to separate network problems from local software problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `-p` tells `tcpdump` to *not* put the interface into promiscuous mode. This
    means that the system will only sniff traffic that reaches the local interface.
    (A proper switch configuration should prevent promiscuous-mode sniffing, but using
    `-p` means that the machine won't even try.) The `-i` argument gives the name
    of the interface that you want to listen to, `em0` in this case, which happens
    to be a network card on my system. (Most Linux distributions have a primary network
    interface of `eth0`.) Finally, specify the port your collector runs on, `5678`
    in this case.
  prefs: []
  type: TYPE_NORMAL
- en: This command should print information about every packet arriving at your collector
    host on the specified port. If you don't see any data reaching the collector host,
    check your sensor configuration and the configuration of any firewalls between
    the sensor and collector, and then use your packet sniffer progressively closer
    to the sensor until you find where the data stops. If you reach your sensor and
    find that it's not putting any flow exports on the wire, your sensor configuration
    is suspect. If necessary, contact your vendor for assistance.
  prefs: []
  type: TYPE_NORMAL
- en: If the system is receiving flow data but `flow-capture` is not writing any log
    files, check your flow-capture configuration to be sure that you specified the
    proper UDP port and directory. Verify that the user running `flow-capture` can
    write files to that directory. Also check the system logs, such as */var/log/messages*,
    for error messages. (Remember, `flow-capture` uses LOCAL6\. Be certain to configure
    syslog to log LOCAL6 messages to a file.)
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Hardware Flow Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Configuring hardware flow sensors is your simplest and best option in most cases.
    Many network hardware manufacturers, such as Cisco, include flow export in their
    products. Cisco routers have supported NetFlow since the 1990s. Some larger Cisco
    switches also support NetFlow, but for best results you must configure switches
    differently than routers. Juniper routers also support flow export, so I'll cover
    configuring them. A number of smaller router vendors also support flow export,
    but you'll need to check your vendor's documentation for configuration instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The book's examples will assume that the flow collector is running on the host
    10.10.10.10 on UDP port 5678\. Replace these with appropriate values for your
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Cisco Routers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Configure NetFlow on a Cisco router interface by interface. You probably need
    NetFlow only on your upstream interface(s), not on the local Ethernet interface.
    (If you have a complex Ethernet infrastructure, such as an HSRP or VRRP cluster,
    you might want to monitor flows on the Ethernet interfaces as well.) In the following
    example, you activate NetFlow on the interface Serial0/0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Repeat this configuration on all upstream router interfaces, and then tell the
    router where to send flow data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now save your work. You should find that data goes to your collector almost
    immediately.
  prefs: []
  type: TYPE_NORMAL
- en: To see the flow information tracked on your router, use the IOS command **`show
    ip cache flow`**.
  prefs: []
  type: TYPE_NORMAL
- en: Some Cisco models use a slightly different syntax. If you have trouble, search
    Cisco's website or the Internet for suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: Cisco Switches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NetFlow on Cisco switches is a comparatively recent development and is limited
    to higher-end models. As a rule, only modular switches such as the 4000, 6000,
    and their descendants support NetFlow export. Many switches also require an add-on
    NetFlow card that you might or might not have. Stackable switches do not, nor
    do smaller stand-alone managed switches. Enable or disable NetFlow for the switch
    as a whole, not on a per-interface basis.
  prefs: []
  type: TYPE_NORMAL
- en: NetFlow support varies widely by model and the IOS version installed. I'm providing
    a sample configuration here, but do not blindly install it on your switch! Cisco
    switches can perform many tasks in many different ways, and what works in my environment
    might not work in yours.
  prefs: []
  type: TYPE_NORMAL
- en: Before configuring NetFlow on your router, ask Cisco how to capture all flow
    data from your model of switch in your environment. Your configuration will probably
    look similar to the following sample.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you decide that you want to just try this and see what happens, back up your
    configuration first and try it while you have no users on the network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The second line's configuration statement (`ip route-cache flow`) tells the
    switch to activate flows, and the next one (`ip flow ingress`) tells the switch
    to track flows based on the route they take to enter the switch.
  prefs: []
  type: TYPE_NORMAL
- en: By default these routing-capable switches export information only about flows
    that they route, not switched Ethernet traffic. However, because you want to capture
    flow information about the local Ethernet traffic as well, you tell the switch
    to capture the layer 2 flows as well with **`ip flow ingress layer2-switched`**.
  prefs: []
  type: TYPE_NORMAL
- en: Cisco switches speak NetFlow version 7, so you might as well use it. Turn it
    on with **`ip flow-export version 7`**. Finally, you tell the switch where to
    send the flow data, to `10.10.10.10`, port `5678`.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've completed your configuration, save your work. Data should arrive
    at your collector almost immediately. As with a Cisco router, use the command
    **`show ip cache flow`** to see current flow statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Juniper Routers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Juniper configurations are much longer than Cisco configurations, but the configuration
    isn't that difficult. Start by telling the Juniper how to sample flow traffic
    and where you want the flows sent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Despite all the parentheses, this configuration isn't that hard to understand.
    You begin at ❶ by defining how heavily you want to sample flows. In this example,
    I've set the sampling rate to 1\. This Juniper samples 1 flow out of every 1,
    or all, flows. If you were to set the sampling rate to 100, the Juniper would
    sample 1 out of every 100 packets. This example betrays my personal bias; I record
    all traffic, if possible. If complete traffic recording overloads your router,
    increase the sampling level until the hardware can support it. Those of you with
    10GB networks almost certainly must sample! The only way to determine the lowest
    sustainable sampling rate on your network is through experimentation. Sampling
    too aggressively overloads the router and causes packet loss.
  prefs: []
  type: TYPE_NORMAL
- en: As you might remember, cflowd is an obsolete flow collector. Juniper hardware
    uses that name to specify where the router will send flows. At ❷ you tell the
    Juniper where to find your flow collector, the UDP port (❸) the collector runs
    on, and what address on the router (❹) to send packets from.
  prefs: []
  type: TYPE_NORMAL
- en: Now enable sampling on the interface(s) you want to capture flow data from.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This configuration sends the incoming and outgoing traffic on this interface
    to the flow export (or sampling) engine.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Software Flow Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you need flow export on a network but don't have any switches that support
    flow export. Never fear, you can implement a flow sensor in software instead of
    buying new hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Using software instead of hardware for flow sensing has limitations. For one,
    software needs to run on a server, so it adds another device to your network.
    Also, you can only capture data from a managed Ethernet switch. The software might
    not capture all the traffic, and it might not tell you when it misses something.
    And flow sensor software is yet another thing that the network administrator must
    configure, patch, and maintain. Don't you have enough to do already? Still, for
    a small network, software is the only realistic choice.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Sensor Server Hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flow sensor software requires very few system resources. You can easily use
    a desktop that your management took out of production because it ran so slowly
    that the receptionist couldn't use it anymore. I've run effective flow sensors
    on tiny computers such as the Soekris ([http://www.soekris.com/](http://www.soekris.com/)).
    Even a modest machine with a 266MHz CPU and 128MB of RAM was mostly idle while
    capturing traffic off a low-traffic LAN.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever hardware you choose, your flow sensor machine needs at least two network
    interfaces. One interface must be capable of handling many packets per second.
    If you're recycling a discarded desktop machine with a network card embedded on
    the motherboard, assume that the built-in network card is not of sufficient quality.
    Very few desktop motherboard vendors use high-end network chipsets, preferring
    to use inexpensive chipsets that are just barely adequate for most users. If you're
    not sure what network card to buy, Intel's cards usually suffice for a flow sensor.
    The embedded network cards on server-grade systems are usually adequate; they're
    not great, but they're adequate.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you recycle an old machine for flow sensor hardware, get a new hard
    drive. Flow sensors use very little hard drive space, but the hard drive is the
    system component most likely to fail with age. You don't want to install your
    flow sensor just to see your work evaporate in a few weeks because the hard drive
    shattered! (Senior systems administrators might consider building a flow sensor
    bootable CD out of a tool kit such as FreeSBIE so that you can mass-produce easily
    replaceable sensors.)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I recommend using the same operating system on both your flow sensor
    server and your collector. This is certainly not a requirement, but running multiple
    operating systems in production just because you can is a symptom of having too
    much time on your hands.
  prefs: []
  type: TYPE_NORMAL
- en: Network Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a software flow sensor requires that you have a switch capable of *port
    mirroring* or *port monitoring*. This is often called a *sniffer port*. The switch
    copies all traffic crossing the switch to the monitor interface. You might verify
    this with a packet sniffer before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: To configure a monitor port on a Cisco system, define the interfaces you want
    to monitor and the interface to which you want to direct traffic. In the following
    example, a Cisco switch is copying all traffic from VLAN 1 into interface Gi0/4\.
    (Many switch vendors also offer web-based configuration tools.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You start at ❶ by defining a monitor session and telling the switch which VLAN
    or ports to monitor. Then at ❷ you tell the switch where to replicate the monitored
    traffic. This small switch will now copy all traffic crossing the main VLAN to
    the monitor port.
  prefs: []
  type: TYPE_NORMAL
- en: A switch that can export flows should be large enough to allow you to connect
    all of your primary servers to it so that all critical traffic crosses that big
    switch and your flow exports capture pretty much everything vital. If you use
    a few smaller switches instead of one big one, ensure that the monitor port captures
    traffic from all critical servers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember, any traffic that remains on a closet switch will not be captured.
    You could deploy a flow sensor on every switch, but that quickly becomes cumbersome.
  prefs: []
  type: TYPE_NORMAL
- en: Sensor Server Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A switch will not accept regular traffic on a monitor port, so the sensor machine
    cannot use the monitor port for its network traffic. (Go ahead, try it.) Sniffer
    ports are only for watching the network, not participating in the network. Your
    sensor box needs a second network card for regular network activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux and BSD systems, you must activate the sniffer interface before it
    can sense traffic. It''s also wise to disable ARP on the interface (with the `up`
    and `-arp` commands) so that your sniffer interface doesn''t even try to participate
    in the network. Here, you start the interface em0 and disable ARP on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Substitute your sniffer interface for em0, and then use `tcpdump` to verify
    that you're seeing network traffic on your sniffer interface.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You should see traffic records streaming across the screen. Press **ctrl**-C
    to stop the output.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Sensor on the Collector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your collector runs on higher-end hardware and you don't need many sensors,
    you might consider running the sensor on the collector machine. Add another network
    interface to the machine, have the sensor run on that interface, then configure
    the sensor to transmit to a UDP port on the localhost IP (127.0.0.1), and have
    the collector accept connections only from 127.0.0.1.
  prefs: []
  type: TYPE_NORMAL
- en: This machine becomes doubly security sensitive, however. Not only does the collector
    have a historical record of traffic through your network, but it can view all
    current network activity. Secure this machine!
  prefs: []
  type: TYPE_NORMAL
- en: 'The Sensor: softflowd'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, an Internet search will reveal many different software flow sensors,
    but I recommend `softflowd`, from [http://www.mindrot.org/softflowd.html](http://www.mindrot.org/softflowd.html).
    Go there to identify the current version; if you can find a current package for
    your operating system, use it. If you cannot find a package, download the source
    code, and install the program by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the `softflowd` README file for instructions on how to build. Much like
    flow-tools, `softflowd` has a `configure` script. I prefer to install my add-on
    software in a different directory than the main system so I can easily keep it
    separate. Here I install `softflowd` under */usr/local/softflowd*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This installs two programs (`softflowd` and `softflowctl`), a man page for each
    program, and a README file.
  prefs: []
  type: TYPE_NORMAL
- en: Running softflowd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run `softflowd`, you must know the interface `softflowd` will listen to,
    the collector''s hostname or IP address, and the port the collector is running
    on. Here, I''m having `softflowd` listen to the port em0 and transmit data to
    10.10.10.10 on port 5678:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `-t maxlife=300` in the middle of the command sets the flow timeout to 300
    seconds, or five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '`softflowd` is more flexible than a hardware flow sensor, which means you have
    more opportunities to misconfigure it. The manual page has a complete list of
    ways you can adjust `softflowd`''s performance, but most of them are not useful
    in most environments. The one option that might be helpful, though, is the ability
    to change the upper limit on the number of flows `soft-flowd` will track at any
    one time.'
  prefs: []
  type: TYPE_NORMAL
- en: By default `softflowd` tracks up to 8,192 flows at once, using about 800KB of
    memory. That's an awful lot of simultaneous connections, but if you're monitoring
    a high-bandwidth link, you might need to raise it. (The next section shows how
    to see how many flows `softflowd` is actually tracking at any given time.) Use
    the **`-m`** flag to specify the upper limit on the number of tracked flows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This example tracks twice the normal number of flows.
  prefs: []
  type: TYPE_NORMAL
- en: Watching softflowd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `softflowctl` program allows the network administrator to manage a running
    `softflowd` process. You can use it to start and stop collecting flows, view currently
    tracked flows, and flush all flows being tracked to the collector. The softflowctl
    manual page gives instructions for all of softflowctl's features, but I'll cover
    only the functions used in normal circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: The easy `softflowctl` commands are `shutdown` and `exit`. To tell `softflowd`
    to export all the flows it's currently tracking to the collector and shut itself
    down, use the `shutdown` command. This is the recommended way to terminate `softflowd`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If you want `softflowd` to shut itself off without sending any additional flows
    to the collector, use the `exit` command. You will lose any data `softflowd` has
    gathered but not exported.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The more interesting features of `softflowctl` include the ability to view currently
    tracked flows and the ability to view the `softflowd`'s internal statistics on
    tracked flows.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing Tracked Flows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to see the flows that `softflowd` is currently tracking, use `softflowctl`'s
    `dump-flows` command. Each line is a single flow. If you see flows, `softflowd`
    is working.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This flow involves a host with an IP of 192.0.2.253 (❶) and port 4234 (❷), and
    the other end of the connection is the host 239.255.255.250 (❸) on port 1900 (❹).
    This flow uses protocol 17 (❺), or UDP. Although you can see timing information
    as well, you'll have other ways to view this data more conveniently.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing Flow Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`softflowd` starts tracking flows immediately upon starting. You can use `softflowctl`
    to query `softflowd` about the characteristics of the flows it is currently tracking
    with the `statistics` command, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: I'll break up the lengthy output from this command into a few sections to make
    it easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line at ❶ includes one interesting number: the process ID of the
    `softflowd` process you''re querying. If you''re running multiple instances of
    `softflowd` on one machine, this will help you verify that you''re querying the
    correct one.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of active flows at ❷ tells you how many flows `softflowd` thinks
    are ongoing. This is not the same as the number of active connections on your
    network at the moment. Remember, a sensor tracks a flow unless either it has reason
    to think that the flow has ended or the timeout expires. A flow from a DNS or
    UDP NFS request remains in `softflowd`'s list of tracked flows until the timeout
    expires, which may be long after the corresponding network activity has ceased.
  prefs: []
  type: TYPE_NORMAL
- en: The number of packets processed (❸) should always increase, and the number of
    fragments (❹) probably will increase but less quickly.
  prefs: []
  type: TYPE_NORMAL
- en: The number of packets dropped (❺) is much more interesting. Every network tends
    to have "odd stuff " fluttering across it now and then. Flow management only handles
    TCP/IP and its related protocols, including IPv6, SCTP, and other advanced protocols.
    When `softflowd` encounters packets that are not part of TCP/IP, such as DECNet,
    it counts them but doesn't otherwise process them. Dropped packets might come
    from bad hardware, badly scrambled TCP/IP stacks, unusual network protocols, or
    any other weird error. You might try `netstat -s` to get an idea about which part
    of your system is dropping packets, and why. `softflowd` also drops packets that
    are too short to contain actual data.
  prefs: []
  type: TYPE_NORMAL
- en: You can see at ❻ that the number of flows `softflowd` has expired, as well as
    at ❼ how many it has exported to the collector. These numbers do not necessarily
    equal each other, because the former is a `softflowd` internal statistic. The
    number of flows exported should match the number received on the collector, however.
    If you see that `softflowd` has forced some flows to expire, use `-m` to increase
    the number of flows `softflowd` may track.
  prefs: []
  type: TYPE_NORMAL
- en: Libpcap is the packet-capture software `softflowd` uses. At ❽, the number of
    packets libpcap receives should be roughly comparable to the number of packets
    `softflowd` has processed. At ❾, the number of packets dropped by libpcap should
    be very low for proper data capture. If this number is increasing, investigate.
    Your operating system or your hardware might not be adequate to the load being
    placed on it.
  prefs: []
  type: TYPE_NORMAL
- en: It's also possible that your network card might not be up to the task of capturing
    all the data from your network. Finally, at ❿, the packets dropped by interface
    counter should be zero, or at least very low.
  prefs: []
  type: TYPE_NORMAL
- en: The output continues with information about the expired flows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The smallest flow (❶) had 221 bytes. The smallest number of packets in a flow
    (❷) was 1\. The briefest flow (❸) lasted less than one one-hundredth of a second.
  prefs: []
  type: TYPE_NORMAL
- en: You can make some guesses based on this data. For example, it's likely but not
    certain that the 221-byte flow was in a single packet, and it probably was the
    briefest as well. You might have many flows containing a small number of bytes
    in a single packet, though, and it's possible that the 221-byte flow was broken
    into multiple small packets. To reach any conclusions about particular flows,
    you must perform more detailed analysis. Treat the maximum values similarly; the
    flow with the greatest number of bytes was not necessarily the flow with the greatest
    number of packets.
  prefs: []
  type: TYPE_NORMAL
- en: The average flow (❹) was 263,570 bytes, or roughly 257KB, and lasted about 22
    seconds (❺). Again, you cannot assume that the average-sized flows are the same
    as the flows with the average number of packets or the flows of average duration.
  prefs: []
  type: TYPE_NORMAL
- en: The statistics end with a count of the reasons why `softflowd` expired and exported
    each flow and a per-protocol statistics list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: TCP and UDP have their own entries, including the specific reasons why flows
    were expired. For example, as shown at ❶, TCP flows can be expired by a TCP RST
    indicating that a port is closed or by the TCP FIN (❷) that marks the end of a
    normal session. Similarly, a UDP request to a closed port can generate an ICMP
    response, as shown at ❸, or it might just time out.
  prefs: []
  type: TYPE_NORMAL
- en: '`softflowctl` also displays the number of flows expired because the timeout
    elapsed (❹) and the number expired because they exceeded the maximum size of a
    single flow (❺).'
  prefs: []
  type: TYPE_NORMAL
- en: The `softflowd` program tracks a maximum number of flows simultaneously. If
    `softflowd` detects more flows than it can track, it expires older idle flows
    until it has sufficient capacity to track active flows. If the maxflows number
    (❼) begins climbing, increase the number of flows `softflowd` can track at any
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the flushed entry (❽) shows how many flows were expired when the administrator
    ran `softflowctl expire-all`.
  prefs: []
  type: TYPE_NORMAL
- en: The statistics section ends with per-protocol flow information.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've gathered some data, you'll learn how to look at it in [Chapter 3](ch03.html
    "Chapter 3. VIEWING FLOWS").
  prefs: []
  type: TYPE_NORMAL
