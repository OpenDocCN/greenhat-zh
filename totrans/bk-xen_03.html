<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;PROVISIONING DOMUS"><div class="titlepage"><div><div><h1 class="title"><a id="provisioning_domus"/>Chapter 3. PROVISIONING DOMUS</h1></div></div></div><div class="epigraph"><p><span class="emphasis"><em>You can suck Linux right out of the air, as it were, by downloading the right files and putting them in the right places, but there probably are not more than a few hundred people in the world who could create a functioning Linux system in that way</em></span>.</p><div class="attribution"><span>—<span class="attribution"><span class="author"><span class="firstname">Neal</span> <span class="surname">Stephenson,</span></span>
<em class="citation">In the Beginning Was the Command Line</em>
</span></span></div></div><div class="informalfigure"><div class="mediaobject"><a id="I_mediaobject3_d1e1936"/><img src="httpatomoreillycomsourcenostarchimages333191.png.jpg" alt="image with no caption"/></div></div><p>Up until now, we've focused on administering the dom0, leaving the specifics of domU creation up to the <code class="literal">virt-install</code> tool. However, you'll probably need to build a domU image <a id="idx-CHP-3-0145" class="indexterm"/>from scratch on occasion. There are plenty of good reasons for this—perhaps you want an absolutely minimal Linux environment to use as a base for virtual private server (VPS) hosting setups. Maybe you're deploying some custom application—a <span class="emphasis"><em>server appliance</em></span>—using Xen. It might just seem like a good way to keep systems patched. Possibly you need to create Xen instances without the benefit of a network connection.<a id="idx-CHP-3-0146" class="indexterm"/></p><p>Just as there are many reasons to want custom filesystem images, there are many ways to make the images. We'll give detailed instructions for some that we use frequently, and briefly mention some others, but it would be impossible to provide an exhaustive list (and very boring besides). The goal of this chapter is to give you an idea of the range of options you have in provisioning domU filesystems, a working knowledge of the principles, and just enough step-by-step instruction to get familiar with the processes.</p><div class="sect1" title="A Basic DomU Configuration"><div class="titlepage"><div><div><h1 class="title"><a id="a_basic_domu_configuration"/>A Basic DomU Configuration</h1></div></div></div><p>All of the examples that we're presenting here should work with a <a id="idx-CHP-3-0147" class="indexterm"/>basic—in fact, downright skeletal—domU config file. Something along the lines of this should work:</p><a id="I_programlisting3_d1e1971"/><pre class="programlisting"><a id="idx-CHP-3-0148" class="indexterm"/>kernel = /boot/vmlinuz-2.6-xen.gz
vif = ['']
<a id="idx-CHP-3-0149" class="indexterm"/>disk = ['phy:/dev/targetvg/lv,sda,w']</pre><p>This specifies a kernel, a <a id="idx-CHP-3-0150" class="indexterm"/>network interface, and a disk, and lets Xen use defaults for everything else. Tailor the variables, such as volume group and kernel name, to your site. As we mention elsewhere, we recommend <a id="idx-CHP-3-0151" class="indexterm"/>including other variables, such as a MAC and IP address, but we'll omit them during this chapter for clarity so we can focus on creating domU images.</p><div class="note" title="Note"><h3 class="title"><a id="note-10"/>Note</h3><p><span class="emphasis"><em>This doesn't include a ramdisk. Either add a</em></span> <a id="idx-CHP-3-0152" class="indexterm"/><em class="replaceable"><code>ramdisk=</code></em> <span class="emphasis"><em>line or include</em></span> <a id="idx-CHP-3-0153" class="indexterm"/><em class="replaceable"><code>xenblk</code></em> <span class="emphasis"><em>(and</em></span> <em class="replaceable"><code>xennet</code></em> <span class="emphasis"><em>if you plan on accessing the network before modules are available) in your kernel. When we compile our own kernels, we usually include the</em></span> <em class="replaceable"><code>xenblk</code></em> <span class="emphasis"><em>and</em></span> <em class="replaceable"><code>xennet</code></em> <span class="emphasis"><em>drivers directly in the kernel. We only use a ramdisk to satisfy the requirements of the distro kernels</em></span>.</p></div><p>If you're <a id="idx-CHP-3-0154" class="indexterm"/>using a <a id="idx-CHP-3-0155" class="indexterm"/>modular kernel, which is very likely, you'll also need to ensure that the kernel has a matching set of modules that it can load from the domU filesystem. If you're <a id="idx-CHP-3-0156" class="indexterm"/>booting the domU using the same kernel as the dom0, you can copy over the modules like this (if the domU image is mounted on <span class="emphasis"><em>/mnt</em></span>):</p><a id="I_programlisting3_d1e2057"/><pre class="programlisting"># mkdir -p /mnt/lib/modules
# cp -a /lib/modules/`uname -r` /mnt</pre><p>Note that this command only works if the domU kernel is the same as the dom0 kernel! Some install procedures will install the correct modules automatically; others won't. No matter how you create the domU, remember that modules need to be accessible from the domU, even if the kernel lives in the dom0. If you have trouble, make sure that the kernel and module versions match, either by booting from a different kernel or copying in different modules.</p></div></div>
<div class="sect1" title="Selecting a Kernel"><div class="titlepage"><div><div><h1 class="title"><a id="selecting_a_kernel"/>Selecting a Kernel</h1></div></div></div><p>Traditionally, one boots a domU image using a kernel stored in the dom0 filesystem, as in the sample config file in the last section. In this case, it's common to use the same kernel <a id="idx-CHP-3-0157" class="indexterm"/>for domUs and the dom0. However, this can lead to trouble—one distro's kernels may be too specialized to work properly with another distro. We recommend either using the proper distro kernel, copying it into the dom0 filesystem so the domain builder can find it, or compiling your own generic kernel.</p><p>Another possible choice is to download Xen's binary distribution, which includes precompiled domU kernels, and extracting an appropriate domU kernel from that.</p><p>Alternatively (and this is the option that we usually use when dealing with distros that ship Xen-aware kernels), you can bypass the entire problem of <a id="idx-CHP-3-0158" class="indexterm"/>kernel selection and use PyGRUB to boot the distro's own kernel from within the <a id="idx-CHP-3-0159" class="indexterm"/>domU filesystem. <a id="idx-CHP-3-0160" class="indexterm"/>For more details on PyGRUB, see <a class="xref" href="ch07.html" title="Chapter 7. HOSTING UNTRUSTED USERS UNDER XEN: LESSONS FROM THE TRENCHES">Chapter 7</a>. PyGRUB also makes it more intuitive to match modules to kernels by keeping both the domU kernel and its corresponding modules in the domU.</p></div>
<div class="sect1" title="Quick-and-Dirty Install via tar"><div class="titlepage"><div><div><h1 class="title"><a id="quick-and-dirty_install_via_tar"/>Quick-and-Dirty Install via tar</h1></div></div></div><p>Let's start by considering the most basic install method possible, just to get an idea of the principles involved. We'll generate a <a id="idx-CHP-3-0161" class="indexterm"/>root filesystem by copying files out of the dom0 (or an entirely separate physical machine) and into the domU. This approach copies out a filesystem known to work, requires no special tools, and is easy to debug. However, it's also likely to pollute the domU with a lot of unnecessary stuff from the source system and is kind of a lot of work.<a id="idx-CHP-3-0162" class="indexterm"/></p><p>A good set of commands for this "cowboy" approach might be:</p><a id="I_programlisting3_d1e2108"/><pre class="programlisting"># xm block-attach 0 duncan.img /dev/xvda1 w 0
# mke2fs -j /dev/xvda1
# mount /dev/xvda1 /mnt
# cd /
# tar -c -f - --exclude /home --exclude /mnt --exclude /tmp --exclude \
    /proc --exclude /sys --exclude /var | ( cd /mnt/ ; tar xf - )
# mkdir /mnt/sys
# mkdir /mnt/proc</pre><div class="note" title="Note"><h3 class="title"><a id="note-11"/>Note</h3><p><span class="emphasis"><em>Do all this as root</em></span>.</p></div><p>These commands, in order, map the backing file to a virtual device in the dom0, create a filesystem on that device, mount the filesystem, and tar up the dom0 root directory while omitting <span class="emphasis"><em>/home</em></span>, <span class="emphasis"><em>/mnt</em></span>, <span class="emphasis"><em>/tmp</em></span>, <span class="emphasis"><em>/proc</em></span>, <span class="emphasis"><em>/sys</em></span>, and <span class="emphasis"><em>/var</em></span>. The output from this <code class="literal">tar</code> command then goes to a complementary <code class="literal">tar</code> used to extract the file in <span class="emphasis"><em>/mnt</em></span>. Finally, we make some directories that the domU will need after it boots. At the end of this process, we have a self-contained domU in <em class="filename">duncan.img</em>.</p><div class="sect2" title="Why This Is Not the Best Idea"><div class="titlepage"><div><div><h2 class="title"><a id="why_this_is_not_the_best_idea"/>Why This Is Not the Best Idea</h2></div></div></div><p>The biggest problem with the <a id="idx-CHP-3-0163" class="indexterm"/>cowboy approach, apart from its basic inelegance, is that it copies a lot of unnecessary stuff with no easy way to clear it out. When the domU is booted, you could use the package manager to remove things or just delete files by hand. But that's work, and we are all about avoiding work.</p></div><div class="sect2" title="Stuff to Watch Out For"><div class="titlepage"><div><div><h2 class="title"><a id="stuff_to_watch_out_for"/>Stuff to Watch Out For</h2></div></div></div><p>There are some things <a id="idx-CHP-3-0164" class="indexterm"/>to note:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>You <span class="emphasis"><em>must</em></span> <code class="literal">mkdir /sys</code> and <code class="literal">/proc</code> or else things will not work properly.<a id="idx-CHP-3-0165" class="indexterm"/></p><p>The issue here is that the Linux <a id="idx-CHP-3-0166" class="indexterm"/>startup process uses <span class="emphasis"><em>/sys</em></span> and <span class="emphasis"><em>/proc</em></span> to discover and configure hardware—if, say, <span class="emphasis"><em>/proc/mounts</em></span> doesn't exist, the boot scripts will become extremely annoyed.</p></li><li class="listitem"><p>You may need to <code class="literal">mknod /dev/xvda b 220 0</code>.<a id="idx-CHP-3-0167" class="indexterm"/></p><p><span class="emphasis"><em>/dev/xvd</em></span> is the <a id="idx-CHP-3-0168" class="indexterm"/>standard name for Xen virtual disks, by analogy with the <span class="emphasis"><em>hd</em></span> and <span class="emphasis"><em>sd</em></span> device nodes. The first virtual disk is <span class="emphasis"><em>/dev/xvda</em></span>, which can be partitioned into <span class="emphasis"><em>/dev/xvda1</em></span>, and so on. The command</p><a id="I_programlisting3_d1e2230"/><pre class="programlisting"># /mknod /dev/xvda b 220 0</pre><p>creates the node <span class="emphasis"><em>/dev/xvda</em></span> as a block device (b) with major number 220 (the number reserved for Xen VBDs) and minor number 0 (because it's <span class="emphasis"><em>xvda</em></span>—the first such device in the system).</p><div class="note" title="Note"><h3 class="title"><a id="note-12"/>Note</h3><p><span class="emphasis"><em>On most modern Linux systems, udev makes this unnecessary</em></span>.</p></div></li><li class="listitem"><p>You may need to edit <span class="emphasis"><em>/etc/inittab</em></span> and <span class="emphasis"><em>/etc/securettys</em></span> so that <span class="emphasis"><em>/dev/xvc0</em></span> works as the console and has a proper <code class="literal">getty</code>.<a id="idx-CHP-3-0169" class="indexterm"/><a id="idx-CHP-3-0170" class="indexterm"/></p><p>We've noticed this problem only with Red Hat's kernels: for regular XenSource kernels (at least through 3.1) the default <code class="literal">getty</code> on tty0 should work without further action on your part. If it doesn't, read on!</p><p>The term <span class="emphasis"><em>console</em></span> is something of a holdover from the days of giant time-sharing machines, when the system operator sat at a dedicated terminal called the <span class="emphasis"><em>system console</em></span>. Nowadays, the console is a device that receives system administration messages—usually a graphics device, sometimes a serial console.</p><p>In the Xen case, all output goes to the Xen <a id="idx-CHP-3-0171" class="indexterm"/>virtual console, <code class="literal">xvc0</code>. The <code class="literal">xm console</code> command attaches to this device with help from <code class="literal">xenconsoled</code>. To log in to it, Xen's <a id="idx-CHP-3-0172" class="indexterm"/>virtual console must be added to <span class="emphasis"><em>/etc/inittab</em></span> so that <code class="literal">init</code> knows to attach a <code class="literal">getty</code>.<sup>[<a id="CHP-3-FNOTE-1" href="#ftn.CHP-3-FNOTE-1" class="footnote">17</a>]</sup> Do this by adding a line like the following:<a id="idx-CHP-3-0173" class="indexterm"/></p><a id="I_programlisting3_d1e2317"/><pre class="programlisting">xvc:2345:respawn:/sbin/agetty -L xvc0</pre><p>(As with all examples in books, don't take this construction too literally! If you have a differently named <code class="literal">getty</code> binary, for example, you will definitely want to use that instead.)</p><p>You might also, depending on your policy regarding <a id="idx-CHP-3-0174" class="indexterm"/>root logins, want to add <span class="emphasis"><em>/dev/xvc0</em></span> to <span class="emphasis"><em>/etc/securetty</em></span> so that root will be able to log in on it. Simply append a line containing the device name, <span class="emphasis"><em>xvc0</em></span>, to the file.</p></li></ul></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-1" href="#CHP-3-FNOTE-1" class="para">17</a>] </sup><code class="literal">getty</code> gives you a login prompt. What, you didn't think they showed up by magic, did you?</p></div></div></div>
<div class="sect1" title="Using the Package Management System with an Alternate Root"><div class="titlepage"><div><div><h1 class="title"><a id="using_the_package_management_system_with"/>Using the Package Management System with an Alternate Root</h1></div></div></div><p>Another way to obtain a domU image would be to just run the setup program for your distro of choice and instruct it to install to the mounted domU root. The disadvantage here is that most setup programs expect to be installed on a real machine, and they become surly and uncooperative when forced to deal with paravirtualization.</p><p>Nonetheless, this is a viable process for most installers, including both RPM and Debian-based distros. We'll describe installation using both Red Hat's and Debian's tools.</p><div class="sect2" title="Red Hat, CentOS, and Other RPM-Based Distros"><div class="titlepage"><div><div><h2 class="title"><a id="red_hat_centos_and_other_rpm-based_distr"/>Red Hat, CentOS, and Other RPM-Based Distros</h2></div></div></div><p>On Red Hat–derived <a id="idx-CHP-3-0175" class="indexterm"/>systems, we treat this as a <span class="emphasis"><em>package</em></span> installation, rather than a <span class="emphasis"><em>system installation</em></span>. Thus, rather than using <code class="literal">anaconda</code>, the system installer, we use <code class="literal">yum</code>, which has an installation mode suitable for this sort of thing.<a id="idx-CHP-3-0176" class="indexterm"/></p><p>First, it's easiest to make sure that <a id="idx-CHP-3-0177" class="indexterm"/>SELinux is disabled or nonenforcing because its extended permissions and policies don't work well with the installer.<sup>[<a id="CHP-3-FNOTE-2" href="#ftn.CHP-3-FNOTE-2" class="footnote">18</a>]</sup> The quickest way to do this is to issue <code class="literal">echo 0 &gt;/selinux/enforce</code>. A more permanent solution would be to boot with <code class="literal">selinux=0</code> on the kernel command line.</p><div class="note" title="Note"><h3 class="title"><a id="note-13"/>Note</h3><p><span class="emphasis"><em>Specify kernel parameters as a space-separated list on the "module" line that loads the Linux kernel—either in</em></span> /boot/grub/menu.lst <span class="emphasis"><em>or by pushing</em></span> e <span class="emphasis"><em>at the GRUB menu</em></span>.</p></div><p>When that's done, mount your <a id="idx-CHP-3-0178" class="indexterm"/>target domU image somewhere appropriate. Here we create the <a id="idx-CHP-3-0179" class="indexterm"/>logical volume <span class="emphasis"><em>malcom</em></span> in the volume group <span class="emphasis"><em>scotland</em></span> and mount it on <span class="emphasis"><em>/mnt</em></span>:</p><a id="I_programlisting3_d1e2419"/><pre class="programlisting"># lvcreate -L 4096 -n malcom scotland
# mount /dev/scotland/malcom /mnt/</pre><p>Create some vital directories, just as in the <code class="literal">tar</code> example:</p><a id="I_programlisting3_d1e2426"/><pre class="programlisting"># cd /mnt
# mkdir proc sys etc</pre><p>Make a basic <code class="literal">fstab</code> (you can just copy the one from dom0 and edit the root device as appropriate—with the sample config file mentioned earlier, you would use <span class="emphasis"><em>/dev/sda</em></span>):<a id="idx-CHP-3-0180" class="indexterm"/></p><a id="I_programlisting3_d1e2439"/><pre class="programlisting"># cp /etc/fstab /mnt/etc
# vi /mnt/etc/fstab</pre><p>Fix <em class="filename">modprobe.conf</em>, so that the kernel knows where to find its device drivers. (This step isn't technically necessary, but it enables <code class="literal">yum upgrade</code> to properly build a new initrd when the kernel changes—handy if you're <a id="idx-CHP-3-0181" class="indexterm"/>using PyGRUB.)</p><a id="I_programlisting3_d1e2456"/><pre class="programlisting"># echo "alias scsi_hostadapter xenblk\nalias eth0 xennet" &gt; /mnt/etc/modprobe.conf</pre><p>At this point you'll need an <a id="idx-CHP-3-0182" class="indexterm"/>RPM that describes the software release version and creates the <code class="literal">yum</code> <a id="idx-CHP-3-0183" class="indexterm"/>configuration files—we installed CentOS 5, so we used <code class="literal">centos-release-5.el5.centos.i386.rpm</code>.</p><a id="I_programlisting3_d1e2476"/><pre class="programlisting"># wget http://mirrors.prgmr.com/os/centos/5/os/i386/CentOS/centos-release-5.el5.centos.i386.rpm
# rpm -ivh --nodeps --root /mnt centos-release-5.el5.centos.i386.rpm</pre><p>Normally, the CentOS release RPM includes the minor version number, but it is hard to find old versions. See the <em class="filename">README.prgmr</em> file in the same directory <a id="idx-CHP-3-0184" class="indexterm"/>for a full explanation.</p><p>Next we install <code class="literal">yum</code> under the new install tree. If we don't do this before <a id="idx-CHP-3-0185" class="indexterm"/>installing other packages, <code class="literal">yum</code> will complain about transaction errors:</p><a id="I_programlisting3_d1e2503"/><pre class="programlisting"># yum --installroot=/mnt -y install yum</pre><p>Now that the directory has been appropriately populated, we can use <code class="literal">yum</code> to finish the install.</p><a id="I_programlisting3_d1e2510"/><pre class="programlisting"># yum --installroot=/mnt -y groupinstall Base</pre><p>And that's really all there is to it. Create a domU config file as normal.</p></div><div class="sect2" title="Debootstrap with Debian and Ubuntu"><div class="titlepage"><div><div><h2 class="title"><a id="debootstrap_with_debian_and_ubuntu"/>Debootstrap with Debian and Ubuntu</h2></div></div></div><p>Debootstrap is quite a bit easier. Create a target for the install (using LVM or a flat file), mount it, and then use debootstrap to install a base system into that directory. For example, to install <a id="idx-CHP-3-0186" class="indexterm"/>Debian Etch on an x68_64 machine:<a id="idx-CHP-3-0187" class="indexterm"/></p><a id="I_programlisting3_d1e2526"/><pre class="programlisting"># mount /dev/scotland/banquo /mnt
# debootstrap --include=ssh,udev,linux-image-xen-amd64 etch /mnt http://mirrors.easynews.com/
linux/debian</pre><p>Note the <code class="literal">--include=</code> option. Because Xen's networking requires the hot-plug system, the domU must include a working install of udev with its support scripts. (We've also included SSH, just for convenience and to demonstrate the syntax for multiple items.) If you are on an i386 platform, add libc6-xen to the include list. Finally, to ensure that we have a compatible kernel and module set, we add a suitable kernel to the <code class="literal">include=</code> list. We use <code class="literal">linux-image-xen-amd64</code>. Pick an appropriate kernel for your hardware.</p><p>If you want to use PyGRUB, create <span class="emphasis"><em>/mnt/etc/modules</em></span> before you run <code class="literal">debootstrap</code>, and put in that file:<a id="idx-CHP-3-0188" class="indexterm"/></p><a id="I_programlisting3_d1e2550"/><pre class="programlisting">xennet
xenblk</pre><p>Also, create a <span class="emphasis"><em>/mnt/boot/grub/menu.lst</em></span> file as for a physical machine.</p><p>If you're not planning to use PyGRUB, make sure that an appropriate <a id="idx-CHP-3-0189" class="indexterm"/>Debian kernel and ramdisk are accessible from the dom0, or make sure that modules matching your planned kernel are available <a id="idx-CHP-3-0190" class="indexterm"/>within the domU. In this case, we'll copy the sdom0 kernel modules into the domU.</p><a id="I_programlisting3_d1e2569"/><pre class="programlisting"># cp -a /lib/modules/&lt;domU kernel version&gt; /mnt/lib/modules</pre><p>When that's done, copy over <code class="literal">/etc/fstab</code> to the new system, editing it if necessary:</p><a id="I_programlisting3_d1e2576"/><pre class="programlisting"># cp /etc/fstab /mnt/etc</pre><div class="sect3" title="Renaming Network Devices"><div class="titlepage"><div><div><h3 class="title"><a id="renaming_network_devices"/>Renaming Network Devices</h3></div></div></div><p>Debian, like many systems, uses udev to tie <code class="literal">eth0</code> and <code class="literal">eth1</code> to consistent physical devices. It does this by assigning the device name (<code class="literal">ethX</code>) based on the MAC address of the Ethernet device. It will do this during debootstrap—this means that it ties <code class="literal">eth0</code> to the MAC of the box you are running <code class="literal">debootstrap</code> on. In turn, the domU's Ethernet interface, which presumably has a different MAC address, will become <code class="literal">eth1</code>.<sup>[<a id="CHP-3-FNOTE-3" href="#ftn.CHP-3-FNOTE-3" class="footnote">19</a>]</sup> You can avoid this by removing <span class="emphasis"><em>/mnt/etc/udev/rules.d/z25_persistent-net.rules</em></span>, which contains the stored mappings between MAC addresses and device names. That file will be recreated next time you reboot. If you only have one interface, it might make sense to remove the file that generates it, <span class="emphasis"><em>/mnt/etc/udev/rules.d/z45_persistent-net-generator.rules</em></span>.<a id="idx-CHP-3-0191" class="indexterm"/><a id="idx-CHP-3-0192" class="indexterm"/><a id="idx-CHP-3-0193" class="indexterm"/></p><a id="I_programlisting3_d1e2626"/><pre class="programlisting"># rm /mnt/etc/udev/rules.d/z25_persistent-net.rules</pre><p>Finally, unmount the install root. Your system should then essentially work. You may want to change the hostname and edit <span class="emphasis"><em>/etc/inittab</em></span> within the domU's filesystem, but these are purely optional steps.</p><a id="I_programlisting3_d1e2633"/><pre class="programlisting"># umount /mnt</pre><p>Test the new install by creating a config file as previously described (say, <span class="emphasis"><em>/etc/xen/banquo</em></span>) and issuing:</p><a id="I_programlisting3_d1e2640"/><pre class="programlisting"># xm create -c /etc/xen/banquo</pre></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-2" href="#CHP-3-FNOTE-2" class="para">18</a>] </sup>Although we don't really approve of the tendency to disable SELinux at the first hint of trouble, we decided to take the path of least resistance.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-3" href="#CHP-3-FNOTE-3" class="para">19</a>] </sup>Or another device, depending on how many Ethernet devices the original machine had.</p></div></div></div>
<div class="sect1" title="QEMU Install"><div class="titlepage"><div><div><h1 class="title"><a id="qemu_install"/>QEMU Install</h1></div></div></div><p>Our favorite way to create the domU image—the way that most closely simulates a real machine—is probably to install using QEMU and then take the installed filesystem and use that as your domU root filesystem. This allows you, the installer, to leverage your years of experience <a id="idx-CHP-3-0194" class="indexterm"/>installing Linux. Because it's installing in a virtual machine as strongly partitioned as Xen's, the install program is very unlikely to do anything surprising and even more unlikely to interact badly with the existing system. QEMU also works equally well with all distros and even non-Linux operating systems.</p><p>QEMU does have the disadvantage of being slow. Because <a id="idx-CHP-3-0195" class="indexterm"/>KQEMU (the kernel acceleration module) isn't compatible with Xen, you'll have to fall back to software-only full emulation. Of course, you can use this purely <a id="idx-CHP-3-0196" class="indexterm"/>for an initial image-creation step and then copy the pristine disk images around as needed, in which case the speed penalty becomes less important.</p><div class="sidebar"><a id="qemus_relation_to_xen"/><p class="title">QEMU'S RELATION TO XEN</p><p>You may already have noted that QEMU gets mentioned fairly often in connection with Xen. There's a good reason for this: The two projects complement each other. Although QEMU is a <span class="emphasis"><em>pure</em></span>, or <span class="emphasis"><em>classic</em></span>, full emulator, there's some overlap in QEMU's and Xen's requirements. For example, Xen can use QCOW images for its disk emulation, and it uses QEMU fully virtualized drivers when running in hardware virtualization mode. QEMU also furnishes some code for the hardware virtualization built into the Linux kernel, <a id="idx-CHP-3-0197" class="indexterm"/>KVM (kernel virtual machine)<sup>[<a id="CHP-3-FNOTE-4" href="#ftn.CHP-3-FNOTE-4" class="footnote">20</a>]</sup> and win4lin, on the theory that there's no benefit in reinventing the wheel.<a id="idx-CHP-3-0198" class="indexterm"/></p><p>Xen and QEMU aren't the same, but there's a general consensus that they complement each other well, with Xen more suited to high-performance production environments, and QEMU is aimed more at exact emulation. Xen's and QEMU's developers have begun sharing patches and working together. They're distinct projects, but Xen developers have acknowledged that QEMU "played a critical role in Xen's success."<sup>[<a id="CHP-3-FNOTE-5" href="#ftn.CHP-3-FNOTE-5" class="footnote">21</a>]</sup></p></div><p>This technique works by running QEMU as a pure emulator for the duration of the install, using emulated devices. Begin by getting and installing QEMU. Then run:</p><a id="I_programlisting3_d1e2709"/><pre class="programlisting"># qemu -hda /dev/scotland/macbeth -cdrom slackware-11.0-install-dvd.iso -boot d</pre><p>This command runs QEMU with the target device—a logical volume in this case—as its hard drive and the install medium as its virtual CD drive. (The Slackware ISO here, as always, is just an example—install whatever you like.) The <code class="literal">-boot d</code> option tells QEMU to boot from the emulated CD drive.</p><p>Now install to the virtual machine as usual. At the end, you should have a completely functional domU image. Of course, you're still going to have to create an appropriate domU config file and handle the other necessary configuration from the dom0 side, but all of that is reasonably easy to automate.</p><p>One last caveat that bears repeating because it applies to many of these install methods: If the domU kernel isn't Xen-aware, then you will have to either use a kernel from the dom0 or mount the domU and replace its kernel.</p><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-4" href="#CHP-3-FNOTE-4" class="para">20</a>] </sup>Although we don't cover KVM extensively, it's another interesting virtualization technology. More information is available at the KVM web page, <a class="ulink" href="http://kvm.sf.net/">http://kvm.sf.net/</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-5" href="#CHP-3-FNOTE-5" class="para">21</a>] </sup><a id="idx-CHP-3-0199" class="indexterm"/>Liguori, Anthony, "<a id="idx-CHP-3-0200" class="indexterm"/>Merging QEMU-DM upstream," <a class="ulink" href="http://www.xen.org/files/xensummit_4/Liguori_XenSummit_Spring_2007.pdf">http://www.xen.org/files/xensummit_4/Liguori_XenSummit_Spring_2007.pdf</a>.</p></div></div></div>
<div class="sect1" title="virt-install&#x2014;Red Hat's One-Step DomU Installer"><div class="titlepage"><div><div><h1 class="title"><a id="virt-installmdashred_hats_one-step_domu_"/>virt-install—Red Hat's One-Step DomU Installer</h1></div></div></div><p>Red Hat opted to support a generic <a id="idx-CHP-3-0201" class="indexterm"/>virtualization <span class="emphasis"><em>concept</em></span> rather than a specific <span class="emphasis"><em>technology</em></span>. Their approach is to wrap the virtualization in an abstraction layer, <a id="idx-CHP-3-0202" class="indexterm"/>libvirt. Red Hat then provides support software that uses this library to take the place of the virtualization package-specific control software.<sup>[<a id="CHP-3-FNOTE-6" href="#ftn.CHP-3-FNOTE-6" class="footnote">22</a>]</sup> (For information on the management end of libvirt, <code class="literal">virt-manager</code>, see <a class="xref" href="ch06.html" title="Chapter 6. DOMU MANAGEMENT: TOOLS AND FRONTENDS">Chapter 6</a>.)<a id="idx-CHP-3-0204" class="indexterm"/></p><p>For example, Red Hat includes <code class="literal">virsh</code>, a command-line interface that controls virtual machines. <code class="literal">xm</code> and <code class="literal">virsh</code> do much the same thing, <a id="idx-CHP-3-0205" class="indexterm"/>using very similar commands. The advantage of <code class="literal">virsh</code> and libvirt, however, is that the <code class="literal">virsh</code> interface will remain consistent if you decide to switch to another virtualization technology. Right now, for example, it can control QEMU and KVM in addition to Xen using a consistent set of commands.<a id="idx-CHP-3-0206" class="indexterm"/><a id="idx-CHP-3-0207" class="indexterm"/></p><p>The installation component of this system is <code class="literal">virt-install</code>. Like <code class="literal">virsh</code>, it builds on libvirt, which provides a platform-independent wrapper around different virtualization packages. No matter which virtualization backend you're using, <code class="literal">virt-install</code> works by providing an environment for the standard network install method: First it asks the user for configuration information, then it writes an appropriate config file, makes a virtual machine, loads a kernel from the install medium, and finally bootstraps a network install using the standard Red Hat installer, <code class="literal">anaconda</code>. At this point <code class="literal">anaconda</code> takes over, and installation proceeds as normal.</p><p>Unfortunately, this means that <code class="literal">virt-install</code> only works <a id="idx-CHP-3-0208" class="indexterm"/>with network-accessible Red Hat–style directory trees. (Other distros don't have the install layout that the installer expects.) If you're planning to standardize on Red Hat, CentOS, or Fedora, this is okay. Otherwise, it could be a serious problem.</p><p>Although <code class="literal">virt-install</code> is usually called from within Red Hat's <code class="literal">virt-manager</code> GUI, it's also an independent executable that you can use manually in an interactive or scripted mode. Here's a sample <code class="literal">virt-install</code> session, <a id="idx-CHP-3-0209" class="indexterm"/>with our inputs in bold.</p><a id="I_programlisting3_d1e2846"/><pre class="programlisting"># /usr/sbin/virt-install

Would you like a fully virtualized guest (yes or no)? This will allow you to
run unmodified operating systems.<strong class="userinput"><code> no</code></strong>

What is the name of your virtual machine?<strong class="userinput"><code> donalbain</code></strong>

How much RAM should be allocated (in megabytes)?<strong class="userinput"><code> 512</code></strong>

What would you like to use as the disk (path)?<strong class="userinput"><code> /mnt/donalbain.img</code></strong>

How large would you like the disk (/mnt/donalbain.img) to be (in gigabytes)?<strong class="userinput"><code> 4</code></strong>

Would you like to enable graphics support? (yes or no)<strong class="userinput"><code> no</code></strong>

What is the install location?
<strong class="userinput"><code>ftp://mirrors.easynews.com/linux/centos/4/os/i386/</code></strong></pre><p>Most of these inputs are self-explanatory. Note that the install location can be <span class="emphasis"><em>ftp://</em></span>, <span class="emphasis"><em>http://</em></span>, <span class="emphasis"><em>nfs</em></span>:, or an SSH-style path (<span class="emphasis"><em>user@host:/path</em></span>). All of these can be local if necessary—a local FTP or local HTTP server, for example, is a perfectly valid source. <span class="emphasis"><em>Graphics support</em></span> indicates whether to use the virtual framebuffer—it tweaks the <code class="literal">vfb=</code> line in the config file.</p><p>Here's the config file generated from that input:</p><a id="I_programlisting3_d1e2892"/><pre class="programlisting">name = "donalbain"
memory = "512"
disk = ['tap:aio:/mnt/donalbain.img,xvda,w', ]
vif = [ 'mac=00:16:3e:4b:af:c2, bridge=xenbr0', ]
uuid = "162910c8-2a0c-0333-2349-049e8e32ba90"
bootloader = "/usr/bin/pygrub"
vcpus = 1
on_reboot = 'restart'
on_crash  = 'restart'</pre><p>There are some niceties about <code class="literal">virt-install</code>'s config file that we'd like to mention. First, note that <code class="literal">virt-install</code> accesses the disk image <a id="idx-CHP-3-0210" class="indexterm"/>using the tap driver for improved performance. (For more details on the tap driver, see <a class="xref" href="ch04.html" title="Chapter 4. STORAGE WITH XEN">Chapter 4</a>.)</p><p>It also exports the disk as <code class="literal">xvda</code> to the guest operating system, rather than as a SCSI or IDE device. The generated config file also includes a randomly generated MAC for each <code class="literal">vif</code>, using the <code class="literal">00:16:3e</code> prefix assigned to Xen. Finally, the image boots using PyGRUB, rather than specifying a kernel within the config file.</p><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-6" href="#CHP-3-FNOTE-6" class="para">22</a>] </sup>There's nothing <span class="emphasis"><em>inherently</em></span> Red Hat–specific about libvirt, but Red Hat is currently driving its adoption. See <a class="ulink" href="http://libvirt.org/">http://libvirt.org/</a> <a id="idx-CHP-3-0203" class="indexterm"/>for more information.</p></div></div></div>
<div class="sect1" title="Converting VMware Disk Images"><div class="titlepage"><div><div><h1 class="title"><a id="converting_vmware_disk_images"/>Converting VMware Disk Images</h1></div></div></div><p>One of the great things about virtualization is that it allows people to distribute <span class="emphasis"><em>virtual appliances</em></span>—complete, ready-to-run, preconfigured OS images. <a id="idx-CHP-3-0211" class="indexterm"/>VMware has been pushing most strongly in that direction, but with a little work, it's possible to use <a id="idx-CHP-3-0212" class="indexterm"/>VMware's prebuilt virtual machines with Xen.<a id="idx-CHP-3-0213" class="indexterm"/></p><div class="sidebar"><a id="pygrub_pypxeboot_and_friends"/><p class="title">PYGRUB, PYPXEBOOT, AND FRIENDS</p><p>The <a id="idx-CHP-3-0214" class="indexterm"/>principle behind PyGRUB, pypxeboot, and similar programs is that they allow Xen's domain builder to load a kernel that isn't directly accessible from the dom0 filesystem. This, in turn, improves Xen's simulation of a real machine. For example, an automated provisioning tool that uses PXE can provision Xen domains without modification. This becomes especially important in the context of domU images because it allows the image to be a self-contained package—plop a generic config file on top, and it's ready to go.</p><p>Both PyGRUB and pypxeboot take the place of an analogous utility for physical machines: GRUB and PXEboot, respectively. Both are emulations written in Python, specialized to work with Xen. Both acquire the kernel from a place where the ordinary loader would be unable to find it. And both can help you, the hapless Xen administrator, in your day-to-day life.</p><p>For more notes on setting up PyGRUB, see <a class="xref" href="ch07.html" title="Chapter 7. HOSTING UNTRUSTED USERS UNDER XEN: LESSONS FROM THE TRENCHES">Chapter 7</a>. For more on pypxeboot, see <a class="xref" href="ch03s08.html#installing_pypxeboot" title="Installing pypxeboot">Installing pypxeboot</a> on <a class="xref" href="ch03s08.html#installing_pypxeboot" title="Installing pypxeboot">Installing pypxeboot</a>.</p></div><p>Other virtualization providers, by and large, use disk formats that do more than Xen's—for example, they include configuration or provide snapshots. Xen's approach is to leave that sort of feature to standard tools in the dom0. Because Xen uses open formats and standard tools whenever possible, its disk images are simply … filesystems.<sup>[<a id="CHP-3-FNOTE-7" href="#ftn.CHP-3-FNOTE-7" class="footnote">23</a>]</sup></p><p>Thus, the biggest part of <a id="idx-CHP-3-0215" class="indexterm"/>converting a virtual appliance to work with Xen is in <a id="idx-CHP-3-0216" class="indexterm"/>converting over the disk image. Fortunately, <code class="literal">qemu-img</code> supports most of the image formats you're likely to encounter, including VMware's <span class="emphasis"><em>.vmdk</em></span>, or Virtual Machine Disk format.<a id="idx-CHP-3-0217" class="indexterm"/></p><p>The conversion process is pretty easy. First, get a VMware image to play with. There are some good ones at <a class="ulink" href="http://www.vmware.com/appliances/directory/">http://www.vmware.com/appliances/directory/</a>.</p><p>Next, take the image and use <code class="literal">qemu-img</code> to convert it to a QCOW or <a id="idx-CHP-3-0218" class="indexterm"/>raw image:</p><a id="I_programlisting3_d1e3006"/><pre class="programlisting"># qemu-img convert foo.vmdk -o qcow hecate.qcow</pre><p>This command duplicates the contents of <span class="emphasis"><em>foo.vmdk</em></span> in a QCOW image (hence the <code class="literal">-o qcow</code>, for output format) called <em class="filename">hecate.qcow</em>. (<span class="emphasis"><em>QCOW</em></span>, by the way, is a disk image format that originates with the QEMU emulator. It supports AES encryption and transparent decompression. It's also supported by Xen. More details on using QCOW images with Xen are in <a class="xref" href="ch04.html" title="Chapter 4. STORAGE WITH XEN">Chapter 4</a>.) At this point you can boot it as usual, loading the kernel via PyGRUB if it's Xen-aware or if you're using HVM, or using a standard domU kernel from within the dom0 otherwise.</p><p>Unfortunately, this won't generate a configuration suitable for booting the image with Xen. However, it should be easy to create a basic config file that uses the QCOW image as its root device. For example, here's a fairly minimal generic config that relies on the default values to the extent possible:</p><a id="I_programlisting3_d1e3026"/><pre class="programlisting">name = "hecate"
memory = 128
disk = ['tap:qcow:/mnt/hecate.img,xvda,w' ]
vif = [ '' ]
kernel = "/boot/vmlinuz-2.6-xenU"</pre><p>Note that we're using a kernel from the dom0 filesystem rather than loading the kernel from the <a id="idx-CHP-3-0219" class="indexterm"/>VMware disk image with PyGRUB, as we ordinarily suggest. This is so we don't have to worry about whether or not that kernel works with Xen.</p><div class="sidebar"><a id="rpaths_rbuilder_a_new_approach"/><p class="title">RPATH'S RBUILDER: A NEW APPROACH</p><p><a id="idx-CHP-3-0220" class="indexterm"/>RPath is kind of interesting. It probably doesn't merit extended discussion, but their approach to <a id="idx-CHP-3-0221" class="indexterm"/>building <a id="idx-CHP-3-0222" class="indexterm"/>virtual machines is cool. Neat. Elegant.<a id="idx-CHP-3-0223" class="indexterm"/></p><p>RPath starts by focusing on the application that the machine is meant to run and then uses software that determines precisely what the machine needs to run it by examining library dependencies, noticing which config files are read, and so on. The promise of this approach is that it delivers compact, tuned, refined virtual machine images with known characteristics—all while maintaining the high degree of automation necessary to manage large systems.</p><p>Their website is <a class="ulink" href="http://rpath.org/">http://rpath.org/</a>. They've got a good selection of prerolled VMs, aimed at both testing and deployment. (Note that although we think their approach is worth mentioning, we are not affiliated with rPath in any way. You may want to give them a shot, though.)</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-7" href="#CHP-3-FNOTE-7" class="para">23</a>] </sup>Except when they're QCOW images. Let's ignore that for now.</p></div></div></div>
<div class="sect1" title="Mass Deployment"><div class="titlepage"><div><div><h1 class="title"><a id="mass_deployment"/>Mass Deployment</h1></div></div></div><p>Of course, all this is tied up in the broader question of <span class="emphasis"><em>provisioning infrastructure</em></span> and higher-level tools like Kickstart, SystemImager, and so on. Xen amplifies the problem by increasing the number of servers you own exponentially and making it easy and quick to bring another server online. That means you now need the ability to automatically deploy lots of hosts.<a id="idx-CHP-3-0224" class="indexterm"/></p><div class="sect2" title="Manual Deployment"><div class="titlepage"><div><div><h2 class="title"><a id="manual_deployment"/>Manual Deployment</h2></div></div></div><p>The most basic approach (analogous to <span class="emphasis"><em>tarring up a filesystem</em></span>) is probably to build a single <a id="idx-CHP-3-0225" class="indexterm"/>tarball using any of the methods we've discussed and then make a script that partitions, formats, and mounts each domU file and then extracts the tarball.<a id="idx-CHP-3-0226" class="indexterm"/></p><p>For example:</p><a id="I_programlisting3_d1e3094"/><pre class="programlisting">#!/bin/bash

LVNAME=$1

lvcreate -C y -L 1024 -n ${LVNAME} lvmdisk

parted /dev/lvmdisk/${LVNAME} mklabel msdos
parted /dev/lvmdisk/${LVNAME} mkpartfs primary ext2 0 1024

kpartx -p "" -av /dev/lvmdisk/${LVNAME}

tune2fs -j /dev/mapper/${LVNAME}1

mount /dev/mapper/${LVNAME}1 /mountpoint

tar -C /mountpoint -zxf /opt/xen/images/base.tar.gz

umount /mountpoint

kpartx -d /dev/lvmdisk/${LVNAME}

cat &gt;/etc/xen/${LVNAME} &lt;&lt;EOF

name = "$LVNAME"
memory = 128
disk = ['phy:/dev/lvmdisk/${LVNAME},xvda,w']
vif = ['']
kernel = "/boot/vmlinuz-2.6-xenU"

EOF

exit 0</pre><p>This script takes a domain name as an argument, provisions storage from a <a id="idx-CHP-3-0227" class="indexterm"/>tarball at <span class="emphasis"><em>/opt/xen/images/base.tar.gz</em></span>, and writes a config file for a basic domain, with a gigabyte of disk and 128MB of memory. Further extensions to this script are, as always, easy to imagine. We've put this script here mostly to show how simple it can be to create a large number of domU images quickly with Xen. Next, we'll move on to more elaborate provisioning systems.</p></div><div class="sect2" title="QEMU and Your Existing Infrastructure"><div class="titlepage"><div><div><h2 class="title"><a id="qemu_and_your_existing_infrastructure"/>QEMU and Your Existing Infrastructure</h2></div></div></div><p>Another way to do <a id="idx-CHP-3-0228" class="indexterm"/>mass provisioning is with QEMU, extending the QEMU installation we previously outlined. Because QEMU simulates a physical machine, you can use your existing provisioning tools with QEMU—in effect treating virtual machines exactly like physical machines. For example, we've done this using SystemImager to perform automatic installs on the <a id="idx-CHP-3-0229" class="indexterm"/>emulated machines.<a id="idx-CHP-3-0230" class="indexterm"/></p><p>This approach is perhaps the most flexible (and most likely integrates best with your current provisioning system), but it's slow. Remember, <a id="idx-CHP-3-0231" class="indexterm"/>KQEMU and Xen are not compatible, so you are running old-school, software-only <a id="idx-CHP-3-0232" class="indexterm"/>QEMU. Slow! And needlessly slow because when a VM has been created, there's nothing to keep you from duplicating it rather than going through the entire process again. But it works, and it works the exact same way as your previous provisioning system.<sup>[<a id="CHP-3-FNOTE-8" href="#ftn.CHP-3-FNOTE-8" class="footnote">24</a>]</sup></p><p>We'll describe a basic setup with SystemImager and QEMU, which should be easy enough to generalize to whichever other provisioning system you've got in place.</p><div class="sect3" title="Setting Up SystemImager"><div class="titlepage"><div><div><h3 class="title"><a id="setting_up_systemimager"/>Setting Up SystemImager</h3></div></div></div><p>First, install SystemImager using your method of choice—<code class="literal">yum</code>, <code class="literal">apt-get</code>, download from <a class="ulink" href="http://wiki.systemimager.org/">http://wiki.systemimager.org/</a>—whichever. We downloaded the RPMs from SystemImager using the <a id="idx-CHP-3-0235" class="indexterm"/>sis-install script:<a id="idx-CHP-3-0236" class="indexterm"/></p><a id="I_programlisting3_d1e3177"/><pre class="programlisting"># wget http://download.systemimager.org/pub/sis-install/install
# sh install -v --download-only --tag=stable --directory.  <a id="idx-CHP-3-0237" class="indexterm"/>systemconfigurator
\ systemimager-client systemimager-common  systemimager-i386boot-standard \
systemimager-i386initrd_template  systemimager-server</pre><p>SystemImager works by taking a system image of a <span class="emphasis"><em>golden client</em></span>, hosting that image on a server, and then automatically rolling the image out to targets. In the Xen case, these components—golden client, server, and targets—can all exist on the same machine. We'll assume that the server is dom0, the client is a domU that you've installed by some other method, and the targets are new domUs.</p><p>Begin by installing the dependency, <code class="literal">systemconfigurator</code>, on the server:</p><a id="I_programlisting3_d1e3193"/><pre class="programlisting"># rpm -ivh systemconfigurator-*</pre><p>Then install the server packages:</p><a id="I_programlisting3_d1e3197"/><pre class="programlisting"># rpm -ivh systemimager-common-* systemimager-server-* \
     systemimager-i386boot-standard-*</pre><p>Boot the golden client using <code class="literal">xm create</code> and install the packages (note that we are performing these next steps within the domU rather than the dom0):<a id="idx-CHP-3-0238" class="indexterm"/></p><a id="I_programlisting3_d1e3207"/><pre class="programlisting"># scp user@server:/path/to/systemimager/* .
# rpm -ivh systemconfigurator-*
# rpm -ivh systemimager-common-* systemimager-client-* \
systemimager-i386boot-initrd_template-*</pre><p><a id="idx-CHP-3-0239" class="indexterm"/>SystemImager's process <a id="idx-CHP-3-0240" class="indexterm"/>for <a id="idx-CHP-3-0241" class="indexterm"/>generating an image from the golden client is fairly automated. It uses <code class="literal">rsync</code> to copy files from the client to the image <a id="idx-CHP-3-0242" class="indexterm"/>server. Make sure the two hosts can communicate over the network. When that's done, run on the client:<a id="idx-CHP-3-0243" class="indexterm"/></p><a id="I_programlisting3_d1e3237"/><pre class="programlisting"># si_prepareclient --server &lt;server address&gt;</pre><p>Then run on the server:</p><a id="I_programlisting3_d1e3241"/><pre class="programlisting"># si_getimage --golden_client &lt;client address&gt; --image porter --exclude /mnt</pre><p>The server will connect to the client and build the image, using the name <span class="emphasis"><em>porter</em></span>.</p><p>Now you're ready to configure the server to actually serve out the image. Begin by running the <code class="literal">si_mkbootserver</code> script and answering its questions. It'll configure DHCP and TFTP for you.</p><a id="I_programlisting3_d1e3253"/><pre class="programlisting"># si_mkbootserver</pre><p>Then answer some more questions about the clients:</p><a id="I_programlisting3_d1e3257"/><pre class="programlisting"># si_mkclients</pre><p>Finally, use the provided script to enable <a id="idx-CHP-3-0244" class="indexterm"/>netboot for the requisite clients:</p><a id="I_programlisting3_d1e3265"/><pre class="programlisting"># si_mkclientnetboot --netboot --clients lennox rosse angus</pre><p>And you're ready to go. Boot the QEMU machine from the <a id="idx-CHP-3-0245" class="indexterm"/>emulated network adapter (which we've left unspecified on the command line because it's active by default):</p><a id="I_programlisting3_d1e3274"/><pre class="programlisting"># qemu --hda /xen/lennox/root.img --boot n</pre><p>Of course, after the clients install, you will need to create domU configurations. One way might be to use a simple script (in Perl this time, for variety):</p><a id="I_programlisting3_d1e3278"/><pre class="programlisting">#!/usr/bin/perl
$name = $ARGV[0];
open(XEN, '&gt;', "/etc/xen/$name");
print XEN &lt;&lt;CONFIG;
kernel = "/boot/vmlinuz-2.6.xenU"
memory = 128
name = "$name"
disk = ['tap:aio:/xen/$name/root.img,hda1,w']
vif = ['']
root = "/dev/hda1 ro"
CONFIG
close(XEN);</pre><p>(Further refinements, such as generating an IP based on the name, are of course easy to imagine.) In any case, just run this script <a id="idx-CHP-3-0246" class="indexterm"/>with the name as argument:</p><a id="I_programlisting3_d1e3288"/><pre class="programlisting"># makeconf.pl lennox</pre><p>And then start your shiny new Xen machine:</p><a id="I_programlisting3_d1e3292"/><pre class="programlisting"># xm create -c /etc/xen/lennox</pre></div></div><div class="sect2" title="Installing pypxeboot"><div class="titlepage"><div><div><h2 class="title"><a id="installing_pypxeboot"/>Installing pypxeboot</h2></div></div></div><p>Like PyGRUB, pypxeboot is a Python script that acts as a domU bootloader. Just as PyGRUB loads a kernel from the domain's virtual disk, pypxeboot loads a kernel from the network, after the fashion of PXEboot (for Preboot eXecution Environment) on standalone computers. It accomplishes this by calling <code class="literal">udhcpc</code> (the micro-DHCP client) to get a network configuration, and then TFTP to download a kernel, based on the MAC address specified in the domain config file.<a id="idx-CHP-3-0247" class="indexterm"/><a id="idx-CHP-3-0248" class="indexterm"/></p><p>pypxeboot isn't terribly hard to get started with. You'll need the pypxeboot package itself, udhcp, and tftp. Download the <a id="idx-CHP-3-0249" class="indexterm"/>packages and extract them. You can get pypxeboot from <a class="ulink" href="http://book.xen.prgmr.com/mediawiki/index.php/pypxeboot">http://book.xen.prgmr.com/mediawiki/index.php/pypxeboot</a> and udhcp from <a class="ulink" href="http://book.xen.prgmr.com/mediawiki/index.php/udhcp">http://book.xen.prgmr.com/mediawiki/index.php/udhcp</a>. Your distro will most likely include the tftp client already.</p><p>The pypxeboot package includes a patch for udhcp that allows udhcp to take a MAC address from the command line. Apply it.</p><a id="I_programlisting3_d1e3326"/><pre class="programlisting"># patch -p0 &lt; pypxeboot-0.0.2/udhcp_usermac.patch
patching file udhcp-0.9.8/dhcpc.c
patching file udhcp-0.9.8/dhcpc.h
patching file udhcp-0.9.8/README.udhcpc</pre><p>Build udhcp. A simple <code class="literal">make</code> followed by <code class="literal">make install</code> did the trick for us.</p><p>Copy pypxeboot and <span class="emphasis"><em>outputpy.udhcp.sh</em></span> to appropriate places:</p><a id="I_programlisting3_d1e3341"/><pre class="programlisting"># cp pypxeboot-0.0.2/pypxeboot /usr/bin
# cp pypxeboot-0.0.2/outputpy.udhcp.sh /usr/share/udhcpc</pre><p>Next set <a id="idx-CHP-3-0250" class="indexterm"/>up the <a id="idx-CHP-3-0251" class="indexterm"/>TFTP server for network boot. The boot server can be essentially the same as a boot server for physical <a id="idx-CHP-3-0252" class="indexterm"/>machines, with the caveat that the kernel and initrd need to support Xen paravirtualization. We used the setup generated by Cobbler, but any PXE environment should work.</p><p>Now you should be able to use pypxeboot with a domU configuration similar to the following:<a id="I_indexterm3_d1e3361" class="indexterm"/><a id="I_indexterm3_d1e3366" class="indexterm"/></p><a id="I_programlisting3_d1e3371"/><pre class="programlisting">bootloader="/usr/bin/pypxeboot"
vif=['mac=00:16:3E:11:11:11']
bootargs=vif[0]</pre><div class="note" title="Note"><h3 class="title"><a id="note-14"/>Note</h3><p><span class="emphasis"><em>The regex that finds the MAC address in pypxeboot is easily confused. If you specify other parameters, put spaces between the</em></span> <em class="replaceable"><code>mac=</code></em> <span class="emphasis"><em>parameter and the surrounding commas, for example</em></span>, <em class="replaceable"><code>vif = ['vifname=lady , mac=00:16:3E:11:11:11 , bridge=xenbr0']</code></em>.<a id="idx-CHP-3-0253" class="indexterm"/></p></div><p>Create the domain:</p><a id="I_programlisting3_d1e3393"/><pre class="programlisting"># xm create lady
Using config file "/etc/xen/lady".
pypxeboot: requesting info for MAC address 00:16:3E:11:11:11
pypxeboot: getting cfg for IP 192.l68.4.114 (C0A80427) from server 192.168.4.102
pypxeboot: <a id="idx-CHP-3-0254" class="indexterm"/>downloading initrd using cmd: tftp 192.168.4.102 -c
get /images/scotland-xen-i386/initrd.img /var/lib/xen/initrd.BEUTCy
pypxeboot: downloading kernel using cmd: tftp 192.168.4.102 -c
get /images/scotland-xen-i386/vmlinuz /var/lib/xen/kernel.8HJDNE
Started domain lady</pre></div><div class="sect2" title="Automated Installs the Red Hat Way"><div class="titlepage"><div><div><h2 class="title"><a id="automated_installs_the_red_hat_way"/>Automated Installs the Red Hat Way</h2></div></div></div><p>Red Hat uses Kickstart to provision standalone <a id="idx-CHP-3-0255" class="indexterm"/>systems. A full discussion of Kickstart is probably best left to Red Hat's documentation—suffice it to say that Kickstart has been designed so that, with some supporting tools, you can install Xen domUs with it.<a id="idx-CHP-3-0256" class="indexterm"/><a id="idx-CHP-3-0257" class="indexterm"/></p><p>The tools you'll most likely want to use to install virtual machines are <a id="idx-CHP-3-0258" class="indexterm"/>Cobbler and <code class="literal">koan</code>. Cobbler is the server software, while <code class="literal">koan</code> (<span class="emphasis"><em>Kickstart over a network</em></span>)<sup>[<a id="CHP-3-FNOTE-9" href="#ftn.CHP-3-FNOTE-9" class="footnote">25</a>]</sup> is the client. With the <code class="literal">--virt</code> option, <code class="literal">koan</code> supports <a id="idx-CHP-3-0259" class="indexterm"/>installing to a virtual machine.<a id="idx-CHP-3-0260" class="indexterm"/></p><p>This being a Red Hat tool, you can install it with <code class="literal">yum</code>.</p><p>No, sorry, we lied about that. First you'll need to add the <span class="emphasis"><em>Extra Packages for Enterprise Linux</em></span> repository to your <code class="literal">yum</code> configuration. Install the package describing the additional repo:<a id="idx-CHP-3-0261" class="indexterm"/></p><a id="I_programlisting3_d1e3473"/><pre class="programlisting">rpm -ivh http://download.fedora.redhat.com/pub/epel/5/i386/epel-release-5-3.noarch.rpm</pre><p><span class="emphasis"><em>Now</em></span> you can install Cobbler with <code class="literal">yum</code>:</p><a id="I_programlisting3_d1e3482"/><pre class="programlisting"># yum install cobbler</pre><p>Then you'll want to configure it. Run <code class="literal">cobbler check</code>, which will give you a list of issues that may interfere with Cobbler. For example, out of the box, Cobbler reported these issues for us:</p><a id="I_programlisting3_d1e3489"/><pre class="programlisting">The following potential problems were detected:
#0: The 'server' field in /var/lib/cobbler/settings must be set to something
other than localhost, or kickstarting features will not work.  This should be
a resolvable hostname or IP for the boot server as reachable by all machines
that will use it.
#1: For PXE to be functional, the 'next_server' field in /var/lib/cobbler/
settings must be set to something other than 127.0.0.1, and should match the
IP of the boot server on the PXE network.
#2: change 'disable' to 'no' in /etc/xinetd.d/tftp
#3: service httpd is not running
#4: since iptables may be running, ensure 69, 80, 25150, and 25151 are
unblocked
#5: reposync is not installed, need for cobbler reposync, install/upgrade yum-
utils?
#6: yumdownloader is not installed, needed for cobbler repo add with --rpm-
list parameter, install/upgrade yum-utils?</pre><p>After you've fixed these problems, you're ready to use <a id="idx-CHP-3-0262" class="indexterm"/>Cobbler. This involves setting up install media and adding profiles.</p><p>First, find some install media. Kickstart is a <a id="idx-CHP-3-0263" class="indexterm"/>Red Hat–specific package, so Cobbler works only with Red Hat–like distros (SUSE is also supported, but it's experimental). Cobbler supports importing a Red Hat–style install tree via <code class="literal">rsync</code>, a mounted DVD, or NFS. Here we'll use a DVD—for other options, see Cobbler's man page.</p><a id="I_programlisting3_d1e3509"/><pre class="programlisting"># cobbler import --path/mnt/dvd --name=scotland</pre><p>If you're using a network install source, this may take a while. A full mirror of one architecture is around 5GB of software. When it's done downloading, you can see the mirror status by running <code class="literal">cobbler report</code>. When you've got a directory tree, you can use it as an install source by adding a <span class="emphasis"><em>profile</em></span> for each type of virtual machine you plan to install. We suggest <a id="idx-CHP-3-0264" class="indexterm"/>installing through Cobbler rather than <span class="emphasis"><em>bare</em></span> pypxeboot and Kickstart because it has features aimed specifically at setting up virtual machines. For example, you can specify the domU image size and RAM amount in the machine profile (in GB and MB, respectively):</p><a id="I_programlisting3_d1e3528"/><pre class="programlisting"># cobbler profile add -name=bar -distro=foo -virt-file-size=4 -virt-ram=128</pre><p>When you've added profiles, the next step is to tell Cobbler to regenerate some data, including PXEboot menus:</p><a id="I_programlisting3_d1e3532"/><pre class="programlisting"># cobbler sync</pre><p>Finally, you can use the client, <code class="literal">koan</code>, to build the virtual machine. Specify the Cobbler server, a profile, and optionally a name for the virtual machine. We also used the <code class="literal">--nogfx</code> option to disable the VNC framebuffer. If you leave the framebuffer enabled, you won't be able to interact with the domU via <code class="literal">xm console</code>:</p><a id="I_programlisting3_d1e3545"/><pre class="programlisting"># koan --virt --server=localhost --profile=scotland --virt-name=lady --nogfx</pre><p><code class="literal">koan</code> will then create a virtual machine, install, and automatically create a domU config so that you can then start the domU using <code class="literal">xm</code>:</p><a id="I_programlisting3_d1e3554"/><pre class="programlisting"># xm create -c lady</pre></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-8" href="#CHP-3-FNOTE-8" class="para">24</a>] </sup>This can be made faster by using an HVM domU <a id="idx-CHP-3-0233" class="indexterm"/>for the <a id="idx-CHP-3-0234" class="indexterm"/>SystemImager install, rather than a QEMU instance. Not <span class="emphasis"><em>blazing fast</em></span>, but an improvement.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-3-FNOTE-9" href="#CHP-3-FNOTE-9" class="para">25</a>] </sup>It begs the question of whether there are non-networked Kickstart installs, but we'll let that slide.</p></div></div></div>
<div class="sect1" title="And Then&#x2026;"><div class="titlepage"><div><div><h1 class="title"><a id="and_thenhellip"/>And Then…</h1></div></div></div><p>In this chapter, we've gone through a bunch of install methods, ranging from the generic and brute force to the specialized and distro-specific. Although we haven't covered anything in exhaustive detail, we've done our best to outline the procedures to emphasize when you might want to, say, use <code class="literal">yum</code>, and when you might want to use QEMU. We've also gestured in the direction of possible pitfalls with each method.</p><p>Many of the higher-level domU management tools also include a quick-and-easy way to install a domU if none of these more generic methods strike your fancy. (See <a class="xref" href="ch06.html" title="Chapter 6. DOMU MANAGEMENT: TOOLS AND FRONTENDS">Chapter 6</a> for details.) For example, you're most likely to encounter <code class="literal">virt-install</code> in the context of Red Hat's <code class="literal">virt-manager</code>.</p><p>The important thing, though, is to tailor the install method to your needs. Consider how many systems you're going to install, how similar they are to each other, and the intended role of the domU, and then pick whatever makes the most sense.<a id="I_indexterm3_d1e3576" class="indexterm"/><a id="I_indexterm3_d1e3581" class="indexterm"/><a id="I_indexterm3_d1e3586" class="indexterm"/></p></div></body></html>