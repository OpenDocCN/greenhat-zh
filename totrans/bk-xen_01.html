<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;XEN: A HIGH-LEVEL OVERVIEW"><div class="titlepage"><div><div><h1 class="title"><a id="xen_a_high_level_overview"/>Chapter 1. XEN: A HIGH-LEVEL OVERVIEW</h1></div></div></div><div class="informalfigure"><div class="mediaobject"><a id="I_mediaobject1_d1e353"/><img src="httpatomoreillycomsourcenostarchimages333191.png.jpg" alt="image with no caption"/></div></div><p>We'll start by explaining what makes Xen different from other <a id="idx-CHP-1-0001" class="indexterm"/>virtualization techniques and then provide some low-level detail on how Xen works and how its components fit together.</p><div class="sect1" title="Virtualization Principles"><div class="titlepage"><div><div><h1 class="title"><a id="virtualization_principles"/>Virtualization Principles</h1></div></div></div><p>First, we might want to mention that computers, even new and fast ones with modern multitasking operating systems, can perform only one instruction at a time.<sup>[<a id="CHP-1-FNOTE-1" href="#ftn.CHP-1-FNOTE-1" class="footnote">8</a>]</sup> Now, you say, "But my computer is performing many tasks at once. Even now, I can see a clock running, hear music playing, download files, and chat with friends, all at the same time." And this is true. However, what is <span class="emphasis"><em>actually</em></span> happening is that the computer is switching between these different tasks so quickly that the delays become imperceptible. Just as a movie is a succession of still images that give the illusion of movement, a computer performs tasks that are so seamlessly interweaved as to appear simultaneous.<a id="idx-CHP-1-0002" class="indexterm"/></p><p>Virtualization just extends this metaphor a bit. Ordinarily, this <a id="idx-CHP-1-0003" class="indexterm"/>multiplexing takes place under the direction of the operating system, which acts to supervise tasks and make sure that each receives its fair share of CPU time. Because the operating system must therefore <span class="emphasis"><em>schedule</em></span> tasks to run on the CPU, this aspect of the operating system is called a <span class="emphasis"><em>scheduler</em></span>. With Xen virtualization, the same process occurs, with entire operating systems taking the place of tasks. The scheduling aspect is handled by the Xen kernel, which runs on a level superior to the "supervising" guest operating systems, and which we thus call the <span class="emphasis"><em>hypervisor</em></span>.<a id="idx-CHP-1-0004" class="indexterm"/></p><p>Of course, it's not quite so simple—operating systems, even ones that have been modified to be Xen-friendly, use a different, more comprehensive, set of assumptions than applications, and switching between them is almost by definition going to involve more complexity.</p><p>So let's look at an overview of how virtualization is traditionally done and how Xen's design is new and different. A traditional virtual machine is designed to mimic a real machine in every way, such that it's impossible to tell from within the virtual machine that it isn't real. To preserve that illusion, fully virtualized machines intercept attempts to access hardware and emulate that hardware's functionality in software—thus maintaining perfect compatibility with the applications inside the virtual machine. This layer of indirection makes the virtual machine very slow.</p><p>Xen bypasses this slowdown using an approach called <span class="emphasis"><em>paravirtualization</em></span>—<span class="emphasis"><em>para</em></span> as a prefix means <span class="emphasis"><em>similar to</em></span> or <span class="emphasis"><em>alongside</em></span>. As the name suggests, it's not "real" virtualization in the traditional sense because it doesn't try to provide a seamless illusion of a machine. Xen presents only a partial abstraction of the underlying hardware to the hosted operating system, exposing some aspects of the machine as <span class="emphasis"><em>limitations</em></span> on the guest OS, which needs to know that it's running on Xen and should handle certain hardware interactions accordingly.<a id="idx-CHP-1-0005" class="indexterm"/></p><div class="note" title="Note"><h3 class="title"><a id="note-2"/>Note</h3><p><span class="emphasis"><em>Newer processors incorporate support for hardware virtualization, allowing unmodified operating systems to run under Xen. See <a class="xref" href="ch12.html" title="Chapter 12. HVM: BEYOND PARAVIRTUALIZATION">Chapter 12</a> for details</em></span>.</p></div><p>Most of these limitations—by design—aren't noticeable to the system's users. To run under Xen, the guest OS kernel needs to be modified so that, for example, it asks Xen for memory rather than allocating it directly. One of the design goals for Xen was to have these changes occur in the hardware-dependent bits of the guest operating system, without changing the interface between the kernel and user-level software.</p><p>This design goal reduces the difficulty of moving to Xen by ensuring that existing binaries will work unmodified on the Xen guest OS and that the virtual machine will, in most regards, act exactly like a real one, at least from the perspective of the system's end users.</p><p>Xen therefore trades seamless virtualization for a high-performance paravirtualized environment. The paper in which the original Xen developers initially presented this project, "<a id="idx-CHP-1-0006" class="indexterm"/>Xen and the Art of Virtualization,"<sup>[<a id="CHP-1-FNOTE-2" href="#ftn.CHP-1-FNOTE-2" class="footnote">9</a>]</sup> puts this in strong terms, saying "<a id="idx-CHP-1-0007" class="indexterm"/>Paravirtualization is necessary to attain high performance and strong resource isolation on uncooperative machine architectures such as x86." It's not quite as simple as "paravirtualization makes a computer fast"—I/O, for example, can lead to expensive context switches—but it is generally faster than other approaches. We generally assume that a <a id="idx-CHP-1-0008" class="indexterm"/>Xen guest will run at about 95 percent of its native speed on physical hardware, assuming that other guests on the machine are idle.</p><p>However, paravirtualization isn't the only way to run a virtual machine. There are two competing approaches: <a id="idx-CHP-1-0009" class="indexterm"/>full virtualization and OS-level virtualization.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-1-FNOTE-1" href="#CHP-1-FNOTE-1" class="para">8</a>] </sup>Of course, SMP and multicore CPUs make this not entirely true, and we are drastically simplifying pipelining, superscalar execution, and so forth, but the principle still holds—at any instant, each core is only doing one thing.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-1-FNOTE-2" href="#CHP-1-FNOTE-2" class="para">9</a>] </sup>See <a class="ulink" href="http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf">http://www.cl.cam.ac.uk/research/srg/netos/papers/2003-xensosp.pdf</a>.</p></div></div></div>
<div class="sect1" title="Virtualization Techniques: Full Virtualization"><div class="titlepage"><div><div><h1 class="title"><a id="virtualization_techniques_full_virtualiz"/>Virtualization Techniques: Full Virtualization</h1></div></div></div><p>Not all virtualization methods use Xen's approach. Virtualization software come in three flavors. At one extreme you have <span class="emphasis"><em>full virtualization</em></span>, or <a id="idx-CHP-1-0010" class="indexterm"/>emulation, in which the virtual machine is a software simulation of hardware, real or fictional—as long as there's a driver, it doesn't matter much. Products in this category include <a id="idx-CHP-1-0011" class="indexterm"/>VMware and QEMU.<a id="idx-CHP-1-0012" class="indexterm"/></p><div class="note" title="Note"><h3 class="title"><a id="note-3"/>Note</h3><p><span class="emphasis"><em>And what, you ask, is this fictional hardware? Apart from the obvious "not real" answer, one good example is the VTPM driver. TPM (Trusted Platform Module) hardware is relatively uncommon, but it has some potential applications with signing code—for example, making sure that the running kernel is the correct one, rather than a fake put on by a rootkit or virus. Xen therefore makes a virtual TPM available to the domUs</em></span>.<a id="idx-CHP-1-0013" class="indexterm"/></p></div><p>With full virtualization, an unmodified<sup>[<a id="CHP-1-FNOTE-3" href="#ftn.CHP-1-FNOTE-3" class="footnote">10</a>]</sup> OS "hosts" a userspace program that emulates a machine on which the "guest" OS runs. This is a popular approach because it doesn't require the <a id="idx-CHP-1-0015" class="indexterm"/>guest OS to be changed in any way. It also has the advantage that the virtualized architecture can be completely different from the host architecture—for example, QEMU can simulate a MIPS processor on an IA-32 host and a startling array of other chips.</p><p>However, this level of hardware independence comes at the cost of an enormous speed penalty. <a id="idx-CHP-1-0016" class="indexterm"/>Unaccelerated QEMU is an order of magnitude slower than native execution, and accelerated QEMU or VMware ESX server can only accelerate the emulated machine if it's the same architecture as the underlying hardware. In this context, for normal usage, the increased hardware versatility of a full emulator isn't a significant advantage over Xen.</p><p>VMware is currently the best-known vendor of full-virtualization products, with a robust set of tools, broad support, and a strong brand. Recent versions of VMware address the speed problem by running instructions in place where possible and dynamically translating code when necessary. Although this approach is elegant and doesn't require <a id="idx-CHP-1-0017" class="indexterm"/>guest OS modification, it's not as fast as Xen, making it less desirable for production setups or for a full-time work environment.</p><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-1-FNOTE-3" href="#CHP-1-FNOTE-3" class="para">10</a>] </sup>Or a slightly modified OS—QEMU, for example, has the <a id="idx-CHP-1-0014" class="indexterm"/>KQEMU kernel module, which speeds up the emulated code by allowing it to run directly on the processor when possible.</p></div></div></div>
<div class="sect1" title="Virtualization Techniques: OS Virtualization"><div class="titlepage"><div><div><h1 class="title"><a id="virtualization_techniques_os_virtualizat"/>Virtualization Techniques: OS Virtualization</h1></div></div></div><p>On the other extreme is <a id="idx-CHP-1-0018" class="indexterm"/><a id="idx-CHP-1-0019" class="indexterm"/><span class="emphasis"><em>OS-level virtualization</em></span>, where what's being virtualized is the operating environment, rather than the complete machine. FreeBSD jails and Solaris <a id="idx-CHP-1-0020" class="indexterm"/>Containers take this <a id="idx-CHP-1-0021" class="indexterm"/>approach.</p><p>OS virtualization takes the position that the operating system already provides, or at least can be made to provide, sufficient isolation to do everything that a normal VM user expects—install <a id="idx-CHP-1-0022" class="indexterm"/>software systemwide, upgrade system libraries in the guest without affecting those in the host, and so forth. Thus, rather than emulating physical hardware, OS virtualization emulates a complete OS userspace using operating system facilities.</p><p>FreeBSD jails and Solaris Containers (or <a id="idx-CHP-1-0023" class="indexterm"/>Zones) are two popular implementations of <a id="idx-CHP-1-0024" class="indexterm"/>OS-level virtualization. Both derive from the classic Unix <code class="literal">chroot</code> jail. The idea is that the jailed process can only access parts of the filesystem that reside under a certain directory—the rest of the filesystem, as far as this process can tell, simply doesn't exist. If we install an OS into that directory, it can be considered a complete virtual environment. Jails and Zones expand on the concept by also restricting certain system calls and providing a virtual network interface to enhance isolation between virtual machines. Although this is incredibly useful, it's neither as useful or as versatile as a full-fledged virtual machine would be. Because the jails share a kernel, for example, a kernel panic will bring down all the VMs on the hardware.</p><p>However, because they bypass the overhead of virtualizing hardware, virtualized machines can be about as fast as native execution—in fact, they are native.</p><p>OS virtualization and Xen complement each other, each being useful in different situations, possibly even simultaneously. One can readily imagine, for example, giving a user a single Xen VM, which he then partitions into multiple Zones for his own use.</p></div>
<div class="sect1" title="Paravirtualization: Xen's Approach"><div class="titlepage"><div><div><h1 class="title"><a id="paravirtualization_xens_approach"/>Paravirtualization: Xen's Approach</h1></div></div></div><p>Finally, somewhere between the two, there's <span class="emphasis"><em>paravirtualization</em></span>, which relies on the operating system being modified to work in concert with a sort of "super operating system," which we call the <span class="emphasis"><em>hypervisor</em></span>. This is the approach Xen uses.</p><div class="sect2" title="How Paravirtualization Works"><div class="titlepage"><div><div><h2 class="title"><a id="how_paravirtualization_works"/>How Paravirtualization Works</h2></div></div></div><p>Xen works by introducing a very small, very compact and focused piece of software that runs directly on the hardware and provides services to the virtualized operating systems.<sup>[<a id="CHP-1-FNOTE-4" href="#ftn.CHP-1-FNOTE-4" class="footnote">11</a>]</sup></p><p>Xen's <a id="idx-CHP-1-0026" class="indexterm"/>approach to virtualization <a id="idx-CHP-1-0027" class="indexterm"/>does away <a id="idx-CHP-1-0028" class="indexterm"/>with most <a id="idx-CHP-1-0029" class="indexterm"/>of the split between host OS and <a id="idx-CHP-1-0030" class="indexterm"/>guest OS. Full virtualization and OS-level virtualization have a clear distinction—the host OS is the one that runs with full privileges. With Xen, only the <a id="idx-CHP-1-0031" class="indexterm"/>hypervisor has full privileges, and it's designed to be as <a id="idx-CHP-1-0032" class="indexterm"/>small and limited as possible.</p><p>Instead of this "<a id="idx-CHP-1-0033" class="indexterm"/>host/guest" split, the hypervisor relies on a trusted <a id="idx-CHP-1-0034" class="indexterm"/>guest OS (<a id="idx-CHP-1-0035" class="indexterm"/>domain 0, the <span class="emphasis"><em>driver domain</em></span>, or more informally, <span class="emphasis"><em>dom0</em></span>) to provide hardware drivers, a kernel, and a userland. This privileged domain is uniquely distinguished as the domain that the hypervisor allows to access devices and perform control <a id="idx-CHP-1-0036" class="indexterm"/>functions. By doing this, the Xen developers ensure that the hypervisor remains small and maintainable and that it occupies as little memory as possible. <a class="xref" href="ch01s04.html#shown_here_is_the_hypervisor_with_domain" title="Figure 1-1. Shown here is the hypervisor with domains. Note that the hypervisor runs directly on the hardware but doesn't itself mediate access to disk and network devices. Instead, dom0 interacts directly with disk and network devices, servicing requests from the other domains. In this diagram, domU 1 also acts as a driver domain for an unnamed PCI device.">Figure 1-1</a> shows this relationship.<a id="idx-CHP-1-0037" class="indexterm"/></p><div class="figure"><a id="shown_here_is_the_hypervisor_with_domain"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject1_d1e680"/><img src="httpatomoreillycomsourcenostarchimages333193.png.jpg" alt="Shown here is the hypervisor with domains. Note that the hypervisor runs directly on the hardware but doesn't itself mediate access to disk and network devices. Instead, dom0 interacts directly with disk and network devices, servicing requests from the other domains. In this diagram, domU 1 also acts as a driver domain for an unnamed PCI device."/></div></div><p class="title">Figure 1-1. Shown here is the hypervisor with domains. Note that the hypervisor runs directly on the hardware but doesn't itself mediate access to disk and network devices. Instead, dom0 interacts directly with disk and network devices, servicing requests from the other domains. In this diagram, domU 1 also acts as a driver domain for an unnamed PCI device.</p></div><div class="note" title="Note"><h3 class="title"><a id="note-4"/>Note</h3><p><span class="emphasis"><em>See also "Safe Hardware Access with the Xen Virtual Machine Monitor," Fraser et al.<sup>[<a id="CHP-1-FNOTE-5" href="#ftn.CHP-1-FNOTE-5" class="footnote">12</a>]</sup> Also, non-dom0 driver domains can exist—however, they're not recommended on current hardware in the absence of an IOMMU (I/O Memory Management Unit) and therefore will not be covered here. For more on IOMMU development, see <a class="xref" href="ch12.html" title="Chapter 12. HVM: BEYOND PARAVIRTUALIZATION">Chapter 12</a></em></span>.<a id="idx-CHP-1-0038" class="indexterm"/><a id="idx-CHP-1-0039" class="indexterm"/></p></div><p>Domain 0's <a id="idx-CHP-1-0040" class="indexterm"/>privileged operations broadly fall into two categories. First, dom0 functions as an area from which to administer Xen. From the dom0, the administrator can control the other domains running on the machine—create, destroy, save, restore, etc. Network and storage devices can also be manipulated—created, presented to the kernel, assigned to domUs, etc.</p><p>Second, dom0 has uniquely <a id="idx-CHP-1-0041" class="indexterm"/>privileged access to hardware. The domain 0 kernel has the usual hardware drivers and uses them to export abstractions of hardware devices to the hypervisor and thence to virtual machines. Think of the machine as a car, with the dom0 as driver. He's also a passenger but has privileges and responsibilities that the other passengers don't.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-1-FNOTE-4" href="#CHP-1-FNOTE-4" class="para">11</a>] </sup>Some would call the Xen hypervisor a <a id="idx-CHP-1-0025" class="indexterm"/>microkernel. Others wouldn't.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-1-FNOTE-5" href="#CHP-1-FNOTE-5" class="para">12</a>] </sup>See <a class="ulink" href="http://www.cl.cam.ac.uk/research/srg/netos/papers/2004-oasis-ngio.pdf">http://www.cl.cam.ac.uk/research/srg/netos/papers/2004-oasis-ngio.pdf</a>.</p></div></div></div>
<div class="sect1" title="Xen's Underpinnings: The Gory Details"><div class="titlepage"><div><div><h1 class="title"><a id="xens_underpinnings_the_gory_details"/>Xen's Underpinnings: The Gory Details</h1></div></div></div><p>So, with this concept of virtual devices firmly in mind, the question becomes: What does a computer need to provide at the most basic level? The Xen developers considered this question at length and concluded that Xen would have to manage <span class="emphasis"><em>CPU time, interrupts, memory, block devices</em></span>, and <span class="emphasis"><em>network</em></span>.</p><p>The hypervisor operates much like the very core of a traditional operating system, parceling out CPU time and resources to the operating systems that run under it, which in turn allocate resources to their individual processes. Just as modern operating systems can transparently pause a process, the Xen hypervisor can pause an operating system, hand control to another for a while, and then seamlessly restart the paused system.</p><p>Because Xen is designed to be small and simple, the hypervisor interacts with the OSs that run under it using a very few well-defined interfaces, which the Xen team refers to as <span class="emphasis"><em>hypercalls</em></span>.<a id="idx-CHP-1-0042" class="indexterm"/></p><p>These hypercalls take the place of a standard operating system's system calls, with a similar interface. In effect, they have the same function—to allow user code to execute privileged operations in a way that can be controlled and managed by trusted code.</p><p>The hypercalls have several design goals and requirements. First, they are <span class="emphasis"><em>asynchronous</em></span> so that hypercalls don't block other processes or other OSs—while one domain waits for a hypercall to finish, another domain can get some CPU time. Second, they are small, simple, and clearly defined—Xen has only about 50 hypercalls, in contrast with over 300 syscalls for Linux. Finally, the hypercalls use a common system of notifications to interact with the Xen hypervisor.</p><div class="sect2" title="Scheduling"><div class="titlepage"><div><div><h2 class="title"><a id="scheduling"/>Scheduling</h2></div></div></div><p>The CPU, regardless of Xen virtualization, is still a physical object, subject to all the messy and intractable laws of physical reality. It can perform only one instruction at a time, and so the various demands on its attention have to be scheduled. Xen schedules processes to run on the CPU in response to instructions from the guest OSs, subject to its own accounting of which guest should have access to the CPU at any given time.<a id="idx-CHP-1-0043" class="indexterm"/></p><p>Each guest maintains its own internal queues of which instructions to run next—which process gets a CPU time slice, essentially. In an ordinary machine, the OS would run the process at the head of a queue on the physical CPU. (Under Linux, the run queue.) On a virtual machine, it instead notifies Xen to run that process for a certain amount of time, expressed in domain-virtual terms.</p><p>The guest can also "make an appointment" with Xen, requesting an interrupt and CPU time at a later time, based on either a <a id="idx-CHP-1-0044" class="indexterm"/>domain-virtual timer or <a id="idx-CHP-1-0045" class="indexterm"/>system timer.</p><p>The domain-virtual timer is used mostly for internal <a id="idx-CHP-1-0046" class="indexterm"/>scheduling between processes—the domU kernel can request that the hypervisor preempt a task and run another one after a certain amount of virtual time has passed. Note that the domain doesn't actually schedule processes directly on the CPU—that sort of hardware interaction has to be handled by the hypervisor.</p><p>The system <a id="idx-CHP-1-0047" class="indexterm"/>timer is used for events that are sensitive to real-world time, such as networking. Using the <a id="idx-CHP-1-0048" class="indexterm"/>system timer, the domain can give up the CPU for a while and request to be woken back up in time to refill the network buffer or send out the next ping.</p><p>The administrator can also tune the <a id="idx-CHP-1-0049" class="indexterm"/>scheduling parameters that Xen uses to allocate resources to domains. There are a number <a id="idx-CHP-1-0050" class="indexterm"/>of different algorithms, with varying degrees of usefulness. See <a class="xref" href="ch07.html" title="Chapter 7. HOSTING UNTRUSTED USERS UNDER XEN: LESSONS FROM THE TRENCHES">Chapter 7</a> for more details on <a id="idx-CHP-1-0051" class="indexterm"/>scheduling.</p></div><div class="sect2" title="Interrupts"><div class="titlepage"><div><div><h2 class="title"><a id="interrupts"/>Interrupts</h2></div></div></div><p>In computing terms, an <span class="emphasis"><em>interrupt</em></span> is a request for attention. An interrupt usually occurs when some piece of hardware needs to interact with its control software (that is, drivers). Traditionally, interrupts must be handled immediately, and all other processes have to wait until the interrupt handler has finished. In the context of virtualization, this is patently unacceptable.<a id="idx-CHP-1-0052" class="indexterm"/></p><p>Xen therefore intercepts interrupts, rather than passing them directly through to guest domains. This allows Xen to retain control of the hardware, <span class="emphasis"><em>scheduling</em></span> interrupt servicing, rather than merely reacting. Domains can register interrupt handlers with the hypervisor in advance. Then, when an interrupt comes in, Xen notifies the appropriate guest domain and schedules it for execution. Interrupts that occur while the domain is waiting to execute are coalesced into a nice package, avoiding unnecessary notifications. This also contributes to Xen's performance because context switches between domains are expensive.</p></div><div class="sect2" title="Memory"><div class="titlepage"><div><div><h2 class="title"><a id="memory"/>Memory</h2></div></div></div><p>The hypervisor has <a id="idx-CHP-1-0053" class="indexterm"/>authority over memory that is both localized and absolute. It must allocate all <a id="idx-CHP-1-0054" class="indexterm"/>memory used by the domains, but it only deals with physical memory and the page table—the guest OSs handle all other memory management functions.<a id="idx-CHP-1-0055" class="indexterm"/></p><p>This, as it turns out, is quite as much as any sensible implementor could desire. Memory, under x86, is difficult and arcane. The Xen authors point out, in a classic understatement, that "the x86 processors use a complex hybrid memory management scheme." <a class="xref" href="ch01s05.html#lets_take_the_example_of_translating_an_" title="Figure 1-2. Let's take the example of translating an address given by an application. First, at the left, we have the address as given. This consists of a segment selector and offset. The MMU looks up the segment selector in the GDT (Global Descriptor Table) to find that segment's location in the linear address space, which is the complete address space accessible to the process (usually 4GB). The offset then acts as an address within that segment. This gives the processor a linear address relative to the process's address space. The MMU then decomposes that address into two indices and an offset—first it looks through the page directory to find the correct page table, then it finds the correct page in the page table, and finally it uses the offset to return a machine address—actual, physical memory.">Figure 1-2</a> shows an overview of address translation on the x86.</p><p>On the most fundamental, hardware-dependent level, or at least the lowest level we're willing to mention here, we have the machine memory. This can be accessed one word at a time, via numbered addresses. That's the final product, shown on the right in <a class="xref" href="ch01s05.html#lets_take_the_example_of_translating_an_" title="Figure 1-2. Let's take the example of translating an address given by an application. First, at the left, we have the address as given. This consists of a segment selector and offset. The MMU looks up the segment selector in the GDT (Global Descriptor Table) to find that segment's location in the linear address space, which is the complete address space accessible to the process (usually 4GB). The offset then acts as an address within that segment. This gives the processor a linear address relative to the process's address space. The MMU then decomposes that address into two indices and an offset—first it looks through the page directory to find the correct page table, then it finds the correct page in the page table, and finally it uses the offset to return a machine address—actual, physical memory.">Figure 1-2</a>.</p><p>However, this approach is too hardware dependent for a modern computer, which needs to be able to swap to disk, memory map I/O, use DMA, and so on. The processor therefore implements <span class="emphasis"><em>virtual memory</em></span>, which provides two advantages for the programmer. First, it allows each process to access its own memory as if it were the only thing running on the computer—that is, as if it had the entirety of physical memory to itself. Second, virtual memory enables a process to access much more memory than is physically available, swapping to disk as necessary.</p><div class="figure"><a id="lets_take_the_example_of_translating_an_"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject1_d1e857"/><img src="httpatomoreillycomsourcenostarchimages333195.png.jpg" alt="Let's take the example of translating an address given by an application. First, at the left, we have the address as given. This consists of a segment selector and offset. The MMU looks up the segment selector in the GDT (Global Descriptor Table) to find that segment's location in the linear address space, which is the complete address space accessible to the process (usually 4GB). The offset then acts as an address within that segment. This gives the processor a linear address relative to the process's address space. The MMU then decomposes that address into two indices and an offset—first it looks through the page directory to find the correct page table, then it finds the correct page in the page table, and finally it uses the offset to return a machine address—actual, physical memory."/></div></div><p class="title">Figure 1-2. Let's take the example of translating an address given by an application. First, at the left, we have the address as given. This consists of a segment selector and offset. The MMU looks up the segment selector in the GDT (Global Descriptor Table) to find that segment's location in the linear address space, which is the complete address space accessible to the process (usually 4GB). The offset then acts as an address within that segment. This gives the processor a linear address relative to the process's address space. The MMU then decomposes that address into two indices and an offset—first it looks through the page directory to find the correct page table, then it finds the correct page in the page table, and finally it uses the offset to return a machine address—actual, physical memory.</p></div><p>Like <a id="idx-CHP-1-0056" class="indexterm"/>physical <a id="idx-CHP-1-0057" class="indexterm"/>memory, virtual <a id="idx-CHP-1-0058" class="indexterm"/>memory is accessed one word at a time, via numbered <a id="idx-CHP-1-0059" class="indexterm"/>addresses. The mapping between physical addresses <a id="idx-CHP-1-0060" class="indexterm"/>and virtual addresses is handled by <span class="emphasis"><em>page tables</em></span>, which associate chunks of physical memory with pages of virtual memory.<a id="idx-CHP-1-0061" class="indexterm"/></p><p>This level of abstraction applies even when there's only one operating system running on the machine. It's one of the basic forms of virtualization, so ubiquitous as to go unnoticed by most non-programmers.</p><p>Xen interposes itself at this point, acting as the sole gatekeeper of the page tables. Because applications have to go through Xen to update their mappings between virtual and physical memory, the hypervisor can ensure that domains only access memory within their reservation—memory that a domain doesn't have access to isn't mapped to any of its pages and therefore doesn't exist from the domain's perspective. <a class="xref" href="ch01s05.html#the_hypervisors_main_role_is_to_validate" title="Figure 1-3. The hypervisor's main role is to validate the domU's updates to the page tables, ensuring that domU only maps memory allocated to it. The domU handles memory using physical pages directly, generating pseudophysical addresses where necessary.">Figure 1-3</a> shows the relationship between the hypervisor, physical memory, and pseudophysical mappings.</p><p>So far so good. x86 handles this partially in hardware, using an area of the processor called the <span class="emphasis"><em>MMU</em></span>, or <span class="emphasis"><em>Memory Management Unit</em></span>.<a id="idx-CHP-1-0062" class="indexterm"/><a id="idx-CHP-1-0063" class="indexterm"/></p><p>Although this mapping should be sufficient to provide memory protection and the <span class="emphasis"><em>illusion</em></span> of contiguous virtual memory, the x86 architecture also uses segmentation to protect memory and increase the amount of addressable memory.<sup>[<a id="CHP-1-FNOTE-6" href="#ftn.CHP-1-FNOTE-6" class="footnote">13</a>]</sup> Application-level addresses are <span class="emphasis"><em>logical addresses</em></span>, each of which includes a 16-bit segment selector and a 32-bit segment offset, which the processor then maps to virtual (or <span class="emphasis"><em>linear</em></span>) <a id="idx-CHP-1-0065" class="indexterm"/>addresses, which are eventually turned into <a id="idx-CHP-1-0066" class="indexterm"/>physical <a id="idx-CHP-1-0067" class="indexterm"/>addresses.<a id="idx-CHP-1-0068" class="indexterm"/></p><div class="figure"><a id="the_hypervisors_main_role_is_to_validate"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject1_d1e952"/><img src="httpatomoreillycomsourcenostarchimages333197.png.jpg" alt="The hypervisor's main role is to validate the domU's updates to the page tables, ensuring that domU only maps memory allocated to it. The domU handles memory using physical pages directly, generating pseudophysical addresses where necessary."/></div></div><p class="title">Figure 1-3. The hypervisor's main role is to validate the domU's updates to the page tables, ensuring that domU only maps memory allocated to it. The domU handles memory using physical pages directly, generating pseudophysical addresses where necessary.</p></div><p>In practice, however, modern software usually avoids the <a id="idx-CHP-1-0069" class="indexterm"/>segment registers as much as possible—the segments are simply made equivalent to the entire address space, which has the practical effect of allowing processes to ignore their existence. However, the unused segmentation model provides a perfect way for Xen to protect its own memory reservation. The Xen hypervisor reserves a small amount of memory at the beginning of each domain's allocation and arranges the domain's segments so that they don't include the hypervisor's memory region.<a id="idx-CHP-1-0070" class="indexterm"/><a id="idx-CHP-1-0071" class="indexterm"/><a id="idx-CHP-1-0072" class="indexterm"/><a id="idx-CHP-1-0073" class="indexterm"/><a id="idx-CHP-1-0074" class="indexterm"/></p><div class="note" title="Note"><h3 class="title"><a id="note-5"/>Note</h3><p><span class="emphasis"><em>This leads to the common</em></span> <a id="idx-CHP-1-0075" class="indexterm"/>/lib/tls <span class="emphasis"><em>problem. See <a class="xref" href="ch15.html" title="Chapter 15. TROUBLESHOOTING">Chapter 15</a> for more information</em></span>.</p></div><p>But wait! There's more. Each memory segment can also be protected by the system of <span class="emphasis"><em>rings</em></span>, which specify the privilege levels that allow access to the memory on a per-process basis. Xen protects the hypervisor by allowing it to run in the privileged ring 0, while the guest OS uses privilege rings 1 through 3. This way, the processor can trap access attempts to the protected beginning of the segment.<a id="idx-CHP-1-0076" class="indexterm"/></p><p>Finally, Xen adds another layer to this memory-management tower of cards. Because the physical memory allocated to a domain is likely to be fragmented, and because most guest OSs don't expect to have to deal with this sort of thing, they must be modified to build a mapping between the hardware and the virtual machine, or <span class="emphasis"><em>real physical</em></span> and <span class="emphasis"><em>pseudophysical</em></span> addresses. This mapping is used for all other components of the guest OS so that they have the illusion of operating in a contiguous address space.</p><p>Thus, <a id="idx-CHP-1-0077" class="indexterm"/>guest OS page tables still contain real machine addresses, which the guest itself translates to pseudophysical addresses for the benefit <a id="idx-CHP-1-0078" class="indexterm"/>of applications. This helps Xen to remain fast, but it means that the guests cannot be trusted to manipulate page tables directly.</p><p>The internal update mechanisms are replaced by two hypercalls that request Xen to manipulate the page tables on the domain's behalf.</p></div><div class="sect2" title="I/O Devices"><div class="titlepage"><div><div><h2 class="title"><a id="io_devices"/>I/O Devices</h2></div></div></div><p>Obviously, the domUs cannot be trusted to handle devices by themselves. Part of Xen's model is that even actively malicious guest domains should be unable to interfere with the hardware or other domains. All device access is through the hypervisor, with the aid of the dom0.<a id="idx-CHP-1-0079" class="indexterm"/></p><p>Xen handles <a id="idx-CHP-1-0080" class="indexterm"/>domain I/O by using <span class="emphasis"><em>device channels</em></span> and <span class="emphasis"><em>virtual devices</em></span>. These are point-to-point links between a frontend device in the domU and a backend device in dom0, implemented as <span class="emphasis"><em>ring buffers</em></span>, as shown in <a class="xref" href="ch01s05.html#a_ring_buffer_is_a_simple_data_structure" title="Figure 1-4. A ring buffer is a simple data structure that consists of preallocated memory regions, each tagged with a descriptor. As one party writes to the ring, the other reads from it, each updating the descriptors along the way. If the writer reaches a &quot;written&quot; block, the ring is full, and it needs to wait for the reader to mark some blocks empty.">Figure 1-4</a>. (Note that these are distinct from x86 privilege rings.)<a id="idx-CHP-1-0081" class="indexterm"/><a id="idx-CHP-1-0082" class="indexterm"/><a id="idx-CHP-1-0083" class="indexterm"/></p><div class="figure"><a id="a_ring_buffer_is_a_simple_data_structure"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject1_d1e1062"/><img src="httpatomoreillycomsourcenostarchimages333199.png.jpg" alt="A ring buffer is a simple data structure that consists of preallocated memory regions, each tagged with a descriptor. As one party writes to the ring, the other reads from it, each updating the descriptors along the way. If the writer reaches a &quot;written&quot; block, the ring is full, and it needs to wait for the reader to mark some blocks empty."/></div></div><p class="title">Figure 1-4. A ring buffer is a simple data structure that consists of preallocated memory regions, each tagged with a descriptor. As one party writes to the ring, the other reads from it, each updating the descriptors along the way. If the writer reaches a "written" block, the ring is full, and it needs to wait for the reader to mark some blocks empty.</p></div><p>The important qualities of these rings is that they're fixed size and lightweight—the domain operates directly on physical <a id="idx-CHP-1-0084" class="indexterm"/>memory, without the need for constant hypervisor intervention. At opportune times, the virtual machine notifies the hypervisor that it's updated the ring, and the hypervisor then takes appropriate action (sending packets, replying with data, etc.).<a id="idx-CHP-1-0085" class="indexterm"/></p><p>For performance reasons, the rings generally contain I/O descriptors rather than actual data. The data is kept in separate buffers accessed through DMA, which Xen maintains control of using principles similar to those for memory allocation. The hypervisor also locks the pages in question, ensuring that the application doesn't try to give them away or use them incorrectly.</p><p>As the contents of a ring buffer are read, they're replaced by empty descriptors, indicating that the buffer has space for more data. Meanwhile, the reading process moves on to the next buffer entry. At the end of the buffer, it simply wraps around.<a id="I_indexterm1_d1e1080" class="indexterm"/></p><p>When a ring fills up, the backend device silently drops data intended for it. This is analogous to a network card or disk filling its buffer and usually results in a re-request for the data at a more convenient time.</p></div><div class="sect2" title="Networking"><div class="titlepage"><div><div><h2 class="title"><a id="networking"/>Networking</h2></div></div></div><p>The <a id="idx-CHP-1-0086" class="indexterm"/>networking architecture (shown in <a class="xref" href="ch01s05.html#the_domu_uses_the_netfront_or_network_fr" title="Figure 1-5. The domU uses the netfront or network frontend driver as its network device, which then transparently flips packets to the netback driver in the dom0. The packets then go through the Linux software bridge, traverse Linux's network stack (including interaction with iptables and friends), and finally go to the network via Linux's network driver.">Figure 1-5</a>) of Xen is designed to reuse <a id="idx-CHP-1-0087" class="indexterm"/>as much code as possible. Xen provides <a id="idx-CHP-1-0088" class="indexterm"/>virtual network interfaces to domains and functions, via device channels, as a medium by which packets can move from a virtual interface in a guest domain to a virtual interface in the driver domain. Other functions are left to standard networking tools.</p><div class="figure"><a id="the_domu_uses_the_netfront_or_network_fr"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject1_d1e1109"/><img src="httpatomoreillycomsourcenostarchimages333201.png.jpg" alt="The domU uses the netfront or network frontend driver as its network device, which then transparently flips packets to the netback driver in the dom0. The packets then go through the Linux software bridge, traverse Linux's network stack (including interaction with iptables and friends), and finally go to the network via Linux's network driver."/></div></div><p class="title">Figure 1-5. The domU uses the netfront or network frontend driver as its network device, which then transparently flips packets to the netback driver in the dom0. The packets then go through the Linux software bridge, traverse Linux's network stack (including interaction with iptables and friends), and finally go to the network via Linux's network driver.</p></div><p>The hypervisor functions solely as a data channel by which packets can move from the physical network interface to the domU's virtual interface. It mediates access between domains, but it doesn't validate packets or perform accounting—these are handled by iptables rules in dom0.</p><p>Accordingly, the virtual network interface is relatively simple—a buffer to receive packets, a buffer to send them, and a hypercall to notify the hypervisor that something has changed.</p><p>The other side of this is that there's a lot of configurability in Xen's networking because you can act on the virtual interfaces using all the standard Linux tools. For more information on networking and suggestions on how to use this nigh-unlimited power, see <a class="xref" href="ch05.html" title="Chapter 5. NETWORKING">Chapter 5</a>.</p></div><div class="sect2" title="Block Devices"><div class="titlepage"><div><div><h2 class="title"><a id="block_devices"/>Block Devices</h2></div></div></div><p>In practical terms, <span class="emphasis"><em>block devices</em></span> are disks or disklike devices. MD arrays, filesystem images, and physical disks all fall under the general category of block devices.<a id="idx-CHP-1-0089" class="indexterm"/><a id="idx-CHP-1-0090" class="indexterm"/></p><p>Xen handles block devices in much the same way as network devices. The hypervisor exports <span class="emphasis"><em>virtual block devices</em></span> (often referred to as VBDs) to the domUs and relies on the dom0 to provide backend drivers that map the functionality of the real block device to the VBD. The system of rings and limited hypercalls is also similar, as shown in <a class="xref" href="ch01s05.html#a_domus_request_for_a_block_device_begin" title="Figure 1-6. A domU's request for a block device begins with the blkfront or block frontend driver, which uses a buffer in the hypervisor to interact with the block backend driver in domain 0. Blkback then reads or writes the requested blocks through dom0's block device driver (which can be a SCSI driver, IDE, fibre channel, etc.).">Figure 1-6</a>.</p><div class="figure"><a id="a_domus_request_for_a_block_device_begin"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject1_d1e1147"/><img src="httpatomoreillycomsourcenostarchimages333203.png.jpg" alt="A domU's request for a block device begins with the blkfront or block frontend driver, which uses a buffer in the hypervisor to interact with the block backend driver in domain 0. Blkback then reads or writes the requested blocks through dom0's block device driver (which can be a SCSI driver, IDE, fibre channel, etc.)."/></div></div><p class="title">Figure 1-6. A domU's request for a block device begins with the blkfront or block frontend driver, which uses a buffer in the hypervisor to interact with the block backend driver in domain 0. Blkback then reads or writes the requested blocks through dom0's block device driver (which can be a SCSI driver, IDE, fibre channel, etc.).</p></div><p>Xen relies on the dom0 to create block devices and provide device drivers that map physical devices to Xen virtual devices.</p><p>For more information about this, see <a class="xref" href="ch04.html" title="Chapter 4. STORAGE WITH XEN">Chapter 4</a>.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-1-FNOTE-6" href="#CHP-1-FNOTE-6" class="para">13</a>] </sup>This is untrue for <a id="idx-CHP-1-0064" class="indexterm"/>AMD64, which does away with segmentation entirely. Instead, Xen on x86_64 uses page-level protection for its memory regions. Stranger things in Heaven and Earth, Horatio.</p></div></div></div>
<div class="sect1" title="Putting It Together"><div class="titlepage"><div><div><h1 class="title"><a id="putting_it_together"/>Putting It Together</h1></div></div></div><p>In general, all of these implementation details demonstrate Xen's focus on simplicity and code reuse. Where possible, the Xen developers have chosen to focus on providing and managing channels between physical devices and virtual devices, letting Linux userspace tools and kernel mechanisms handle arbitration and device access. Also, the actual work is offloaded as much as possible to the dom0 so as to reduce the complexity of the hypervisor and maximize device support.</p><p>For the administrator, this means that Xen can be administered and monitored, by and large, with standard tools, and that most interactions with Xen take place at the level of the dom0. When Xen is installed and domains are running, the Xen domains act like normal, physical machines, running unmodified userspace programs, with some caveats. Let's move on to the next chapter to see how to set this up in practice.<a id="I_indexterm1_d1e1165" class="indexterm"/></p></div></body></html>