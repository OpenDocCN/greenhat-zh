- en: Part IV. The Big Picture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Our legal department advised us not to say “the network is the computer” here*'
  prefs: []
  type: TYPE_NORMAL
- en: —
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 16. Parasitic Computing, or How Pennies Add Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Where the old truth that having an army of minions is better than doing the
    job yourself is once again confirmed*'
  prefs: []
  type: TYPE_NORMAL
- en: —
  prefs: []
  type: TYPE_NORMAL
- en: I hope you’ve enjoyed the ride so far. I’ve discussed a number of fancy problems
    that affect the security and privacy of information from its input at the keyboard
    to its ultimate destination hundreds or thousands of miles away. But it is too
    early for either of us to throw a party; something is missing from the picture—something
    far bigger than what we have discussed so far. The dark matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with our story so far is simple: communications do not occur in
    a void. Although the process of exchanging data is usually limited to two systems
    and a dozen or so intermediate ones, the grand context of all events simply cannot
    be ignored; the properties of the surrounding environment can shape the reality
    of a chitchat between endpoints in profound ways. We cannot ignore the relevance
    of systems that are not directly involved in communications or the importance
    of all the tiny, seemingly isolated bits of individually trivial events that data
    meets along its path. It can be fatal to focus only on what appears relevant to
    a specific application or a particular case, as I hope this book has shown you
    thus far.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than fall into this shortsighted trap, I’ve chosen to embrace the grand
    scheme of things in all its glory. Thus, the fourth and last part of this book
    focuses exclusively on the security of networking as a whole and discusses the
    Internet as an ecosystem, instead of a collection of systems accomplishing specific
    tasks. We pay tribute to the seemingly inert matter that binds the world together.
  prefs: []
  type: TYPE_NORMAL
- en: This part of the book begins with an analysis of a concept that appears to be
    the most appropriate way to make the transition. For many computer geeks, this
    concept, called parasitic computing, has revolutionized the way we think of the
    Internet.
  prefs: []
  type: TYPE_NORMAL
- en: Nibbling at the CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A humble research paper published in letters to *Nature* by Albert-Laszlo Barabasz,
    Vincent W. Freeh, Hawoong Jeong, and Jay B. Brochman in 2001^([[108](apb.html#ftn.CHP-16-BIB-1)])
    could easily have gone unnoticed. At first glance, this letter did not seem worthy
    of much attention; in fact, it posed a seemingly laughable proposition. The authors
    suggest that traffic could be created within well-established network protocols
    such as TCP/IP that would pose (as a message) a trivial arithmetic challenge—a
    problem to be solved—to a remote computer; the remote system would unwittingly
    solve the problem while parsing the message and preparing a response. But why
    would anyone waste time casting riddles at emotionless machines? What could one
    gain from this? Wouldn’t it be as much fun to solve them yourself? Of course,
    the answer is quite interesting.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there is a business to solving puzzles with a computer: much of today’s
    cryptography is based on the relative difficulty of solving a set of so-called
    non-polynomial^([[33](#ftn.CHP-16-FN-1)]) (NP) problems. NP-complete problems
    seem to take pleasure in crashing every codebreaker’s party at the least opportune
    times. The ability to solve them efficiently—whether with enormous computing power,
    clever algorithms, or both—would likely take a lucky inventor one step closer
    to world domination. There’s the incentive, then, but how would one do it?'
  prefs: []
  type: TYPE_NORMAL
- en: The method proposed in the research is quite novel. The paper first states that
    many NP problems in mathematics can be easily expressed in terms of Boolean satisfiability
    (SAT) equations. SAT equations represent these problems as Boolean logic operations,
    effectively constructing a sequence of parameters and variables (a Boolean formula).
    A classic example of an SAT formula might be
  prefs: []
  type: TYPE_NORMAL
- en: '| P = (x[1] XOR x[2]) AND (~x[2] AND x[3]) |'
  prefs: []
  type: TYPE_TB
- en: Here, *P* is the formula (problem) itself, and *x*[1]to *x*[3] are binary inputs,
    or parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are 2³ possible combinations of values for x[1], x[2], and x[3],
    only one of them makes *P* true: x[1] = 1, x[2] = 0, x[3] = 1\. Hence, we say
    that only this triplet is a solution to *P*. Finding solutions to SAT problems
    boils down to determining a set of values for all variables in the equation, for
    which the whole formula that incorporates those variables has a logic value of
    truth. Although trivial SAT problems like the one shown earlier are easy to solve,
    even without invoking any solving mechanism other than trial and error, more complex
    multivariable cases are indeed NP complete, and, consequently, other NP problems
    can be reduced to SAT problems in polynomial (meaning sane) time.'
  prefs: []
  type: TYPE_NORMAL
- en: And here lies the problem. We can formulate a hard NP problem in terms of SAT,
    but this does not buy us much. As of this writing, when it comes to a non-trivial
    equation, even the best SAT-solving algorithms known aren’t much more effective
    than a brute-force search whereby all possibilities are tried, and the value of
    the formula is evaluated for each possibility. This means that if we have a SAT
    problem and enough computing power to even consider approaching it, attempting
    a solution using brute force is not such an insane approach, and we would not
    get much further by with a more sophisticated one. Anyway, there’s not much to
    lose by trying.
  prefs: []
  type: TYPE_NORMAL
- en: 'And here’s the revelation that binds SAT problems and TCP/IP networking. The
    basic observation made by the researchers is fairly obvious (or should be, if
    you subscribe to *Nature*): the checksumming algorithm of TCP (or IP), as discussed
    in [Chapter 9](ch09.html "Chapter 9. Foreign Accent"), although in principle designed
    for a wholly different purpose than solving equations, is nothing more than a
    set of Boolean operations subsequently performed on bits of the input message.
    After all, at the low level, the algorithm boils down to pure Boolean logic carried
    out on words of the transmitted packet. They conclude that, by providing specific
    contents of the packet (“input”), the remote system can thus be forced to carry
    out a set of arithmetic operations and then evaluate its correctness—its agreement
    with the checksum declared in the TCP or IP header.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the operation performed by the remote system during the checksumming
    process is in every single iteration exactly the same, it has a functionality
    sufficient to serve as a universal logic gate, a mechanism we remember from [Chapter 2](ch02.html
    "Chapter 2. Extra Efforts Never Go Unnoticed"). By interleaving the actual tested
    input with carefully chosen “control” words that invert or otherwise alter the
    partial checksum computed thus far, it is possible to carry out any Boolean operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This, in turn, means that SAT logic can be easily re-created using a specific
    sequence control and “input” bits in a packet once the data is exposed to a checksumming
    algorithm; equation variables (chosen this or the other way) are interleaved with
    fixed words that are used to transmogrify the current checksum value so that the
    outcome of the next operation mimics a specific Boolean operator. The final result—the
    value to which a packet sums—denotes the final outcome: the logic value of a formula
    to be evaluated.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the satisfiability test is quite accidentally carried out by the remote
    recipient when, upon arrival, it attempts to validate the checksum. If the checksum
    comes out as 1 (or as some other value that in our SAT computation system corresponds
    to an SAT statement evaluating true), it passes the satisfiability test for the
    variable values chosen for this particular packet (and the traffic is passed to
    higher layers and acted upon). If the checksum fails, the formula has not been
    satisfied, and the packet is dropped silently. In other words, if our input bits
    denoted a specific hypothesis, the recipient had either verified it or proved
    it wrong, taking different actions depending on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Further, a party wanting to solve an SAT problem quickly can prepare a set of
    all possible combinations of variable values (inputs) for a given formula, interleave
    it with information that causes the inputs to combine with others in the most
    desirable way, stuff this information into TCP packets, and send them out (nearly
    in parallel) to a large number of hosts around the globe. The checksum for a packet
    would be set manually to a value we know the “hypothesis” would produce if proven
    true, instead of actually calculating it. Only hosts that receive packets with
    variable values for which the formula evaluates to the desired value would respond
    to the traffic; other systems would simply disregard such traffic as corrupted
    due to the checksum mismatch. The sender can thus determine the correct solution
    without performing massive computations and can simply look up the set of values
    used in packets sent to those hosts that replied to a request.
  prefs: []
  type: TYPE_NORMAL
- en: The research goes further and reports on a successful attempt to solve an NP
    problem using real-world hosts across the globe, thus providing not only theoretical
    background, but also actual confirmation of the approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The impact of this technique is quite subtle, but also important: it proves
    that it is possible to effectively “outsource” computations to unaware and unwilling
    remote parties on the network, including sets of operations needed to solve real-world
    computing problems, without actually attacking these systems, taking them over,
    installing malicious software, or otherwise interfering with legitimate tasks.
    One person can thus, effectively, divide a specific computational task among a
    large number of systems. In the process, they can consume only a tiny and negligible
    fraction of a system’s computing power that could nevertheless add up to the equivalent
    of a decent supercomputer, when millions of systems work on a problem together.'
  prefs: []
  type: TYPE_NORMAL
- en: World domination at hand? Not so fast.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[33](#CHP-16-FN-1)]) In complexity theory, polynomial problems can be solved
    by a Turing machine in time that is polynomially proportional to input length
    (number or size of variables for which the answer must be found). This means that
    the time needed to solve a polynomial problem corresponds directly to the input
    length raised to a constant exponent, which can be zero (causing the time not
    to depend on input length at all, as with testing for parity). Non-polynomial
    (NP) problems have no known solutions of this nature and may require dramatically
    more time to solve as the input length increases, exhibiting, for example, exponential
    dependency. A subset of NP problems, known as NP complete, are proven to have
    no polynomial time solutions. NP problems are generally regarded as “hard” for
    nontrivial inputs, whereas P problems are less expensive to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: . . . or, perhaps, not just yet. The approach suggested in the aforementioned
    research is revolutionary and interesting, but not necessarily a particularly
    practical way to build a supercomputer by stealing from the rich. The amount of
    bandwidth needed to sustain a reasonable computing rate, and the amount of computations
    needed to prepare trivia for other systems to solve, is quite high. As a result,
    this scheme is not efficient enough to outsource the solving of complex mathematical
    problems to a global supercluster of unwilling victims.
  prefs: []
  type: TYPE_NORMAL
- en: In the scheme outlined earlier, the requirement of exponential computing power
    is exchanged for the requirement of exponential bandwidth. This is not necessarily
    a decent trade-off, particularly because only relatively simple tests can be pushed
    out, considering the packet size limitations of most networks. (All of them could
    likely be solved in the time it takes to transmit this data over Ethernet.) This
    technique proves that the attack is possible and provides a truly universal venue
    to facilitate it, but using more specific attack scenarios might yield much more
    useful results.
  prefs: []
  type: TYPE_NORMAL
- en: Other ways of stealing negligible amounts of individual computing power are
    perhaps more interesting as ways to achieve impressive computing power at a low
    cost. For example, certain types of client software (such as web browsers) can
    be easily used to execute even fairly complex algorithms in a relatively trivial
    way. One such example, a “Chinese lottery” computing scheme detailed in RFC 3607,^([[109](apb.html#ftn.CHP-16-BIB-2)])
    is used by a tiny Java applet that Jean-Luc Cooke’s [md5crk.com](http://md5crk.com)
    website encourages webmasters to add to their web pages. Once this applet is added
    to a site, every visitor to it can execute the applet on their system, borrowing
    a negligible amount of CPU cycles in order to contribute them to a project aimed
    at finding MD5 shortcut function collisions. (Collisions are two different messages
    that produce the same shortcut. They are elusive and anecdotal, although most
    definitely possible,^([[34](#ftn.CHP-16-FN-2)]) beings that can allow us to better
    understand the weaknesses of shortcut functions and could empirically prove and
    demonstrate that MD5 is too weak to be a match for today’s computers.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Java applets are small pieces of machine-independent programs that are by default
    executed by web browsers in special, restricted “sandbox” environments. They have
    no access to local disk storage and (only in theory) no ability to do any harm,
    though they can use limited network connectivity to perform computations and to
    add certain visual elements to a web page. They are most commonly used to enhance
    websites with additional features, such as interactive games, visual effects,
    and so on. But Jean-Luc used these applets to do something else: to find likely
    candidates for collisions using the joint computing power of hundreds or thousands
    of systems around the world, simultaneously.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle behind the applet’s operation was trivial: The applet was executed
    on client systems worldwide whenever a cooperating website was visited; then,
    once launched, the applet tried to calculate MD5 shortcuts for different randomly
    chosen messages. This continued until a shortcut that matched a certain arbitrarily
    chosen and fixed masking pattern was found. Such a pattern could be “any shortcut
    with zero for the last four bytes” or something similar. The pattern was chosen
    so that it does not take too long to find a suitable shortcut by trial and error
    (so that the person does not have to leave the web page and stop the code before
    it is found), but so that only a small fraction of all possible shortcuts would
    match the rule.'
  prefs: []
  type: TYPE_NORMAL
- en: Once a suitable message was found, the program “phoned home” with the candidate.
    The author could then examine the submissions. The applet had already examined
    and rejected a number of collision candidates, and only submitted those that matched
    a predefined condition (ones that were partly identical). Because much less variation
    is possible in the data collected this way, the likelihood of a collision in a
    chunk of *n* entries is considerably higher than for purely random data. By analogy,
    the likelihood of running into two visually indistinguishable apples in an amount
    of fruit we are capable of going through within one day is higher if we order
    for delivery only those apples that have nearly the same weight and color, as
    opposed to purchasing a wagon of arbitrary fruit.
  prefs: []
  type: TYPE_NORMAL
- en: Although somewhere in the gray area of cyber-ethics, this ingenious approach
    first openly deployed by [md5crk.com](http://md5crk.com) really worked and provided
    a good demonstration of how parasitic computing can be both quite effective and
    stealthy. It appears that the ability to steal processor cycles originally intended
    to be used for “rightful” purposes is well within reach, and perhaps used more
    often that we want it to be. And this possibility is here to stay.
  prefs: []
  type: TYPE_NORMAL
- en: But, a cranky skeptic continues, can parasitic computing do more than just nibble
    tiny bits of CPU power to facilitate cracking encryption schemes, a task few of
    us are truly interested in?
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[34](#CHP-16-FN-2)]) While this book was being prepared for printing, a team
    of Chinese researchers from Shandong University—Xiaoyun Wang, Dengguo Feng, Xuejia
    Lai, and Hongbo Yu—advised of a technique for finding and provided samples of
    MD4, MD5, HAVAL-128, and RIPEMD-128 collisions. This is one of the more important
    bits of news in modern cryptography, and confirmation that those functions are
    inadequate for some security-related applications. While the [md5crk.com](http://md5crk.com)
    project has closed down, its contributions to exploring the field of parasitic
    computing remain valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parasitic Storage: The Early Days'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you shout, acoustic waves move through the air, gradually losing energy
    and dispersing in all directions. However, if they encounter a solid obstacle
    along the way they will likely bounce, and, if the angle is just right, they will
    bounce back to you. The audible result is that a split second after shouting you
    will hear an echo of your own voice.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what happens when an information theory geek reads their code aloud standing
    on the top of a mountain, directing their words toward a rocky valley? I thought
    you’d never ask. In such case, they cannot help but make a clever observation:
    if they read it fast and then immediately forget about what they just recited
    (because they become preoccupied with other matters), they can still eventually
    recover the information he when it bounces back off the bottom of the valley and
    is echoed back. Voilà—a convenient data storage mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: Sounds ridiculous? Maybe we are just too young. Early types of computer memory
    modules used a similar acoustic technique that allowed the processor to store
    some information “offline” and recover it later. Instead of using air (through
    which waves spread a bit too fast to provide reasonable storage capacities without
    building extremely large memory units), a mercury-filled drum was used (an environment
    in which acoustic waves propagate much more slowly). The principle remained the
    same, however, and even gave an interesting meaning to the term *memory leak*.
    Such a device, *mercury delay line memory*, was used, for example, in the famous
    UNIVAC I.^([[35](#ftn.CHP-16-FN-3)])
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this slow, bulky, dangerous, and inconvenient sort of memory was
    dropped in favor of other solutions as soon as the technology matured. However,
    the invention itself had some charm to it, and wouldn’t fade into oblivion that
    easily. A short presentation by Saqib A. Khan at the DefCON conference in Las
    Vegas in 2002 revived it and gave us the first hints about how to use the properties
    of a large-scale network to construct similar types of momentary storage using
    the Internet as a medium. But this time, the description of acoustic memory did
    not sound ridiculously primitive, but rather unbelievably cool to all hackers
    and geeks watching this short slide show. Acoustic memory had made its comeback
    in style.
  prefs: []
  type: TYPE_NORMAL
- en: Because the round-trip times for packets (the time needed for a message to arrive
    at a remote system, and for a response to come back) are nonzero, a certain amount
    of data can always be kept “on the wire” by repeatedly sending out and receiving
    portions of it and waiting for it to echo back. Saqib used ICMP (Internet Control
    Message Protocol) “echo request” (ping) packets to achieve this effect; most systems
    on the Internet respond to such packets with “echo reply,” quoting the original
    payload they received.
  prefs: []
  type: TYPE_NORMAL
- en: This seemed like a cool trick. However, it was also far from practical for any
    reasonable application, because it required frequent retransmissions of portions
    of data. Because ICMP “echo reply” is sent back nearly immediately after the “echo
    request” is received, only a small amount of data could be pushed out before being
    sent back and needing to be recovered off the wire. As a result, the amount of
    data that could be stored this way could be no larger than the amount that the
    user could push out in, at best, a couple of seconds (and more commonly, under
    a tenth of a second).
  prefs: []
  type: TYPE_NORMAL
- en: Ah, but parasitic storage could be improved.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[35](#CHP-16-FN-3)]) Perhaps it is worth noting that a low-capacity, analog
    delay line memory was also used in early implementations of SECAM (Séquentiel
    Couleur avec Mémoire, or Sequential Color with Memory) TV receivers. Unlike NTSC
    or PAL, the SECAM signal uses a reduced color resolution; red and blue chrominance
    components are transmitted alternatively, never both at once. The other component
    must be taken from the preceeding line to determine how a specific pixel should
    look. To make this possible, a memory device needed to be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Making Parasitic Storage Feasible
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2003, Wojciech Purczynski and I coauthored a paper called “Juggling with
    Packets: Parasitic Data Storage.” We took the concept of parasitic storage a bit
    further and considered a number of methods that could be used to dramatically
    extend the Internet’s storage capacity, while conserving the bandwidth needed
    to sustain the information. Our research focused on several other ways to store
    data on remote systems and classified them based on the properties of the storage
    medium (its visibility, volatility, and reliability). We also included a detailed
    discussion of the hypothetical storage capacities for each of the techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: The paper was quite short and—I hope—refreshing and humorous, and it’s included
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Applications, Social Considerations, and Defense
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But what now? What is the benefit of having practical parasitic computing and
    storage schemes, if the benefits are still not nearly good enough to make it a
    tempting alternative to just getting more hardware?
  prefs: []
  type: TYPE_NORMAL
- en: Despite advances in the practical exploitation of parasitic computing, applications
    that aim to extend the sheer computing power or storage space of a traditional
    system may appear pointless when we consider the abundance of cheap memory and
    gigahertz processors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unseen potential of this technology may, however, lie in a wholly different
    set of applications: *volatile computing*. The ability to build usable distributed
    computers that can disperse at will, leaving no physical traces and storing no
    meaningful data at any one location, might be a powerful privacy tool and also
    pose some challenges for forensics and law enforcement. The ability to build volatile
    store-and-keep memory that collapses shortly after taking a single node offline,
    but that does not involve frequent retransmissions of data, might provide a good
    level of deniability for an offender (or an oppressed entity, for that matter)
    and require many common evidence collection procedures to change quite dramatically.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, imagine volatile systems that could, once bootstrapped and initialized,
    sustain themselves for extended periods of time, living in the Internet and taking
    no localized physical presence. Two designs are possible for volatile, distributed
    computer systems, and neither is that absurd:'
  prefs: []
  type: TYPE_NORMAL
- en: Systems can be designed so that they complete a complex task by finding a solution
    in parallel (already largely accomplished by the SAT computing scheme discussed
    previously). The disadvantage of such systems is that the computation result must
    be retrieved and the next iteration of processing must be initiated manually by
    occasionally “reseeding” the entire system from some location. Solutions that
    rely on low-level properties of protocols such as TCP would likely fall into this
    category.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems can be designed so that they execute subsequent iterations of distributed
    computing themselves. All types of abuse of higher-level features (such as embedded
    document-rendering algorithms) and of some network services might be used to facilitate
    this type of activity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each case, the consequences can be quite profound. For example, how do you
    take down a redundant self-repairing machine that uses no single system, but rather
    borrows tiny bits of memory and processing power from others for fractions of
    a second—and uses no vulnerabilities to do so or clearly distinguishable traffic
    that can be filtered out? And isn’t it also a bit disconcerting to realize that
    we would not be able to immediately discern the goals of such a distributed computer?
    Bowing respectfully to the masters of bad science fiction, I believe the domination
    of computers is imminent and want to welcome our new machine overlords.
  prefs: []
  type: TYPE_NORMAL
- en: Food for Thought
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defense against parasitic computing is generally extremely difficult. The ability
    to store data or to cause the other party to perform certain trivial computations
    is often bound to the fundamental functionality of network protocols. This is
    a characteristic that we cannot conceive of removing without wiping out the Internet
    as we know it and introducing a host of new problems more serious than the one
    remedied.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting a single system against becoming a node for parasitic computing is
    also fairly difficult, because the number of resources stolen from a system is
    often a negligible fraction of the idle CPU time and memory and, hence, might
    easily go unnoticed.
  prefs: []
  type: TYPE_NORMAL
- en: Chances are good that parasitic computing has yet to show its full potential
    and that the threat—irrelevant or nonexistent for single systems but significant
    for the net as a whole—is here to stay.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 17. Topology of the Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*On how the knowledge of the world around us may help track down friends and
    foes*'
  prefs: []
  type: TYPE_NORMAL
- en: —
  prefs: []
  type: TYPE_NORMAL
- en: What is the shape of the Internet? No committee oversees it or decides where,
    how, and why it should expand or how new and existing systems should be organized
    or managed. The Internet grows in all directions in ways that are equally driven
    by demand, economics, politics, technology, and blind luck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet the Internet is not a shapeless blob: there are planned, locally governed
    hierarchies of autonomous systems, with core routers surrounded by lesser nodes,
    with links configured by automatic mechanisms or carefully designed by humans.
    The Internet is a spectacular mesh, a complex and fragile cobweb covering the
    entire industrialized and developing world. The task of capturing this ever-changing
    topology appears challenging, but also tempting, especially when we realize how
    we can benefit from the information collected.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll first discuss two notable attempts to map the Internet’s
    topology, and then I’ll moralize once more on the potential uses for the information
    gathered this way to do things that our ancestors could not even dream of.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing the Moment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most comprehensive attempt to map the Internet was undertaken by the Cooperative
    Association for Internet Data Analysis (CAIDA), an organization funded, among
    others, by federal research agencies (NSF, DHS, DARPA) and the industry (Cisco,
    Sun). The organization was formed to come up with traffic and infrastructure analysis
    and tools for the common benefit of the Internet community, in hopes of making
    it better, more reliable, more resilient, and more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Since 2000, one of CAIDA’s flagship public projects has been the creation and
    maintenance of the autonomous system core network map (aka “Skitter”). As of this
    publication, their most recent capture represents data for 12,517 major autonomous
    systems, corresponding to 1,134,634 IP addresses and 2,434,073 links (logical
    paths) between them.
  prefs: []
  type: TYPE_NORMAL
- en: Despite sounding astonishingly arcane, the CAIDA Internet map was created with
    only publicly accessible router BGP configuration data, empirical network testing
    results (traceroute), and WHOIS records for network blocks. This map is organized
    using polar coordinates. Points representing each system are located at an angle
    corresponding to the physical location of a network’s declared headquarters location
    and the radius corresponding to the “peering relevance” of this particular autonomous
    system. The latter parameter is derived by calculating the number of other autonomous
    systems observed to accept traffic from this particular node. Thus, massive core
    systems are located toward the center of the map, whereas systems that have direct
    contact with only a couple of nodes are located near the outer perimeter. Lines
    in the graph simply correspond to peering relations between routers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quite regrettably, we were not allowed to use a graphical representation of
    CAIDA Skitter graphics in the book free of charge. I encourage you, however, to
    see this stunning picture online at [http://www.caida.org/analysis/topology/as_core_network/pics/ascoreApr2003.gif](http://www.caida.org/analysis/topology/as_core_network/pics/ascoreApr2003.gif)
    where it is available to the general public at no cost.
  prefs: []
  type: TYPE_NORMAL
- en: Another noteworthy attempt to map the network used an approach that relied on
    analyzing distances to various networks, as seen from a particular location (in
    this case, from Bell Laboratories), to build a treelike structure quite unlike
    the complex mesh created by CAIDA. Conducted by Bill Cheswick in 2000,^([[110](apb.html#ftn.CHP-17-BIB-1)])
    this analysis resulted in the map shown in [Figure 17-1](ch17.html#bill_cheswickas_map_of_the_internet
    "Figure 17-1. Bill Cheswick’s map of the Internet"). This structure does not parametrize
    the graph depending on the physical or administrative location of a system; the
    relative distance from the center corresponds to the number of hops between that
    node and Bell Labs, however.
  prefs: []
  type: TYPE_NORMAL
- en: Although the two attempts appear to involve massive data collection and analysis,
    it is not prohibitively difficult for an amateur to attempt to map the network
    on even a fairly low-end link. Probing all publicly routable subnets with a single
    packet might require generating only a couple of gigabytes of traffic—the equivalent
    of a couple of hours to one day on a typical DSL connection. The only risk is
    that of upsetting some system administrators, but with the proliferation of computer
    worms and automated attacks, very few have a sensitivity threshold that low. Mapping
    the observed structure of the Internet is possible, and it can be rewarding, especially
    because it can tell us a lot about how the worldwide network is organized.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bill Cheswick’s map of the Internet](httpatomoreillycomsourcenostarchimages1138106.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1. Bill Cheswick’s map of the Internet
  prefs: []
  type: TYPE_NORMAL
- en: But, as it turns out, the data, such as the information acquired by CAIDA, Bill
    Cheswick, or just about any proficient user of the Net, can also be successfully
    used to better understand the nature and better examine the origin of a mysterious
    traffic we might one day stumble upon.
  prefs: []
  type: TYPE_NORMAL
- en: Using Topology Data for Origin Identification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spoofed traffic is one of the Internet’s major problems—or, at the very least,
    one of its more annoying woes. Blindly spoofed packets with bogus or specially
    chosen but deceptive source addresses can be used to abuse trust relationships
    between computers, inject malicious contents (such as unsolicited bulk mailings)
    without leaving conclusive traces and legitimate origin information, and so forth.
    Blind spoofing can also be used to hide the identity of an attacker conducting
    system probes (“decoy scanning” discussed earlier in [Chapter 13](ch13.html "Chapter 13. Smoke
    and Mirrors")). The worst plague of all is, however, spoofing used to carry out
    Denial of Service (DoS) attacks.
  prefs: []
  type: TYPE_NORMAL
- en: In a typical DoS attack, the administrator is given a chance to see the origin
    of malicious traffic directed against one of their services (and presumably intended
    to bring it down and cause inconvenience or loss to the operator). It is possible
    to randomly spoof offending packets, however, and in such cases the administrator
    is left helpless, unable to filter out the traffic coming from the attacker without
    cutting off other users. Their only hope is to work with the upstream provider
    to investigate the actual origin of the traffic on the link layer and pass the
    information to the offender’s ISP; this, however, takes time, and lots of it.
    It also requires convincing all parties, without a court order, that the case
    is worthy of investigation (and their time and money). This situation makes it
    particularly important for the system administrator to be equipped with tools
    and methods to differentiate between spoofed and legitimate traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I used to live and work in the United States (I live in Poland these days),
    my colleague Mark Loveless decided to implement an idea originally proposed by
    Donald McLachlan: He would measure time to live (TTL) on network traffic between
    him and the presumed sender of a packet to automatically determine whether an
    incoming packet had been spoofed. The challenge of identifying the origin of a
    network packet in a world where the information cannot be trusted is important,
    and the ability to do so, even if only in a specific subset of cases, would greatly
    benefit many analytic and administrative tasks, for the reasons mentioned earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand Donald and Mark’s idea, consider that the remote system, from
    which we are seeing traffic, is at a specific logical distance from us, separated
    by a given number of network devices. Thus, all packets legitimately sent by this
    system exhibit a certain TTL on arrival, corresponding to the default initial
    TTL configured on that system, minus the number of intermediate systems the packet
    has gone through (as discussed in [Chapter 9](ch09.html "Chapter 9. Foreign Accent")).
    However, for spoofed traffic that presumably originates on a wholly different
    network, the initial TTL and the distance is most likely different than the aforementioned
    observation would suggest. Mark’s tool, despoof,^([[111](apb.html#ftn.CHP-17-BIB-2)])
    compares the TTLs observed on specially induced and previously received traffic
    in order to distinguish between legitimate and falsified traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, although this method might work well in individual cases when used
    against unsuspecting attackers, at least two problems are associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: A paranoid attacker can measure distances before the attack and choose a TTL
    that matches the expected value. Although possible, this trick is a bit difficult
    to implement. For one thing, the attacker might be physically unable to set TTL
    high enough to achieve a specific value that would match the expected value of
    a real packet once the packet reaches its destination. This attacker’s plan could
    be thwarted if the system that he is trying to impersonate uses a default TTL
    at or near 255 (the maximum possible) and he is farther from the target than the
    system he is trying to impersonate (hence it is very much impossible for him to
    send a packet that would, upon arrival at the destination, have the desired TTL).
    Of course, few systems use the highest possible TTL, and it is rare for an attacker
    to want to impersonate a specific system to begin with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The attacker’s second challenge is that he might not be able to determine the
    exact distance between his victim and the impersonated system if he is nowhere
    near them and does not know the routing specifics between these hosts. But if
    the victim uses despoof to dynamically implement filtering rules to cut off malicious
    packets, the attacker might just try various TTLs from various sources until he
    sees that the victim is no longer capable of making the distinction. (This would
    be obvious: the system targeted would begin to exhibit the effects of a successful
    attack, such as a performance impact.)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each time a suspicious packet is received, the recipient must start an investigation
    and then wait for the results to arrive. This makes it impractical to use despoof
    as a basis for an automatic defense, especially in response to DoS attacks. However,
    this method is still quite useful for determining the actual origin of a “decoy
    scan.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the knowledge of a specific network’s topology, it is difficult to do
    any better than with despoof; the TTL analysis technique implemented by this tool
    is good enough to recognize and stop many common probes and individual attacks,
    but what next?
  prefs: []
  type: TYPE_NORMAL
- en: Combine Mark’s tool with real-time data on the network structure, and apply
    passive fingerprinting to determine the initial TTL of a system that sends specific
    requests, and this technique becomes much more powerful. This additional data
    allows us to perform an initial passive assessment of incoming traffic by comparing
    observed and initial TTLs with the expected distance indicated by the network
    map. ^([[36](#ftn.CHP-17-FN-1)]) Because the distance we should be seeing can
    be determined without initiating any active probe of the network topology data,
    we can instantly distinguish between legitimate and malicious traffic without
    much effort. This, in turn, makes it possible to react to massive incidents quite
    reliably and to detect individual low-profile probes without alerting the attacker
    that a spoofing detection system is in place.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, there is plenty to be gained from taking the structure of a network
    into account when considering peer-to-peer relations. But spoofing detection is
    only the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[36](#CHP-17-FN-1)]) In such an approach, the comparison of TTLs must be
    performed with a certain error margin, because there can be several additional
    hops within internal networks. Too, some routes are asymmetric, and their lengths
    can differ slightly depending on the direction in which the traffic is being exchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Network Triangulation with Mesh-Type Topology Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Network triangulation is a considerably more interesting application of network
    topology mesh-type data for the purpose of traffic analysis. We can use network
    triangulation to determine the approximate location of an attacker who sends spoofed
    packets without the help of those operating the underlying routing backbone, as
    soon as they choose to attack more than one target at once or in succession—truly,
    happiness in misery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, to be correct: although triangulation works best when the attacker chooses
    several targets, in some scenarios, it may work quite well even if they choose
    to attack only one service. In particular, we might be able to observe the same
    attack from different viewpoints when the object attacked has several IP addresses
    and the service is being served from several physical locations in order to distribute
    the load and make the entire structure fault tolerant (as is common with web services).
    In all other scenarios, we can get a range of data on an attack when system administrators
    notice that more than one system is being targeted by an attacker and share their
    data about the incident.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the case, once data believed to come from a single source is
    seen at more than one destination, we can triangulate. For each destination at
    which the traffic is seen, only a specific set of networks are at a distance that
    can be determined by observing the distance through which the offending packet
    has traveled (again, possible to find out by examining TTL^([[37](#ftn.CHP-17-FN-2)])).
    An intersection of all those sets for every observation point would yield a smaller
    set—or, often, only a single network—from which the attack could originate, as
    shown in [Figure 17-2](ch17s04.html#a_naive_network_triangulation_colon_only "Figure 17-2. A
    naive network triangulation: only one origin is consistent with all observations.
    The attacker may be spoofing source addresses, but can’t fool the victims.").'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to perform the trace on our own frees us from unconditional dependence
    on ISPs and helps to precisely pinpoint who is attacking or probing our network—and
    perhaps find out why.
  prefs: []
  type: TYPE_NORMAL
- en: Although this approach is much more difficult to thwart than traditional despoofing,
    a clever attacker might still be able to fool an observer by randomizing a different
    TTL (or range of TTLs) to be used for every target. True, we know of no tools
    to do this at present, but this might change.
  prefs: []
  type: TYPE_NORMAL
- en: The battle is lost? Nope—there is a way to keep perpetrators from fooling us
    that way.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[37](#CHP-17-FN-2)]) Even if the tool uses random TTLs, it is possible to
    judge the distance by using the maximum TTL observed if a number of packets can
    be observed at each destination (which is almost always the case). For example,
    if the scan tool randomizes initial TTLs in the range of 32 to 255, but for several
    thousand packets received at the destination, none had a TTL higher than 247,
    the host is quite likely to be 255 – 247 = 8 systems away.
  prefs: []
  type: TYPE_NORMAL
- en: Network Stress Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The solution, dubbed “network stress analysis,” comes in the form of a fine
    piece of research presented by Hal Brunch and Bill Cheswick at the LISA conference
    in 2000.^([[112](apb.html#ftn.CHP-17-BIB-3)]) Brunch and Cheswick proposed an
    interesting use for tree-type network topology data (similar to the graph shown
    earlier in [Figure 17-1](ch17.html#bill_cheswickas_map_of_the_internet "Figure 17-1. Bill
    Cheswick’s map of the Internet")) obtained for a specific location. They came
    up with a way to use the data to detect the origin of a particular type of spoofed
    traffic: Denial of Service. The approach itself is fairly trivial and is based
    on the assumption that such an attack would stress not only the system against
    which it is being carried out, but also interim routers, and that this stress
    could be externally measured by the victim and used to—almost literally—go back
    and find a yarn by pulling the wire.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A naive network triangulation: only one origin is consistent with all observations.
    The attacker may be spoofing source addresses, but can’t fool the victims.](httpatomoreillycomsourcenostarchimages1138108.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17-2. A naive network triangulation: only one origin is consistent with
    all observations. The attacker may be spoofing source addresses, but can’t fool
    the victims.'
  prefs: []
  type: TYPE_NORMAL
- en: The job of stress-testing network links is achieved by first building or obtaining
    a tree of links from your location to all networks on the Internet and then going
    through subsequent branches of this tree structure when an attack occurs. For
    each branch (which, in reality, denotes a connection to a higher-order router),
    we can iteratively measure network load on this node by sending test traffic to
    or through the router associated with it. (In this particular paper, a UDP [User
    Datagram Protocol] chargen is used, but ICMP requests or any other type of messages
    could be also used.) We choose a more loaded node as a potential candidate for
    the incoming traffic and then list and test all branches that spawn from this
    node until we trace the traffic back to the origin.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-3](ch17s04.html#recursive_attack_backtrace_using_network "Figure 17-3. Recursive
    attack backtrace using network topology data and stress testing") illustrates
    a simple trace-back scenario. In the first phase, the attacked system attempts
    to measure the performance of the three nearest Internet routers when an attack
    occurs; it concludes that the first (topmost) router is the most saturated.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on this information, the victim chooses to test only those routers directly
    connected (peering) with this device. In this particular figure, only three devices
    are to be tested (the remaining six are not to be tested because they do not peer
    with this device), and, again, the first one is the most loaded. The process continues
    until a router that is directly connected to a specific network, for which a physical
    location and owner information can be discovered through public databases, is
    determined to be the final endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '![Recursive attack backtrace using network topology data and stress testing](httpatomoreillycomsourcenostarchimages1138110.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-3. Recursive attack backtrace using network topology data and stress
    testing
  prefs: []
  type: TYPE_NORMAL
- en: 'A potential problem arises: some devices might be heavily loaded for reasons
    other than handling DoS traffic; other devices might have plenty of spare CPU
    cycles and would not be considerably affected by relaying malicious traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve this issue, the research proposes putting an artificial short-term
    load on the router (by generating additional traffic) and then observing how this
    test affects the bandwidth and latency of the DoS requests; if this particular
    device is indeed involved in relaying malicious packets, the attack rate should
    drop when we put load on the device (again, likely by generating additional bogus
    TCP, UDP, or ICMP requests, designed more to consume a device’s CPU power than
    to congest its interfaces). Hence, there should be a correlation only on those
    branches that are involved in servicing the malicious traffic.
  prefs: []
  type: TYPE_NORMAL
- en: This brilliant and simple scheme had been successfully used in test environments.
    However, because it involves interacting with routers and placing an additional
    load on them, certain ethical considerations come into play when we consider using
    it in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Food for Thought
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main difficulty in using the techniques discussed in this chapter for tracking
    down attackers is that we need to construct and update network maps for each location.
    It is not immediately clear how often such maps should be refreshed, and what
    methods would prove most reliable and least intrusive.
  prefs: []
  type: TYPE_NORMAL
- en: Another possible issue is that much of the core Internet infrastructure is redundant.
    Some alternative routes may be chosen only when the primary route fails or is
    saturated, though in some cases the switch may occur as a part of load balancing.
    Thus, some empirical maps may become obsolete in a matter of minutes or hours—although
    such cases are not very common.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, although private, individual uses of various despoofing tactics
    may prove very successful, there are many open questions that need to be answered
    before we can deploy such techniques on a large scale—and some of the questions
    are not as much about technical issues.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 18. Watching the Void
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*When looking down the abyss, what does not kill us makes us stronger*'
  prefs: []
  type: TYPE_NORMAL
- en: —
  prefs: []
  type: TYPE_NORMAL
- en: We have looked at many ways to discover information and intercept data by observing
    the communications between two systems or by watching the side effects of such
    communications. The story does not end here, however. Sometimes, by averting our
    eyes from the target we hope to probe, we can see even more.
  prefs: []
  type: TYPE_NORMAL
- en: An entire set of methods commonly referred to as “black-hole monitoring” is
    dedicated to observing and analyzing unwanted or unsolicited traffic that arrives
    accidentally, erroneously, or in mangled form at a specific destination. These
    methods most often include simply running a packet dump utility and then painstakingly
    analyzing and theorizing about every single observance.
  prefs: []
  type: TYPE_NORMAL
- en: Although in a perfect world, we should gain nothing by looking for data where
    we are not supposed to find it, in reality we can use these methods to gather
    abundant bits of information and invaluable hints as to the condition of a network
    as a whole. Even though the information is mostly random and we cannot choose
    who we listen to, we can still benefit from the effort.
  prefs: []
  type: TYPE_NORMAL
- en: Direct Observation Tactics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One application of black-hole monitoring lies in detecting and analyzing global
    attack trends. Many black hat hackers in possession of new attack techniques often
    simply scan large blocks of network addresses to find vulnerable targets that
    can be compromised and ultimately used for illicit activities (presumably to collect
    skip hosts^([[38](#ftn.CHP-18-FN-1)]) or to build attack drone networks for automated
    attacks). We can use black-hole monitoring to alert us to new vulnerabilities
    being exploited in the wild by simply observing increased standard network scan
    activity from various sources.
  prefs: []
  type: TYPE_NORMAL
- en: Many network administrators deploy black-hole monitoring. They sometimes combine
    it with honeypots (in which a fake “lure” system is put out on the network to
    catch attackers and intercept their tools and identify their techniques^([[113](apb.html#ftn.CHP-18-BIB-1)]))
    to produce an advance warning system that will allow them to be the first to know
    about impeding breakouts of worms and other malware. (You can also use black-hole
    traffic to calibrate “noise levels” and detect targeted attacks against your servers
    more efficiently, without picking up automated, indiscriminate malicious activity.)
  prefs: []
  type: TYPE_NORMAL
- en: Researchers such as Dug Song and Jose Nazario (Jose most recently in his book
    *Defense and Detection Strategies against Internet Worms*^([[114](apb.html#ftn.CHP-18-BIB-2)]))
    have attempted to analyze black-hole activity during massive outbreaks of network
    worms. Their goal is to better understand and model the distribution (initial
    propagation and reinfection) dynamics of the network and to test the efficiency
    and persistence of the worms’ infection algorithms. Their research will help us
    to devise future defenses against massive, distributed threats, while providing
    valuable insight into the state of the network today. Some examples of their findings
    are shown in [Figure 18-1](ch18.html#windows_worm_propagation_characteristics
    "Figure 18-1. Windows worm propagation characteristics") through [Figure 18-4](ch18s02.html#worm_persistence_over_time._note_that_th
    "Figure 18-4. Worm persistence over time. Note that there is no trivial spike-falloff
    pattern for CodeRed and that the model behaves like a biological population model.").
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-1](ch18.html#windows_worm_propagation_characteristics "Figure 18-1. Windows
    worm propagation characteristics") shows how a worm propagates during an outbreak.
    The data is based on the number of observed attack attempts on TCP port 137, a
    part of the Windows NetBIOS implementation, which is installed by default on all
    Windows computers and targeted by many types of self-propagating malware. Notice
    in this figure how, after a week of initial propagation—when both the number of
    infected sites (sources) and systems attacked on the observed black-hole network
    were steadily and rapidly increasing—a stabilization period suddenly stretches
    for over a month with dramatic peaks and valleys. Such a propagation footprint
    is highly unique to a worm and the network conditions in which it operates; it
    also reflects the subtleties of the target selection and infection algorithms
    used by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-2](ch18.html#sqlsnake_worm_target_selection_algorithm "Figure 18-2. SQLSnake
    worm target selection algorithm histogram; note the nonuniform but generally continuous
    coverage of the address space") shows a different aspect of the worm propagation
    algorithm and depicts the properties of the target selection algorithm. In this
    case, a popular worm that targeted Microsoft SQL servers appears to have fairly
    continuous coverage of the address space (although addresses with octets between
    about 200 and 225 are chosen considerably more often, and the worm appears to
    skip values over 225 altogether).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Windows worm propagation characteristics](httpatomoreillycomsourcenostarchimages1138112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-1. Windows worm propagation characteristics
  prefs: []
  type: TYPE_NORMAL
- en: '![SQLSnake worm target selection algorithm histogram; note the nonuniform but
    generally continuous coverage of the address space](httpatomoreillycomsourcenostarchimages1138114.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-2. SQLSnake worm target selection algorithm histogram; note the nonuniform
    but generally continuous coverage of the address space
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-3](ch18.html#the_slapper_worm_target_selection_algori "Figure 18-3. The
    Slapper worm target selection algorithm histogram. This shows a far more uniform
    distribution, but noncontinuous coverage with gaps suggesting that the least significant
    bits of each of the “random” addresses are constant—perhaps due to a programming
    glitch.") shows the same graph for a different network worm, Slapper. This worm
    targeted Linux systems, exploiting a flaw in a popular OpenSSL encryption library.
    The algorithm appears to offer considerably more uniform, but much less continuous
    coverage, with gaping holes across certain values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Slapper worm target selection algorithm histogram. This shows a far more
    uniform distribution, but noncontinuous coverage with gaps suggesting that the
    least significant bits of each of the “random” addresses are constant—perhaps
    due to a programming glitch.](httpatomoreillycomsourcenostarchimages1138116.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-3. The Slapper worm target selection algorithm histogram. This shows
    a far more uniform distribution, but noncontinuous coverage with gaps suggesting
    that the least significant bits of each of the “random” addresses are constant—perhaps
    due to a programming glitch.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 18-4](ch18s02.html#worm_persistence_over_time._note_that_th "Figure 18-4. Worm
    persistence over time. Note that there is no trivial spike-falloff pattern for
    CodeRed and that the model behaves like a biological population model.") shows
    worm persistence patterns over time. For example, some worms appear to die off
    steadily as systems are patched and disinfected, while others use algorithms that
    cause sudden and recurring rise and fall patterns (familiar to anyone who has
    experimented with population or epidemiology models based on natural phenomena).'
  prefs: []
  type: TYPE_NORMAL
- en: As Jose and his colleagues strive to demonstrate, black-hole monitoring may
    not be only a routine and perhaps completely needless activity, but also a great
    way to discover the secret life of all things malicious. Alas, the story does
    not end there. By observing only the traffic we consider aimed at us, we miss
    the most interesting bits of data.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[38](#CHP-18-FN-1)]) Skip host is a system used as an intermediate hop for
    carrying out further attacks or other illicit activity (such as sending spam).
    This technique makes it more difficult to track the ultimate offender, because
    their origin is not directly known, and a number of administrators or jurisdictions
    must cooperate to find them.
  prefs: []
  type: TYPE_NORMAL
- en: Attack Fallout Traffic Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other application of black-hole monitoring relies on observing traffic that
    was never aimed at us in the first place, but which is merely a side effect of
    other activity.
  prefs: []
  type: TYPE_NORMAL
- en: '![Worm persistence over time. Note that there is no trivial spike-falloff pattern
    for CodeRed and that the model behaves like a biological population model.](httpatomoreillycomsourcenostarchimages1138118.png.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-4. Worm persistence over time. Note that there is no trivial spike-falloff
    pattern for CodeRed and that the model behaves like a biological population model.
  prefs: []
  type: TYPE_NORMAL
- en: Here we can see how a number of common reconnaissance and attack schemes use
    address spoofing to conceal an attacker’s identity. The assumption is that an
    administrator will have difficulty differentiating decoy traffic from bogus addresses
    from the attacker’s actual probes. Although as I’ve shown in previous chapters,
    this approach does not guarantee the attacker complete anonymity; in order to
    successfully “despoof” the traffic, an administrator must implement extensive
    logging and additional measures at the time of the attack. Because these procedures
    are not always implemented, attackers can often spoof their attacks quite effectively
    and remain out of the spotlight.
  prefs: []
  type: TYPE_NORMAL
- en: Whether packets are spoofed or not, the attacked system will in good faith respond
    to all requests including those allegedly coming from made-up addresses. However,
    only the responses to packets with a proper source address arrive back at the
    sender; all other probes generate responses that are scattered all around the
    Internet, and we can often catch them.
  prefs: []
  type: TYPE_NORMAL
- en: Although it may seem unlikely that we will receive such a misdirected packet,
    remember that a considerable number of SYN+ACK, RST+ACK, and RST packets are generated
    in response to decoy scans or SYN flood attacks. The Internet address space appears
    vast, with millions of packets typically involved in such attacks, but it is quite
    likely that over time, some will reach every single network block. Although the
    likelihood of a single, randomly generated spoofed packet bouncing back to a specific
    address is only 1 in 4,294,967,296 (1 to 2^(32)), assuming that a typical small
    subnet assigned to a small company or organization usually consists of 256 addresses
    (class C network or equivalent), this probability is increased to 1 in 16,777,216
    (1 to 2^(24)). This can be further improved by ruling out address ranges that
    are known to be reserved for special purposes or which are otherwise not noteworthy
    and thus excluded in certain types of attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Because the face of a single SYN packet is about 40 bytes (and compresses well
    in bulk) and a typical network link available to a casual attacker has a throughput
    of approximately 10 to 150 kilobytes per IP layer per second (low-end DSL and
    T1 line, respectively), he can push out 250 to nearly 3,000 packets in this time
    frame—or 900,000 to circa 10,000,000 packets per hour.^([[39](#ftn.CHP-18-FN-2)])
  prefs: []
  type: TYPE_NORMAL
- en: For a typical DoS attack to produce any noticeable results and cause major inconvenience
    to the victim, it usually has to be carried out for several hours or days. (The
    attacker wants to inconvenience their victim for as long as possible.) As a result,
    dozens to hundreds of millions of packets are sent, generating a similar number
    of SYN+ACK or RST+ACK replies.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this huge amount of traffic, it’s quite reasonable to expect that even
    a relatively small entity could notice the fallout of a small SYN flood attack
    as it happens, even if the recipient host drops many attack packets. Furthermore,
    administrators able to monitor class B equivalent networks (65,356 addresses,
    usually owned by larger companies, ISPs, research institutions, and so forth)
    would be able to pick up much smaller events quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Because all the fallout replies in a spoofed DoS attack include certain details
    of the messages fabricated by the attacker to trigger those responses in the first
    place (such as port and sequence numbers, timing information, and so forth), we
    can use these replies to extract important information about the type and scale
    of attack. We can use these replies to determine whether a specific service has
    been targeted, how many systems have been targeted, the bandwidth available to
    the attacker, and the tool used to perform the attack (by examining source port
    selection, chosen sequence numbers, and “random” IP patterns^([[40](#ftn.CHP-18-FN-3)])).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, by analyzing the sources of these ricochet responses, we might notice
    that a particular network segment is under attack or be able to identify global
    “hostility trends,” perhaps to better prepare if a specific industry or business
    is being targeted. We can also use this information to learn about attacks that
    are being covered up by the victim or to identify false claims of attacks. (Claims
    that certain targets are being attacked by cyber-terrorists are sometimes made
    as a PR stunt to justify financial losses or to push a specific political agenda.
    Of recent, some experts accused SCO of taking their servers off-line and pretending
    to be a victim of a coordinated DoS attack to discredit the Linux users community.)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: ^([[39](#CHP-18-FN-2)]) Note that determined, seasoned attackers proficient
    in Denial of Service attacks often have dozens or hundreds of “zombie” nodes at
    their command, thus increasing this estimate dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: ^([[40](#CHP-18-FN-3)]) For example, some tools only “spoof” packets from even
    or odd IP addresses due to coding flaws. Analyses similar to those conducted by
    Jose Nazario and others typically prove to be as good at pinpointing attack tools
    as they do at identifying worms.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Malformed or Misdirected Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This application for monitoring black holes relies on monitoring traffic that
    does not seem to make any sense, but that still appears to reach a specific destination.
    To better illustrate the problem, allow me this digression.
  prefs: []
  type: TYPE_NORMAL
- en: In 1999, a group of friends, colleagues in Poland, and I began a humble after-hours
    project. Our goals were to track down a hard-to-explain set of RST+ACK packets
    that we had noticed arriving at networks we maintained and to monitor unusual
    and unsolicited traffic patterns arriving at unused network segments in general.
    It was great fun, and, as you might imagine, it resulted in a good deal of speculation
    when we tried to reasonably explain some of the most unusual cases. Our research
    also enabled us to learn more about the world around us as we encountered some
    exceedingly bizarre and seemingly inexplicable traffic that, once properly analyzed,
    provided more insight into the vast conspiraces of our wired world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although formally abandoned, this project ended up in my private “Museum of
    Broken Packets,”^([[115](apb.html#ftn.CHP-18-BIB-3)]) a semihumorous web page
    dedicated to tracking down, documenting, and explaining packets that should never
    have reached their destination or that should never have looked the way they did.
    The stated purpose of the museum was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this museum is to provide a shelter for strange, unwanted, malformed
    packets—abandoned and doomed freaks of nature—as we, mere mortals, meet them on
    the twisted paths of our grand journey called life. Our exhibits—or, if you wish,
    inhabitants—are often just a shadow of what they used to be before they met a
    hostile, faulty router. Some of them were born deformed in the depth of a broken
    IP stack implementation. Others were normal packets, just like their friends (you
    or me), but got lost looking for the ultimate meaning of their existence and arrived
    where we should never have seen them. In every case, we try to discover the unique
    history of each packet’s life, and to help you understand how difficult it is
    to be a sole messenger in the hostile universe of bits and bytes.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And this is what the last type of black-hole monitoring boils down to. Although
    the task can appear pointless at first, it is foolish to assume so. The museum
    made it possible to passively uncover dark secrets about various proprietary devices
    and well-protected networks, and running such an experiment elsewhere would undoubtedly
    result in the same or greater accomplishments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the exhibits in my museum include marvels such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Packets originating from networks with a specific type of web accelerator, router,
    or firewall; the device appends, strips, or otherwise mangles some of the data.
    A good example is a flaw in certain Nortel CVX devices that is responsible for
    the occasional stripping of TCP headers from packets (as discussed in [Chapter 11](ch11.html
    "Chapter 11. In Recognition of Anomalies")). The uniqueness of this flaw enables
    us to learn a good deal about a number of remote networks without having to actually
    go out and probe them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several line noise exhibits, showing packets containing either utter garbage
    or data that certainly did not belong to a specific connection. One of the most
    surprising exhibits is unsolicited traffic containing data that appears to be
    a dump of .de DNS zone contents (a listing of all domains in Germany). The traffic
    could not have originated just anywhere, because mere mortals have no rights to
    obtain such a list. Instead, it must have originated at an authorized party able
    to obtain and transfer this data and must have been mangled either by the sender
    or by a device somewhere along the way. Although all cases shed little light on
    the nature of mishaps on the network, cases such as this one often enrich the
    observer with unexpected—and often valuable—findings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other noteworthy exhibits included cases of apparent espionage camouflaged to
    appear as regular traffic and many other coding or networking hiccups. But enough
    bragging—if you feel compelled to find out more, visit [http://lcamtuf.coredump.cx/mobp/](http://lcamtuf.coredump.cx/mobp/).
  prefs: []
  type: TYPE_NORMAL
- en: Food for Thought
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many regard black-hole monitoring as just another way to detect attacks against
    their systems (and perhaps an expensive way, given the scarcity of public IP space
    resources). But the real value of this technique is that it makes it possible
    to not only identify known attacks (something that can be done just as well in
    many other locations, without wasting IP space), but also detect and analyze subtle
    patterns that would otherwise be lost below the “noise level” in an extensively
    used network.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, performing this type of black-hole monitoring is not easy and remains
    expensive. It takes time to learn how to find that needle in the haystack of the
    usual worm and black hat activity that, in a sufficiently extensive network, usually
    bears no significance beyond statistical reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, for the joy of finally finding the needle, it is often worth a try.
  prefs: []
  type: TYPE_NORMAL
