<html><head></head><body><div class="chapter" title="Chapter&#xA0;12.&#xA0;HVM: BEYOND PARAVIRTUALIZATION"><div class="titlepage"><div><div><h1 class="title"><a id="hvm_beyond_paravirtualization"/>Chapter 12. HVM: BEYOND PARAVIRTUALIZATION</h1></div></div></div><div class="informalfigure"><div class="mediaobject"><a id="I_mediaobject12_d1e12883"/><img src="httpatomoreillycomsourcenostarchimages333191.png.jpg" alt="image with no caption"/></div></div><p>Throughout this book, we've described the standard Xen virtualization technique, <span class="emphasis"><em>paravirtualization</em></span>. Paravirtualization is a wonderful thing—as we've already outlined, it allows excellent performance and strong isolation, two goals that are difficult to achieve simultaneously. (See <a class="xref" href="ch01.html" title="Chapter 1. XEN: A HIGH-LEVEL OVERVIEW">Chapter 1</a> for more on this topic.)</p><p>However, paravirtualization requires a modified operating system. Although porting an OS to Xen is relatively painless, by the standards of such things, it's not a trivial task, and it has the obvious limitation of being impossible with closed source operating systems. (While the Xen team did port Windows to Xen during the development process, no <span class="emphasis"><em>released</em></span> version of Windows can run under Xen in paravirtualized mode.)</p><p>One way around this is to add extensions to the processor so that it supports virtualization in hardware, allowing unmodified operating systems to run on the "bare metal," yet in a virtualized environment. Both Intel and AMD have done precisely that by extending the x86 architecture.</p><p>Intel <a id="idx-CHP-12-0881" class="indexterm"/>uses the term <span class="emphasis"><em>VT-x</em></span> to refer to the <a id="idx-CHP-12-0882" class="indexterm"/>virtualization extensions for x86.<sup>[<a id="CHP-12-FNOTE-1" href="#ftn.CHP-12-FNOTE-1" class="footnote">73</a>]</sup> (<span class="emphasis"><em>VT-i</em></span> is Itanium's hardware virtualization. For our purposes, it's basically identical to VT-x. We will not discuss VT-i separately.<sup>[<a id="CHP-12-FNOTE-2" href="#ftn.CHP-12-FNOTE-2" class="footnote">74</a>]</sup>) <a id="idx-CHP-12-0884" class="indexterm"/>AMD likewise has a set of virtualization extensions.<sup>[<a id="CHP-12-FNOTE-3" href="#ftn.CHP-12-FNOTE-3" class="footnote">75</a>]</sup> Most of the Xen-related documentation that you might find refers to the extensions by their internal code name, <span class="emphasis"><em>Pacifica</em></span>, but you'll also see the AMD marketing term <span class="emphasis"><em>SVM</em></span>, for <span class="emphasis"><em>secure virtual machine</em></span>.<a id="idx-CHP-12-0885" class="indexterm"/><a id="idx-CHP-12-0886" class="indexterm"/></p><p>Although VT-x and Pacifica are implemented slightly differently, we can gloss over the low-level implementation details and focus on capabilities. Both of these are supported by Xen. Both will allow you to run an unmodified operating system as a domU. Both of these will suffer a significant performance penalty on I/O. Although there are differences between the two, the differences are hidden behind an abstraction layer.</p><p>Properly speaking, it's this abstraction layer that we refer to as HVM (hardware virtual machine)—a cross-platform way of hiding tedious implementation details from the system administrator. So, in this chapter, we'll focus on the HVM interface and how to use it rather than on the specifics of either Intel's or AMD's technologies.</p><div class="sect1" title="Principles of HVM"><div class="titlepage"><div><div><h1 class="title"><a id="principles_of_hvm"/>Principles of HVM</h1></div></div></div><p>If you think back to the "concentric ring" security model that we introduced in <a class="xref" href="ch01.html" title="Chapter 1. XEN: A HIGH-LEVEL OVERVIEW">Chapter 1</a>, you can characterize the HVM extensions as adding a <span class="emphasis"><em>ring –1</em></span> inside (that is, with superior privileges to) ring 0. New processor opcodes, invisible to the virtual machine, are used to switch between the superprivileged mode and normal mode. The unmodified operating system runs in ring 0 and operates as usual, without knowing that there's another layer between it and the hardware. When it makes a privileged system call, the call actually goes to ring –1 rather than the actual hardware, where the hypervisor will intercept it, pause the virtual machine, perform the call, and then resume the domU when the call is done.</p><p>Xen also has to handle <a id="idx-CHP-12-0887" class="indexterm"/>memory a bit differently to accommodate unmodified guests. Because these unmodified guests aren't aware of Xen's memory structure, the hypervisor needs to use <a id="idx-CHP-12-0888" class="indexterm"/>shadow page tables that present the illusion of contiguous physical memory starting at address 0, rather than the discontiguous physical page tables supported by Xen-aware operating systems. These shadows are in-memory copies of the page tables used by the hardware, as shown in <a class="xref" href="ch12.html#all_guest_page_table_writes_are_intercep" title="Figure 12-1. All guest page table writes are intercepted by the hypervisor and go to the shadow page tables. When the execution context switches to the guest, the hypervisor translates pseudophysical addresses found in the shadow page tables to machine physical addresses and updates the hardware to use the translated page tables, which the guest then accesses directly.">Figure 12-1</a>. Attempts to read and write to the page tables are intercepted and redirected to the shadow. While the guest runs, it reads its shadow page tables directly, while the hardware uses the pretranslated version supplied to it by the hypervisor.</p><div class="figure"><a id="all_guest_page_table_writes_are_intercep"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject12_d1e12999"/><img src="httpatomoreillycomsourcenostarchimages333241.png.jpg" alt="All guest page table writes are intercepted by the hypervisor and go to the shadow page tables. When the execution context switches to the guest, the hypervisor translates pseudophysical addresses found in the shadow page tables to machine physical addresses and updates the hardware to use the translated page tables, which the guest then accesses directly."/></div></div><p class="title">Figure 12-1. All guest page table writes are intercepted by the hypervisor and go to the shadow page tables. When the execution context switches to the guest, the hypervisor translates pseudophysical addresses found in the shadow page tables to machine physical addresses and updates the hardware to use the translated page tables, which the guest then accesses directly.</p></div><div class="sect2" title="Device Access with HVM"><div class="titlepage"><div><div><h2 class="title"><a id="device_access_with_hvm"/>Device Access with HVM</h2></div></div></div><p>Of course, if you've been paying attention thus far, you're probably asking how the HVM domain can access devices if it hasn't been modified to <a id="idx-CHP-12-0889" class="indexterm"/>use the Xen virtual block and network devices. Excellent question!</p><p>The answer is twofold: First, during boot, Xen uses an emulated <a id="idx-CHP-12-0890" class="indexterm"/>BIOS to provide simulations of standard PC devices, including disk, network, and framebuffer. This BIOS comes from the open source <a id="idx-CHP-12-0891" class="indexterm"/>Bochs emulator at <a class="ulink" href="http://bochs.sourceforge.net/">http://bochs.sourceforge.net/</a>. Second, after the system has booted, when the domU expects to access SCSI, IDE, or Ethernet devices using native drivers, those devices are emulated using code originally found in the QEMU emulator. A userspace program, <code class="literal">qemu-dm</code>, handles translations between the native and emulated models of <a id="idx-CHP-12-0892" class="indexterm"/>device access.</p></div><div class="sect2" title="HVM Device Performance"><div class="titlepage"><div><div><h2 class="title"><a id="hvm_device_performance"/>HVM Device Performance</h2></div></div></div><p>This sort of translation, where we have to mediate hardware access by breaking out of virtualized mode using a software device emulation and then reentering the virtualized OS, is one of the trade-offs involved in running unmodified operating systems.<sup>[<a id="CHP-12-FNOTE-4" href="#ftn.CHP-12-FNOTE-4" class="footnote">76</a>]</sup> Rather than simply querying the host machine for information using a lightweight page-flipping system, HVM domains access devices precisely as if they were physical hardware. This is quite slow.<a id="idx-CHP-12-0894" class="indexterm"/></p><p>Both AMD and Intel have done work aimed at letting guests use hardware directly, using an <a id="idx-CHP-12-0895" class="indexterm"/>IOMMU (I/O Memory Management Unit) to translate domain-virtual addresses into the real PCI address space, just as the processor's MMU handles the translations for virtual memory.<sup>[<a id="CHP-12-FNOTE-5" href="#ftn.CHP-12-FNOTE-5" class="footnote">77</a>]</sup> However, this isn't likely to replace the emulated devices any time soon.</p></div><div class="sect2" title="HVM and SMP"><div class="titlepage"><div><div><h2 class="title"><a id="hvm_and_smp"/>HVM and SMP</h2></div></div></div><p><a id="idx-CHP-12-0896" class="indexterm"/>SMP (symmetric multiprocessing) works with HVM just as with paravirtualized domains. Each virtual processor has its own control structure, which can in turn be serviced <a id="idx-CHP-12-0897" class="indexterm"/>by any of the machine's physical processors. In this case, by <span class="emphasis"><em>physical processors</em></span> we mean logical processors as seen by the machine, including the virtual processors presented by SMT (simultaneous multithreading or hyperthreading).<a id="idx-CHP-12-0898" class="indexterm"/></p><p>To turn on SMP, include the following in the config file:</p><a id="I_programlisting12_d1e13092"/><pre class="programlisting">acpi=1
vcpus=&lt;n&gt;</pre><p>(Where <span class="emphasis"><em>n</em></span> is an integer greater than one. A single CPU does not imply SMP. Quite the opposite, in fact.)</p><div class="note" title="Note"><h3 class="title"><a id="note-46"/>Note</h3><p><span class="emphasis"><em>Although you can specify more CPUs than actually exist in the box, performance will… suffer. We strongly advise against it</em></span>.</p></div><p>Just as in paravirtualized domains, SMP works by providing a VCPU abstraction for each virtual CPU in the domain, as shown in <a class="xref" href="ch12.html#as_each_domains_time_allocation_comes_up" title="Figure 12-2. As each domain's time allocation comes up, its VCPU's processor state is loaded onto the PCPU for further execution. Privileged updates to the VCPU control structure are handled by the hypervisor.">Figure 12-2</a>. Each VCPU can run on any physical CPU in the machine. Xen's CPU-pinning mechanisms also work in the usual fashion.</p><p>Unfortunately, SMP support isn't perfect. In particular, time is a difficult problem with HVM and SMP. Clock synchronization seems to be entirely unhandled, leading to constant complaints from the kernel with one of our test systems (CentOS 5, Xen version 3.0.3-rc5.el5, kernel 2.6.18-8.el5xen). Here's an example:</p><a id="I_programlisting12_d1e13110"/><pre class="programlisting">Timer ISR/0: Time went backwards: delta=-118088543 delta_cpu=25911457 shadow=157034917204
off=452853530 processed=157605639580 cpu_processed=157461639580</pre><div class="figure"><a id="as_each_domains_time_allocation_comes_up"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject12_d1e13115"/><img src="httpatomoreillycomsourcenostarchimages333243.png.jpg" alt="As each domain's time allocation comes up, its VCPU's processor state is loaded onto the PCPU for further execution. Privileged updates to the VCPU control structure are handled by the hypervisor."/></div></div><p class="title">Figure 12-2. As each domain's time allocation comes up, its VCPU's processor state is loaded onto the PCPU for further execution. Privileged updates to the VCPU control structure are handled by the hypervisor.</p></div><p>One other symptom of the problem is in <a id="idx-CHP-12-0899" class="indexterm"/>bogomips values reported <a id="idx-CHP-12-0900" class="indexterm"/>by <span class="emphasis"><em>/proc/cpuinfo</em></span>—on a 2.4GHz core 2 duo system, we saw values ranging from 13.44 to 73400.32. In the dom0, each core showed 5996.61—an expected value.</p><p>Don't worry, this might be unsettling, but it's also harmless.</p></div><div class="sect2" title="HVM and Migration"><div class="titlepage"><div><div><h2 class="title"><a id="hvm_and_migration"/>HVM and Migration</h2></div></div></div><p><a id="idx-CHP-12-0901" class="indexterm"/>HVM migration works as of Xen 3.1. The migration support in HVM domains is based on that for paravirtualized domains but is extended to account for the fact that it takes place without the connivance of the guest OS. Instead, Xen itself pauses the VCPUs, while <code class="literal">xc_save</code> <a id="idx-CHP-12-0902" class="indexterm"/>handles memory and CPU context. <code class="literal">qemu-dm</code> also takes a more active role, saving the state of emulated devices.<a id="idx-CHP-12-0903" class="indexterm"/></p><p>The point of all this is that you can migrate HVM domains just like paravirtualized domains, using the same commands, with the same caveats. (In particular, remember that attempts to migrate an HVM domain to a physical machine that doesn't support HVM will fail ungracefully.)</p></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-12-FNOTE-1" href="#CHP-12-FNOTE-1" class="para">73</a>] </sup>Intel has a nice introduction to their virtualization extensions at <a class="ulink" href="http://www.intel.com/technology/itj/2006/v10i3/3-xen/1-abstract.htm">http://www.intel.com/technology/itj/2006/v10i3/3-xen/1-abstract.htm</a> and a promotional <a id="idx-CHP-12-0883" class="indexterm"/>overview page at <a class="ulink" href="http://www.intel.com/technology/platform-technology/virtualization/index.htm">http://www.intel.com/technology/platform-technology/virtualization/index.htm</a>. They're worth reading.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-12-FNOTE-2" href="#CHP-12-FNOTE-2" class="para">74</a>] </sup>Also, Gentle Reader, your humble authors lack a recent Itanium to play with. Please forward offers of hardware to <span class="email"><a class="email" href="mailto:lsc@prgmr.com">lsc@prgmr.com</a></span>.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-12-FNOTE-3" href="#CHP-12-FNOTE-3" class="para">75</a>] </sup>AMD has a light introduction to their extensions at <a class="ulink" href="http://developer.amd.com/TechnicalArticles/Articles/Pages/630200615.aspx">http://developer.amd.com/TechnicalArticles/Articles/Pages/630200615.aspx</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-12-FNOTE-4" href="#CHP-12-FNOTE-4" class="para">76</a>] </sup>As Intel points out, the actual implementation of HVM drivers is much better than this naïve model. For example, <a id="idx-CHP-12-0893" class="indexterm"/>device access is asynchronous, meaning that the VM can do other things while waiting for I/O to complete.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-12-FNOTE-5" href="#CHP-12-FNOTE-5" class="para">77</a>] </sup>There's an interesting paper on the topic at <a class="ulink" href="http://developer.amd.com/assets/IOMMU-ben-yehuda.pdf">http://developer.amd.com/assets/IOMMU-ben-yehuda.pdf</a>.</p></div></div></div>
<div class="sect1" title="Xen HVM vs. KVM"><div class="titlepage"><div><div><h1 class="title"><a id="xen_hvm_vs_kvm"/>Xen HVM vs. KVM</h1></div></div></div><p>Of course, if your machine supports virtualization in hardware, you might be inclined to wonder what the point of Xen is, rather than, say, KVM or lguest.</p><p>There are some excellent reasons to consider the idea. <a id="idx-CHP-12-0904" class="indexterm"/>KVM and lguest are both easier to install and less invasive than Xen. They support strong virtualization with good <a id="idx-CHP-12-0905" class="indexterm"/>performance.</p><p>However, <a id="idx-CHP-12-0906" class="indexterm"/>KVM is, at the moment, less mature than Xen. It's not (yet) as fast, even with the kernel accelerator. Xen also supports paravirtualization, whereas KVM does not. Xen PV offers a handy way of migrating domUs and a good way of multiplexing virtual machines—that is, expanding to a two-level VM hierarchy. But honestly, we haven't got much experience with KVM, and we've got quite a lot with Xen.</p><p>Similarly, lguest is smaller, lighter, and easier to install than Xen, but it doesn't support features like SMP or PAE (though 64-bit kernels are in development). Lguest also doesn't yet support suspend, resume, or migration.</p><p>Nonetheless, right now it's difficult to say which is better—all of these technologies are out there being actively developed. If you are truly silly, you might even decide to use some combination, running Xen hypervisors under KVM, with paravirtualized domains under that. Or you might use Xen for now but keep your options open for future deployments, perhaps when HVM-capable hardware becomes more common. These technologies are interesting and worth watching, but we'll stick to our usual "wait and see" policy.</p><p>Indeed, Red Hat has opted to do exactly that, focusing its development efforts on a platform-independent interface layer, libvirt, allowing (we hope) for easy migration between virtualization options. See <a class="xref" href="ch06.html" title="Chapter 6. DOMU MANAGEMENT: TOOLS AND FRONTENDS">Chapter 6</a> for more on libvirt and its associated suite of management tools.</p></div>
<div class="sect1" title="Working with HVM"><div class="titlepage"><div><div><h1 class="title"><a id="working_with_hvm"/>Working with HVM</h1></div></div></div><p>Regardless of what sort of hardware virtualization you want to use, the first thing to do is check whether your machine supports HVM. To find out if you've got a supported <a id="idx-CHP-12-0907" class="indexterm"/>processor, check <span class="emphasis"><em>/proc/cpuinfo</em></span>. Processors that support VT-x will report <code class="literal">vmx</code> in the flags field, while Pacifica-enabled processors report <code class="literal">svm</code>.<a id="idx-CHP-12-0908" class="indexterm"/></p><p>Even if you have a supported processor, many manufacturers have HVM turned off in the BIOS by default. Check <code class="literal">xm dmesg</code> to ensure that the hypervisor has found and enabled HVM support correctly—it should report "(XEN) VMXON is done" for each CPU in the machine. If it doesn't, poke about in the BIOS for an option to turn on hardware virtualization. On our boards, it's under <span class="emphasis"><em>chipset</em></span> <span class="emphasis"><em>features</em></span> and called <span class="emphasis"><em>VT</em></span> <span class="emphasis"><em>Technology</em></span>. Your machine may vary.</p><p>The hypervisor will also report capabilities under <span class="emphasis"><em>/sys</em></span>:</p><a id="I_programlisting12_d1e13245"/><pre class="programlisting"># cat /sys/hypervisor/properties/capabilities
xen-3.0-x86_32p hvm-3.0-x86_32 hvm-3.0-x86_32p</pre><p>In this case, the two "hvm" entries show that HVM is supported in both PAE and non-PAE modes.</p><div class="note" title="Note"><h3 class="title"><a id="note-47"/>Note</h3><p><span class="emphasis"><em>One of the minor advantages of HVM is that it sidesteps the PAE-matching problem that we've been prone to harp on. You can run any mix of PAE and non-PAE kernels and hypervisors in HVM mode, although paravirtualized domains will still need to match PAE-ness, even on HVM-capable machines</em></span>.<a id="idx-CHP-12-0909" class="indexterm"/><a id="idx-CHP-12-0910" class="indexterm"/></p></div></div>
<div class="sect1" title="Creating an HVM Domain"><div class="titlepage"><div><div><h1 class="title"><a id="creating_an_hvm_domain"/>Creating an HVM Domain</h1></div></div></div><p>When you've got the hypervisor and domain 0 running on an HVM-capable machine, creating an HVM domain is much like creating any Xen guest.<a id="idx-CHP-12-0911" class="indexterm"/></p><p>Here's a sample <a id="idx-CHP-12-0912" class="indexterm"/>HVM config file. (It's got a snippet of Python at the beginning to set the appropriate library directory, borrowed from the sample configs distributed <a id="idx-CHP-12-0913" class="indexterm"/>with Xen.)</p><a id="I_programlisting12_d1e13285"/><pre class="programlisting">import os, re
arch = os.uname()[4]
if re.search('64', arch):
arch_libdir = 'lib64'
else:
arch_libdir = 'lib'

device_model = '/usr/' + arch_libdir + '/xen/bin/<a id="idx-CHP-12-0914" class="indexterm"/>qemu-dm'

kernel = "/usr/lib/xen/boot/hvmloader" builder='hvm'
memory = 384
shadow_memory = 16
name = "florizel"
vcpus = 1
vif = [ 'type=ioemu,  bridge=xenbr0' ]
disk = [ 'file:/opt/florizel.img,hda,w', 'file:/root/slackware-12.0-install-dvd.iso,hdc:cdrom,r' ]

boot="cda"

sdl=0
vnc=1
stdvga=0
serial='pty'</pre><p>Most of this is pretty standard stuff. It starts with a snippet of Python to choose the correct version of <code class="literal">qemu-dm</code>, then it launches into a standard domU config. The config file changes for HVM guests are approximately as follows:</p><a id="I_programlisting12_d1e13296"/><pre class="programlisting">builder = "HVM"
device_model = "/usr/lib/xen/bin/qemu-dm" kernel = "/usr/lib/xen/boot/hvmloader"</pre><div class="note" title="Note"><h3 class="title"><a id="note-48"/>Note</h3><p><span class="emphasis"><em>There are other directives you can put in, but these are the ones that you can't leave out</em></span>.</p></div><p>Breaking this down, the domain builder changes from the default to HVM, while the devices change from the standard Xen paravirtualized devices to the QEMU emulated devices. Finally, the <code class="literal">kernel</code> line specifies an HVM loader that loads a kernel from within the HVM domain's filesystem, rather than the Linux kernel that would be specified in a PV configuration.</p><div class="sidebar"><a id="a_digression_on_the_domain_builder"/><p class="title">A DIGRESSION ON THE DOMAIN BUILDER</p><p>Up until now, we've avoided discussing the <a id="idx-CHP-12-0915" class="indexterm"/>domain builder, being content merely to gesture in its direction and note that it builds domains. For most purposes, it's enough to think of it as analogous to the standard bootloader.</p><p>However, it has a much more difficult and involved task than a normal bootloader. When a Xen domU starts, it comes up in a radically different environment from the traditional PC "real mode." Because an operating system's already loaded, the processor is already in protected mode with paging enabled. The job of the domain builder is to bring the new domain up to speed—to generate the mapping between domain-virtual and physical memory, load the VM kernel at the appropriate address, and install interrupt handlers.</p><p>The default builder is linux, which builds a paravirtualized domain. (Usually Linux, but it'll work for most paravirtualized OSs.)</p><p>The situation changes somewhat with HVM because the unmodified operating system isn't expecting to take off running on a fully booted machine. To keep the OS happy, the domain builder initializes a complete simulation of real mode, inserting hooks and installing an emulated BIOS into the appropriate regions of pseudophysical memory.</p><p>Historically, the domain builder's been implementation-dependent, with a "vmx" builder for Intel and a similar "svm" builder for AMD. However, with the advent of HVM as an abstraction layer, the administrator can simply specify HVM and let Xen figure out the details.</p></div><p>We're already familiar with the <code class="literal">kernel</code> line, of course. The <code class="literal">device_model</code> line, however, is new. This option defines a userspace program that handles mediation between real and virtual devices. As far as we know, <code class="literal">qemu-dm</code> is the only option, but there's no reason that other device emulators couldn't be written.</p><p>There are some other directives that are only used by HVM domains.</p><a id="I_programlisting12_d1e13339"/><pre class="programlisting"><a id="idx-CHP-12-0916" class="indexterm"/>shadow_memory = 16
boot="dca"</pre><p>The <code class="literal">shadow_memory</code> directive specifies the amount of memory to use for shadow page tables. (Shadow page tables, of course, are the aforementioned copies of the tables that map process-virtual memory to physical memory.) Xen advises allocating at least 2KB per MB of domain memory, and "a few" MB per virtual CPU. Note that this memory is <span class="emphasis"><em>in addition</em></span> to the domU's allocation specified in the memory line.</p><p>Finally we have the <code class="literal">boot</code> directive. The entire concept of boot order, of course, doesn't apply to standard Xen paravirtualized domains because the domain config file specifies either a kernel or a bootloader. However, because HVM emulates legacy BIOS functions, including the traditional bootstrap, Xen provides a mechanism to configure the boot order.</p><p>In that vein, it's worth noting that one advantage of HVM is that it can effectively duplicate the QEMU install procedure we've already described, with Xen instead of QEMU. To recap, this allows you to install in a strongly partitioned virtual machine, using the distro's own install tools, and end <a id="idx-CHP-12-0917" class="indexterm"/>with a ready-to-run VM. We'll leave the details as an exercise to the reader, <a id="idx-CHP-12-0918" class="indexterm"/>of course (we wouldn't want to take all the fun out of it).</p><p>Now that you've got the config file written, create the domain as usual:</p><a id="I_programlisting12_d1e13373"/><pre class="programlisting"># xm create florizel
Using config file "./florizel".
Started domain florizel
# xm list
Name                                      ID Mem(MiB) VCPUs State Time(s)
Domain-0                                   0     3458     2 r----- 5020.8
florizel                                   6      396     1 r----- 14.6</pre><div class="sect2" title="Interacting with an HVM Domain"><div class="titlepage"><div><div><h2 class="title"><a id="interacting_with_an_hvm_domain"/>Interacting with an HVM Domain</h2></div></div></div><p>One of the first changes you might notice is that connecting to the console using <code class="literal">xm -c</code> doesn't work. The Xen console requires some infrastructure support to get working, which a PV-oblivious standard distro naturally doesn't have.<a id="idx-CHP-12-0919" class="indexterm"/></p><p>So, while the machine is <a id="idx-CHP-12-0920" class="indexterm"/>booting, let's chat for a bit about how you actually log in to your shiny new domain.</p><p>As you're no doubt sick of us mentioning, HVM is founded on an idea of <span class="emphasis"><em>total hardware simulation</em></span>. This means that when the machine boots, it loads an emulated VGA BIOS, which gets drawn into a graphics window.<a id="idx-CHP-12-0921" class="indexterm"/></p><p>Xen knows about two targets for its emulated VGA graphics: VNC and SDL. VNC is the familiar and well-beloved network windowing system from AT&amp;T, while <span class="emphasis"><em>SDL, Simple DirectMedia Layer</em></span>, is better known as a lightweight hardware-accelerated graphics option.</p><p>We opted to stick with the VNC console for most of our Linux domUs to reap the benefits of network-centric computing.<sup>[<a id="CHP-12-FNOTE-6" href="#ftn.CHP-12-FNOTE-6" class="footnote">78</a>]</sup></p><p>Now that the domain is created, use VNC to access its console:</p><a id="I_programlisting12_d1e13416"/><pre class="programlisting"># vncviewer 127.0.0.1</pre><p>(Or use whatever the IP address of the Xen machine is.) If you have more than one domU using the VNC console, append a display number—for example, to access the console of the second domU:</p><a id="I_programlisting12_d1e13420"/><pre class="programlisting"># vncviewer 127.0.0.1:1</pre><p>If the <code class="literal">vncunused=</code> option in the config file is set, the domain will take the first available display number. If not, it'll take the display number that corresponds to its domain number. We tend to leave it set, but unset is fine too.<a id="I_indexterm12_d1e13427" class="indexterm"/></p><p>Somewhat to our surprise, X11 worked quite well out of the box <a id="idx-CHP-12-0922" class="indexterm"/>with the <code class="literal">vesa</code> driver, taking over the VNC framebuffer console and providing a usable display without further configuration.</p></div><div class="sect2" title="Getting the Standard Xen Console to Work"><div class="titlepage"><div><div><h2 class="title"><a id="getting_the_standard_xen_console_to_work"/>Getting the Standard Xen Console to Work</h2></div></div></div><p>Now, logging in via the graphical console is an irritating bit of overhead and, we would argue, overkill for a server. Fortunately, you can circumvent this by using the standard Xen emulated serial console. First, make sure that your domU config file (e.g., <span class="emphasis"><em>/etc/xen/leontes</em></span>) contains the following:<a id="idx-CHP-12-0923" class="indexterm"/><a id="idx-CHP-12-0924" class="indexterm"/></p><a id="I_programlisting12_d1e13460"/><pre class="programlisting">serial='pty'</pre><p>This directive tells QEMU (and therefore Xen) to pass serial output to the Xen console.</p><p>Now the bootloader needs to be told to pass its messages to the <code class="literal">serial</code> line. Add the following to <span class="emphasis"><em>/boot/grub.conf</em></span> in the domU:</p><a id="I_programlisting12_d1e13472"/><pre class="programlisting">serial --unit=0 --speed=115200 --word=8 --parity=no --stop=1 serial console</pre><p>These two lines give GRUB some settings for the serial port and tell it to actually use the serial port as its console.</p><p>Next, set up the kernel to output its boot information via serial:</p><a id="I_programlisting12_d1e13478"/><pre class="programlisting">title CentOS (2.6.18-8.el5)
root (hd0,0)
kernel  /vmlinuz-2.6.18-8.el5 ro root=/dev/VolGroup00/LogVol00 rhgb quiet console=ttyS0
module /initrd-2.6.18-8.el5.img</pre><p>(If you're loading the Xen hypervisor in HVM mode—which is a perfectly reasonable thing to do—your <em class="filename">menu.lst</em> file will look a bit different, of course.)</p><p>Finally, edit <code class="literal">inittab</code> to start a <code class="literal">getty</code> on the emulated <code class="literal">serial</code> line by adding a line like this:</p><a id="I_programlisting12_d1e13497"/><pre class="programlisting">s1:12345:respawn:/sbin/agetty -L ttyS0 9600 vt100</pre><p>Boot the machine, and you should be able to see messages and log in via both <code class="literal">xm console</code> and VNC.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-12-FNOTE-6" href="#CHP-12-FNOTE-6" class="para">78</a>] </sup>By which I mean, we didn't want to have to stand up and walk over to a test machine to access the console.</p></div></div></div>
<div class="sect1" title="HVM Devices"><div class="titlepage"><div><div><h1 class="title"><a id="hvm_devices"/>HVM Devices</h1></div></div></div><p>If you poke around in your new HVM domain, you'll see devices like "QEMU harddisk," or "RealTek Ethernet controller." These take the place of the corresponding Xen devices, like <code class="literal">xenblock</code> or <code class="literal">xennet</code>. Examine this <code class="literal">dmesg</code> output, for example:</p><a id="I_programlisting12_d1e13518"/><pre class="programlisting">PIIX3: IDE controller at PCI slot 0000:00:01.1 PIIX3: chipset revision 0
PIIX3: not 100% native mode: will probe IRQs later PCI: Setting latency timer
of device 0000:00:01.1 to 64
ide0: BM-DMA at 0xc000-0xc007, BIOS settings: hda:pio, hdb:pio ide0: BM-DMA at 0xc008-0xc00f, BIOS settings: hdc:pio, hdd:pio
Probing IDE interface ide0...
hda: QEMU HARDDISK, ATA DISK drive</pre><p>This shows the QEMU emulated hard drive. Further on, we see:</p><a id="I_programlisting12_d1e13522"/><pre class="programlisting">eth0: RealTek RTL8139 at 0xc200, 00:16:3e:0b:7c:f0, IRQ 11 eth0: Identified
8139 chip type 'RTL-8139'</pre><p>The difference between PV and HVM domains doesn't end in the domU, either. With HVM domains, you'll see <span class="emphasis"><em>tap</em></span> devices in the dom0.</p><p>Think <a id="idx-CHP-12-0925" class="indexterm"/>of the tap devices as QEMU analogues to the vifs discussed earlier—bits shoved into them come out on the virtual network devices in the domU. You can manage them just like vifs—adding them to bridges, configuring them down or up, and so on.</p><div class="sect2" title="Paravirtualized Drivers"><div class="titlepage"><div><div><h2 class="title"><a id="paravirtualized_drivers"/>Paravirtualized Drivers</h2></div></div></div><p>The best solution to the problem of slow emulated <a id="idx-CHP-12-0926" class="indexterm"/>HVM devices, in the context of Xen, is to use paravirtualized drivers that work on the same split-driver model as non-HVM domains—backend drivers in domain 0, with a small frontend driver in domU that communicates with the backend via event channels, using ring buffers and page flipping. (See <a class="xref" href="ch01.html" title="Chapter 1. XEN: A HIGH-LEVEL OVERVIEW">Chapter 1</a> for boring details on Xen's split driver model.)<a id="idx-CHP-12-0927" class="indexterm"/></p><p>XenSource includes such drivers for Windows, of course. For users of open source Xen, Novell distributes an expensive driver package that performs the same function—paravirtualized device support for Windows. GPL drivers also exist for Windows on open source Xen (we discuss those in more detail in <a class="xref" href="ch13.html" title="Chapter 13. XEN AND WINDOWS">Chapter 13</a>).</p><p>However, you can also build PV drivers for Linux HVM domains. These drivers are included with the Xen source tree in the <span class="emphasis"><em>unmodified_drivers</em></span> directory. Unfortunately, the kernel API keeps changing, so the PV drivers might not compile against your kernel version (the drivers with Xen 3.1 refused to compile with kernel versions 2.6.20 and above).</p><div class="sect3" title="Compiling PV Drivers for HVM Linux"><div class="titlepage"><div><div><h3 class="title"><a id="compiling_pv_drivers_for_hvm_linux"/>Compiling PV Drivers for HVM Linux</h3></div></div></div><p>Nonetheless, the best way to figure out if the drivers will work is to try it. Here's how we compiled our drivers out of a standard Xen source tree.<a id="idx-CHP-12-0928" class="indexterm"/><a id="idx-CHP-12-0929" class="indexterm"/></p><a id="I_programlisting12_d1e13575"/><pre class="programlisting"># cd unmodified_drivers/linux-2.6 # ./mkbuildtree
# make -C /usr/src/linux M=$PWD modules</pre><p>This builds standard Xen devices as modules—install them into your modules tree and load them like any other driver, with <code class="literal">insmod</code> or <code class="literal">modprobe</code>. You'll wind up with four modules, one each for block and network devices, one for xenbus, and one for PCI emulation. Load xen-platform-pci first, then xenbus, then block and network.</p><a id="I_programlisting12_d1e13585"/><pre class="programlisting"># insmod xen-platform-pci
# insmod xenbus
# insmod xenblk
# insmod xennet</pre><p>Because we were using Slackware as our domU, we then built a minimal kernel—without drivers for the emulated IDE or Realtek network card—and built an initrd with the Xen devices included.</p><p>We also needed to modify <span class="emphasis"><em>/etc/fstab</em></span> to refer to the Xen backend devices.</p><p>Finally (this is starting to seem like a lot of trouble, isn't it?) we edited the domain's configuration to specify netfront and blkfront instead of the ioemu devices. We did this by changing the device lines:</p><a id="I_programlisting12_d1e13596"/><pre class="programlisting">vif = [ 'type=ioemu, bridge=xenbr0' ]
disk = [ 'file:/opt/florizel.img,ioemu:hda,w', 'file:/root/slackware-12.0-
install-dvd.iso,ioemu:hdc:cdrom,r' ]</pre><p>to:</p><a id="I_programlisting12_d1e13600"/><pre class="programlisting">vif = [ 'bridge=xenbr0' ]
disk = [ 'file:/opt/florizel.img,hda,w', 'file:/root/slackware-12.0-install-
dvd.iso,hdc:cdrom,r' ]</pre><p>and removing the <code class="literal">device_model=</code> line.</p><p>Modify these directions to work with your setup, of course.</p></div></div></div>
<div class="sect1" title="And, for Our Next Trick&#x2026;"><div class="titlepage"><div><div><h1 class="title"><a id="and_for_our_next_trickhellip"/>And, for Our Next Trick…</h1></div></div></div><p>As always, there are some areas of ongoing work. Both Intel and AMD have announced successor techniques for dealing with guest page tables by adding a new level to the page table structure. AMD terms the concept <span class="emphasis"><em>nested paging</em></span>, while Intel calls it <span class="emphasis"><em>Extended Page Tables</em></span>. IOMMU development is another exciting area of research.</p><p>HVM is nice in general, but of course all of this is a prelude to <a class="xref" href="ch13.html" title="Chapter 13. XEN AND WINDOWS">Chapter 13</a>, where we'll apply all of this stuff to getting a virtual Windows machine up and running in HVM mode. Stay tuned!</p></div></body></html>