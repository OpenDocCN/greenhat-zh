- en: 'Chapter 7. HOSTING UNTRUSTED USERS UNDER XEN: LESSONS FROM THE TRENCHES'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章。在 Xen 下托管不受信任的用户：来自战壕的经验教训
- en: '![image with no caption](httpatomoreillycomsourcenostarchimages333191.png.jpg)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![无标题图片](httpatomoreillycomsourcenostarchimages333191.png.jpg)'
- en: Now that we've gone over the basics of Xen administration—storage, networking,
    provisioning, and management—let's look at applying these basics in practice.
    This chapter is mostly a case study of our VPS hosting firm, prgmr.com, and the
    lessons we've learned from renting Xen instances to the public.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Xen 管理的基础——存储、网络、配置和管理工作——让我们看看如何在实践中应用这些基础知识。本章主要是一个案例研究，关于我们的 VPS
    主机公司 prgmr.com，以及我们从向公众出租 Xen 实例中学到的经验教训。
- en: The most important lesson of public Xen hosting is that the users can't be trusted
    to cooperate with you or each other. Some people will always try to seize as much
    as they can. Our focus will be on preventing this tragedy of the commons.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 公共 Xen 主机托管最重要的教训是，用户不能被信任与他们或彼此合作。有些人总是会试图尽可能多地获取资源。我们的重点将在于防止这种公共资源的悲剧。
- en: Advantages for the Users
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户优势
- en: 'There''s exactly one basic reason that a user would want to use a Xen VPS rather
    than paying to colocate a box in your data center: it''s cheap, especially for
    someone who''s just interested in some basic services, rather than massive raw
    performance.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 用户想要使用 Xen VPS 而不是支付在您的数据中心共同放置一个盒子的费用，只有一个基本原因：它便宜，尤其是对于那些只对一些基本服务感兴趣的人，而不是对大量原始性能感兴趣的人。
- en: GRID COMPUTING AND VIRTUALIZATION
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 网格计算与虚拟化
- en: One term that you hear fairly often in connection with Xen is *grid computing*.
    The basic idea behind grid computing is that you can quickly and automatically
    provision and destroy nodes. Amazon's EC2 service is a good example of a grid
    computing platform that allows you to rent Linux servers by the hour.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Xen 相关的一个经常听到的术语是 *网格计算*。网格计算背后的基本思想是你可以快速自动地配置和销毁节点。亚马逊的 EC2 服务是一个很好的网格计算平台示例，允许你按小时租用
    Linux 服务器。
- en: Grid computing doesn't require virtualization, but the two concepts are fairly
    closely linked. One could design a system using physical machines and PXEboot
    for fast, easy, automated provisioning without using Xen, but a virtualization
    system would make the setup more lightweight, agile, and efficient.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网格计算不需要虚拟化，但这两个概念非常紧密地联系在一起。一个人可以设计一个使用物理机器和 PXEboot 的系统，以实现快速、简单、自动化的配置，而不使用
    Xen，但虚拟化系统会使设置更加轻量级、灵活和高效。
- en: There are several open source projects that are attempting to create a standard
    and open interface to provision "grid computing" resources. One such project is
    Eucalyptus ([http://www.eucalyptus.com/](http://www.eucalyptus.com/)). We feel
    that standard frameworks like this—that allow you to easily switch between grid
    computing providers—are essential if "the grid" is to survive.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源项目正在尝试创建一个标准且开放的接口来配置“网格计算”资源。其中一个这样的项目是 Eucalyptus ([http://www.eucalyptus.com/](http://www.eucalyptus.com/))。我们觉得像这样的标准框架——允许你轻松地在网格计算提供商之间切换——如果“网格”要生存下去，是必不可少的。
- en: 'Xen also gives users nearly all the advantages they''d get from colocating
    a box: their own publicly routed network interface, their own disk, root access,
    and so forth. With a 128MB VM, they can run DNS, light mail service, a web server,
    IRC, SSH, and so on. For lightweight services like these, the power of the box
    is much less important than its basic existence—just having something available
    and publicly accessible makes life more convenient.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Xen 还为用户提供与在数据中心共同放置一个盒子几乎相同的所有优势：他们自己的公开路由网络接口、他们自己的磁盘、root 访问等等。使用 128MB 的虚拟机，他们可以运行
    DNS、轻量级邮件服务、Web 服务器、IRC、SSH 等等。对于这些轻量级服务，盒子的功率远不如其基本存在重要——仅仅有东西可用并且公开可访问就使生活更加便利。
- en: You also have the basic advantages of virtualization, namely, that hosting one
    server with 32GB of RAM is a whole lot cheaper than hosting 32 servers with 1GB
    of RAM each (or even 4 servers with 8GB RAM each). In fact, the price of RAM being
    what it is, I would argue that it's difficult to even economically justify hosting
    a general-purpose server with less than 32GB of RAM.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您还有虚拟化的基本优势，即，托管一个具有 32GB RAM 的服务器比托管 32 个每个 1GB RAM 的服务器（或者甚至 4 个每个 8GB RAM
    的服务器）便宜得多。实际上，考虑到 RAM 的价格，我认为即使是经济上证明托管一个小于 32GB RAM 的一般用途服务器也是困难的。
- en: The last important feature of Xen is that, relative to other virtualization
    systems, it's got a good combination of light weight, strong partitioning, and
    robust resource controls. Unlike some other virtualization options, it's consistent—a
    user can rely on getting exactly the amount of memory, disk space, and network
    bandwidth that he's signed up for and approximately as much CPU and disk bandwidth.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Xen的最后一个重要特性是，与其他虚拟化系统相比，它拥有轻量级、强大分区和稳健资源控制的良好组合。与一些其他虚拟化选项不同，它是连贯的——用户可以依赖获得他注册的确切数量的内存、磁盘空间和网络带宽，以及大约相同数量的CPU和磁盘带宽。
- en: Shared Resources and Protecting Them from the Users
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享资源及其保护
- en: Xen's design is congruent to good security.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Xen的设计与良好的安全性相一致。
- en: — Tavis Ormandy *[http://taviso.decsystem.org/virtsec.pdf](http://taviso.decsystem.org/virtsec.pdf)*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: — Tavis Ormandy *[http://taviso.decsystem.org/virtsec.pdf](http://taviso.decsystem.org/virtsec.pdf)*
- en: It's a ringing endorsement, by security-boffin standards. By and large, with
    Xen, we're not worried about keeping people from breaking out of their virtual
    machines—Xen itself is supposed to provide an appropriate level of isolation.
    In paravirtualized mode, Xen doesn't expose hardware drivers to domUs, which eliminates
    one major attack vector.^([[39](#ftn.CHP-7-FNOTE-1)]) For the most part, securing
    a dom0 is exactly like securing any other server, except in one area.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全专家的标准下，这是一个响亮的认可。总的来说，使用Xen，我们不必担心人们从虚拟机中逃逸——Xen本身应该提供适当的隔离级别。在半虚拟化模式下，Xen不会将硬件驱动程序暴露给domUs，这消除了一个主要的攻击向量.^([[39](#ftn.CHP-7-FNOTE-1)])
    在大多数情况下，保护dom0就像保护任何其他服务器一样，除了在一个领域。
- en: That area of possible concern is in the access controls for shared resources,
    which are not entirely foolproof. The primary worry is that malicious users could
    gain more resources than they're entitled to, or in extreme cases cause denial-of-service
    attacks by exploiting flaws in Xen's accounting. In other words, we are in the
    business of enforcing performance isolation, rather than specifically trying to
    protect the dom0 from attacks via the domUs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那个可能引起关注的领域是共享资源的访问控制，它们并不完全万无一失。主要担忧是恶意用户可能会获得比他们应得的更多资源，或者在极端情况下，通过利用Xen的会计漏洞来造成拒绝服务攻击。换句话说，我们是在执行性能隔离的业务，而不是特别试图通过domUs保护dom0免受攻击。
- en: Most of the resource controls that we present here are aimed at users who aren't
    necessarily malicious—just, perhaps, exuberant.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示的大多数资源控制都是针对那些不一定是有恶意——只是可能过于热情——的用户。
- en: Tuning CPU Usage
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整CPU使用率
- en: The first shared resource of interest is the CPU. While memory and disk size
    are easy to tune—you can just specify memory in the config file, while disk size
    is determined by the size of the backing device—fine-grained CPU allocation requires
    you to adjust the scheduler.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最感兴趣的第一个共享资源是CPU。虽然内存和磁盘大小很容易调整——你可以在配置文件中指定内存，而磁盘大小由后端设备的大小决定——细粒度CPU分配需要你调整调度器。
- en: Scheduler Basics
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度器基础
- en: 'The Xen scheduler acts as a referee between the running domains. In some ways
    it''s a lot like the Linux scheduler: It can preempt processes as needed, it tries
    its best to ensure fair allocation, and it ensures that the CPU wastes as few
    cycles as possible. As the name suggests, Xen''s scheduler schedules domains to
    run on the physical CPU. These domains, in turn, schedule and run processes from
    their internal run queues.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Xen调度器在运行域之间充当裁判。在某种程度上，它很像Linux调度器：它可以在需要时抢占进程，它尽力确保公平分配，并确保CPU尽可能少地浪费周期。正如其名所示，Xen的调度器负责在物理CPU上调度域。这些域反过来，从它们内部的运行队列中调度和运行进程。
- en: Because the dom0 is just another domain as far as Xen's concerned, it's subject
    to the same scheduling algorithm as the domUs. This can lead to trouble if it's
    not assigned a high enough weight because the dom0 has to be able to respond to
    I/O requests. We'll go into more detail on that topic a bit later, after we describe
    the general procedures for adjusting domain weights.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于dom0在Xen看来只是另一个域，因此它受到与domUs相同的调度算法的影响。如果它没有被分配足够高的权重，这可能会导致问题，因为dom0必须能够响应I/O请求。我们将在描述调整域权重的通用程序之后，稍后更详细地讨论这个话题。
- en: Xen can use a variety of scheduling algorithms, ranging from the simple to the
    baroque. Although Xen has shipped with a number of schedulers in the past, we're
    going to concentrate on the *credit scheduler*; it's the current default and recommended
    choice and the only one that the Xen team has indicated any interest in keeping.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Xen可以使用各种调度算法，从简单的到复杂的。尽管Xen过去已经附带了许多调度器，但我们将专注于*信用调度器*；它是当前默认且推荐的选择，也是Xen团队表示有兴趣保持的唯一调度器。
- en: The `xm dmesg` command will tell you, among other things, what scheduler Xen
    is using.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`xm dmesg`命令会告诉您，包括但不限于Xen正在使用哪个调度器。'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you want to change the scheduler, you can set it as a boot parameter—to change
    to the SEDF scheduler, for example, append `sched=sedf` to the kernel line in
    GRUB. (That's the Xen kernel, not the dom0 Linux kernel loaded by the first `module`
    line.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想更改调度器，可以将它设置为引导参数——例如，要将调度器更改为SEDF调度器，请将`sched=sedf`附加到GRUB中的内核行。（这是Xen内核，而不是由第一个`module`行加载的dom0
    Linux内核。）
- en: VCPUs and Physical CPUs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟CPU和物理CPU
- en: 'For convenience, we consider each Xen domain to have one or more virtual CPUs
    (VCPUs), which periodically run on the physical CPUs. These are the entities that
    consume credits when run. To examine VCPUs, use `xm vcpu-list <domain>` :'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们认为每个Xen域都有一个或多个虚拟CPU（VCPUs），这些虚拟CPU定期在物理CPU上运行。这些是在运行时消耗信用点的实体。要检查虚拟CPU，请使用`xm
    vcpu-list <domain>`：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, the domain has two VCPUs, 0 and 1\. VCPU 1 is in the *running*
    state on (physical) CPU 1\. Note that Xen will try to spread VCPUs across CPUs
    as much as possible. Unless you've pinned them manually, VCPUs can occasionally
    switch CPUs, depending on which physical CPUs are available.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，该域有两个虚拟CPU（VCPUs），0和1。虚拟CPU 1在（物理）CPU 1上处于*运行*状态。请注意，Xen会尽可能地将虚拟CPU分散到各个CPU上。除非您手动固定，否则虚拟CPU有时会切换到其他物理CPU，具体取决于可用的物理CPU。
- en: To specify the number of VCPUs for a domain, specify the `vcpus=` directive
    in the config file. You can also change the number of VCPUs while a domain is
    running using `xm vcpu-set`. However, note that you can decrease the number of
    VCPUs this way, but you can't increase the number of VCPUs beyond the initial
    count.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要指定域的虚拟CPU数量，请在配置文件中指定`vcpus=`指令。您还可以在域运行时使用`xm vcpu-set`更改虚拟CPU的数量。但是，请注意，您可以通过这种方式减少虚拟CPU的数量，但不能增加超过初始计数的虚拟CPU数量。
- en: 'To set the CPU affinity, use `xm vcpu-pin <domain> <vcpu> <pcpu>`. For example,
    to switch the CPU assignment in the domain *horatio*, so that VCPU0 runs on CPU2
    and VCPU1 runs on CPU0:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置CPU亲和性，请使用`xm vcpu-pin <domain> <vcpu> <pcpu>`。例如，要切换域*horatio*中的CPU分配，使VCPU0在CPU2上运行，VCPU1在CPU0上运行：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Equivalently, you can pin VCPUs in the domain config file (*/etc/xen/horatio*,
    if you''re using our standard naming convention) like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以在域配置文件中固定虚拟CPU（*/etc/xen/horatio*，如果您使用我们标准的命名约定）如下：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This gives the domain two VCPUs, pins the first VCPU to the first physical CPU,
    and pins the second VCPU to the third physical CPU.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这为域提供了两个虚拟CPU，将第一个虚拟CPU固定在第一个物理CPU上，并将第二个虚拟CPU固定在第三个物理CPU上。
- en: Credit Scheduler
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信用调度器
- en: The Xen team designed the credit scheduler to minimize wasted CPU time. This
    makes it a *work-conserving* scheduler, in that it tries to ensure that the CPU
    will always be working whenever there is work for it to do.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Xen团队设计了信用调度器以最小化浪费的CPU时间。这使得它成为一个*节能*调度器，因为它试图确保CPU在有任何工作要做时始终在工作。
- en: As a consequence, if there is more real CPU available than the domUs are demanding,
    all domUs get all the CPU they want. When there is contention—that is, when the
    domUs in aggregate want more CPU than actually exists—then the scheduler arbitrates
    fairly between the domains that want CPU.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果有比domUs需求更多的真实CPU可用，所有domUs都将获得它们想要的全部CPU。当存在竞争时——也就是说，当domUs总体上需要的CPU比实际存在的CPU多时——调度器将在需要CPU的域之间公平仲裁。
- en: Xen does its best to do a fair division, but the scheduling isn't perfect by
    any stretch of the imagination. In particular, cycles spent servicing I/O by domain
    0 are not charged to the responsible domain, leading to situations where I/O-intensive
    clients get a disproportionate share of CPU usage. Nonetheless, you can get pretty
    good allocation in nonpathological cases. (Also, in our experience, the CPU sits
    idle most of the time anyway.)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Xen尽力做到公平分配，但调度并不完美。特别是，由域0服务I/O所花费的周期不计入责任域，导致I/O密集型客户端获得不成比例的CPU使用量。尽管如此，在非病态情况下，您可以得到相当好的分配。（此外，根据我们的经验，CPU大部分时间都是空闲的。）
- en: The credit scheduler assigns each domain a *weight* and, optionally, a *cap*.
    The weight indicates the relative CPU allocation of a domain—if the CPU is scarce,
    a domain with a weight of 512 will receive twice as much CPU time as a domain
    with a weight of 256 (the default). The cap sets an absolute limit on the amount
    of CPU time a domain can use, expressed in hundredths of a CPU. Note that the
    CPU cap can exceed 100 on multiprocessor hosts.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 信用额调度器为每个域分配一个*权重*和可选的*上限*。权重表示域的相对CPU分配——如果CPU稀缺，权重为512的域将比权重为256（默认值）的域获得两倍的CPU时间。上限设置了一个域可以使用的CPU时间的绝对限制，以CPU的百分之一表示。请注意，在多处理器主机上，CPU上限可以超过100。
- en: 'The scheduler transforms the weight into a *credit* allocation for each VCPU,
    using a separate accounting thread. As a VCPU runs, it consumes credits. If a
    VCPU runs out of credits, it only runs when other, more thrifty VCPUs have finished
    executing, as shown in [Figure 7-1](ch07s02.html#vcpus_wait_in_two_queues_one_for_vcpus_w
    "Figure 7-1. VCPUs wait in two queues: one for VCPUs with credits and the other
    for those that are over their allotment. Once the first queue is exhausted, the
    CPU will pull from the second."). Periodically, the accounting thread goes through
    and gives everybody more credits.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器将权重转换为每个VCPU的*信用额*分配，使用一个单独的会计线程。随着VCPU的运行，它会消耗信用额。如果一个VCPU耗尽了信用额，它只有在其他更节俭的VCPU执行完毕后才能运行，如图[图7-1](ch07s02.html#vcpus_wait_in_two_queues_one_for_vcpus_w
    "图7-1. VCPUs等待在两个队列中：一个用于有信用额的VCPUs，另一个用于超出分配额的VCPUs。一旦第一个队列耗尽，CPU将从第二个队列中获取。")所示。定期地，会计线程会遍历并给每个人更多的信用额。
- en: '![VCPUs wait in two queues: one for VCPUs with credits and the other for those
    that are over their allotment. Once the first queue is exhausted, the CPU will
    pull from the second.](httpatomoreillycomsourcenostarchimages333223.png.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![VCPUs等待在两个队列中：一个用于有信用额的VCPUs，另一个用于超出分配额的VCPUs。一旦第一个队列耗尽，CPU将从第二个队列中获取。](httpatomoreillycomsourcenostarchimages333223.png.jpg)'
- en: 'Figure 7-1. VCPUs wait in two queues: one for VCPUs with credits and the other
    for those that are over their allotment. Once the first queue is exhausted, the
    CPU will pull from the second.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-1. VCPUs等待在两个队列中：一个用于有信用额的VCPUs，另一个用于超出分配额的VCPUs。一旦第一个队列耗尽，CPU将从第二个队列中获取。
- en: 'In this case, the details are probably less important than the practical application.
    Using the `xm sched-credit` commands, we can adjust CPU allocation on a per-domain
    basis. For example, here we''ll increase a domain''s CPU allocation. First, to
    list the weight and cap for the domain horatio:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，细节可能不如实际应用重要。使用`xm sched-credit`命令，我们可以根据每个域调整CPU分配。例如，这里我们将增加域的CPU分配。首先，列出域horatio的权重和上限：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, to modify the scheduler''s parameters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了修改调度器的参数：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Of course, the value "512" only has meaning relative to the other domains that
    are running on the machine. Make sure to set all the domains' weights appropriately.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，"512"这个值只有相对于机器上运行的其它域才有意义。确保适当地设置所有域的权重。
- en: 'To set the cap for a domain:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要为域设置上限：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Scheduling for Providers
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提供商的调度
- en: We decided to divide the CPU along the same lines as the available RAM—it stands
    to reason that a user paying for half the RAM in a box will want more CPU than
    someone with a 64MB domain. Thus, in our setup, a customer with 25 percent of
    the RAM also has a minimum share of 25 percent of the CPU cycles.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们决定按照可用的RAM来划分CPU——从逻辑上讲，支付盒子一半RAM的用户会比拥有64MB域的用户需要更多的CPU。因此，在我们的设置中，拥有25%RAM的客户也至少拥有25%的CPU周期。
- en: The simple way to do this is to assign each CPU a weight equal to the number
    of megabytes of memory it has and leave the cap empty. The scheduler will then
    handle converting that into fair proportions. For example, our aforementioned
    user with half the RAM will get about as much CPU time as the rest of the users
    put together.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事的简单方法是为每个CPU分配一个等于其内存兆字节的权重，并留空上限。然后，调度器将处理将其转换为公平的比例。例如，我们之前提到的拥有半数RAM的用户将获得与其他用户加在一起相当的CPU时间。
- en: Of course, that's the worst case; that is what the user will get in an environment
    of constant struggle for the CPU. Idle domains will automatically yield the CPU.
    If all domains but one are idle, that one can have the entire CPU to itself.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是最坏的情况；这就是用户在CPU不断争夺的环境中会得到的结果。空闲域会自动释放CPU。如果除了一个域之外的所有域都空闲，那么这个域可以独占整个CPU。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*It''s essential to make sure that the dom0 has sufficient CPU to service I/O
    requests. You can handle this by dedicating a CPU to the dom0 or by giving the
    dom0 a very high weight—high enough to ensure that it never runs out of credits.
    At prgmr.com, we handle the problem by weighting each domU with its RAM amount
    and weighting the dom0 at 6000*.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*确保dom0有足够的CPU来处理I/O请求是至关重要的。你可以通过为dom0分配一个CPU或者给dom0一个非常高的权重来实现这一点——足够高以确保它永远不会耗尽信用额度。在prgmr.com，我们通过为每个domU根据其RAM量分配权重，并将dom0的权重设置为6000来解决这一问题*。'
- en: This simple weight = memory formula becomes a bit more complex when dealing
    with multiprocessor systems because independent systems of CPU allocation come
    into play. A good rule would be to allocate VCPUs in proportion to memory (and
    therefore in proportion to weight). For example, a domain with half the RAM on
    a box with four cores (and hyperthreading turned off) should have at least two
    VCPUs. Another solution would be to give all domains as many VCPUs as physical
    processors in the box—this would allow all domains to burst to the full CPU capacity
    of the physical machine but might lead to increased overhead from context swaps.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理多处理器系统时，这个简单的权重=内存公式会变得稍微复杂一些，因为独立的CPU分配系统开始发挥作用。一个好的规则是按内存比例（因此按权重比例）分配VCPUs。例如，在一个具有四个核心（且关闭超线程）的机器上，拥有半数RAM的域至少应该有至少两个VCPUs。另一个解决方案是给所有域分配与机器中物理处理器数量相等的VCPUs——这将允许所有域达到物理机器的完整CPU容量，但可能会导致上下文交换带来的开销增加。
- en: Controlling Network Resources
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制网络资源
- en: Network resource controls are, frankly, essential to any kind of shared hosting
    operation. Among the many lessons that we've learned from Xen hosting has been
    that if you provide free bandwidth, some users will exploit it for all it's worth.
    This isn't a Xen-specific observation, but it's especially noticeable with the
    sort of cheap VPS hosting Xen lends itself to.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 网络资源控制对于任何类型的共享托管操作来说都是至关重要的。我们从Xen托管中学到的许多经验之一是，如果你提供免费带宽，一些用户会充分利用它。这不是Xen特有的观察结果，但在Xen易于提供的廉价VPS托管中尤为明显。
- en: We prefer to use `network-bridge`, since that's the default. For a more thorough
    look at `network-bridge`, take a look at [Chapter 5](ch05.html "Chapter 5. NETWORKING").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢使用`network-bridge`，因为这是默认设置。要更深入地了解`network-bridge`，请参阅[第五章](ch05.html
    "第五章. 网络连接")。
- en: Monitoring Network Usage
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控网络使用
- en: Given that some users will consume as much bandwidth as possible, it's vital
    to have some way to monitor network traffic.^([[40](#ftn.CHP-7-FNOTE-2)])
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些用户可能会尽可能多地消耗带宽，因此拥有一种监控网络流量的方式至关重要.^([[40](#ftn.CHP-7-FNOTE-2)])
- en: To monitor network usage, we use BandwidthD on a physical SPAN port. It's a
    simple tool that counts bytes going through a switch—nothing Xen-specific here.
    We feel comfortable doing this because our provider doesn't allow anything but
    IP packets in or out, and our antispoof rules are good enough to protect us from
    users spoofing their IP on outgoing packets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控网络使用情况，我们在物理SPAN端口上使用BandwidthD。这是一个简单的工具，用于计算通过交换机的字节数——这里没有Xen特有的内容。我们对此感到放心，因为我们的提供商只允许进出IP数据包，并且我们的反欺骗规则足够强大，足以保护我们免受用户在出站数据包上欺骗IP的侵害。
- en: A similar approach would be to extend the *dom0 is a switch* analogy and use
    SNMP monitoring software. As mentioned in [Chapter 5](ch05.html "Chapter 5. NETWORKING"),
    it's important to specify a `vifname` for each domain if you're doing this. In
    any case, we'll leave the particulars of bandwidth monitoring up to you.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似的方法是扩展*dom0是一个交换机*的类比，并使用SNMP监控软件。如[第五章](ch05.html "第五章. 网络连接")中所述，如果你这样做，为每个域指定一个`vifname`是很重要的。无论如何，我们将带宽监控的细节留给你。
- en: ARP CACHE POISONING
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ARP CACHE POISONING
- en: If you use the default `network-bridge setup`, you are vulnerable to ARP cache
    poisoning, just as on any layer 2 switch.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用默认的`network-bridge setup`，你将像在任何二层交换机上一样容易受到ARP缓存中毒的攻击。
- en: The idea is that the interface counters on a layer 2 switch—such as the virtual
    switch used by `network-bridge`—watch traffic as it passes through a particular
    port. Every time a switch sees an Ethernet frame or ARP is-at, it keeps track
    of what port and MAC it came from. If it gets a frame destined for a MAC address
    in its cache, it sends that frame down the proper port (and only the proper port).
    If the bridge sees a frame destined for a MAC that is not in the cache, it sends
    that frame to all ports.^([[41](#ftn.CHP-7-FNOTE-3)])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是，第二层交换机（如`network-bridge`使用的虚拟交换机）上的接口计数器在流量通过特定端口时监控流量。每当交换机看到一个以太网帧或ARP
    is-at时，它会记录它来自哪个端口和MAC地址。如果它收到一个目的地为缓存中MAC地址的帧，它会将那个帧发送到正确的端口（并且只有正确的端口）。如果桥接器看到一个目的地为不在缓存中的MAC地址的帧，它会将那个帧发送到所有端口。[41](#ftn.CHP-7-FNOTE-3)]
- en: Clever, no? In most cases this means that you almost never see Ethernet frames
    destined for other MAC addresses (other than broadcasts, etc.). However, this
    feature is designed purely as an optimization, not a security measure. As those
    of you with cable providers who do MAC address verification know quite well, it
    is fairly trivial to fake a MAC address. This means that a malicious user can
    fill the (limited in size) ARP cache with bogus MAC addresses, drive out the good
    data, and force all packets to go down all interfaces. At this point the switch
    becomes basically a hub, and the counters on all ports will show all traffic for
    any port.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明吗？在大多数情况下，这意味着你几乎从未看到目的地为其他MAC地址的以太网帧（除了广播等）。然而，这个特性纯粹是为了优化，而不是安全措施。正如那些有电缆提供商并且对MAC地址验证非常了解的人所知，伪造MAC地址相当简单。这意味着恶意用户可以用虚假的MAC地址填满（大小有限的）ARP缓存，驱逐好的数据，并迫使所有数据包通过所有接口。此时，交换机基本上变成了一个集线器，所有端口的计数器都会显示任何端口的流量。
- en: There are two ways we have worked around the problem. You could use Xen's `network-route`
    networking model, which doesn't use a virtual bridge. The other approach is to
    ignore the interface counters and use something like BandwidthD, which bases its
    accounting on IP packets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了两种绕过这个问题的方法。你可以使用Xen的`network-route`网络模型，它不使用虚拟桥接。另一种方法是忽略接口计数器，并使用类似BandwidthD的东西，它基于IP数据包进行计费。
- en: Once you can examine traffic quickly, the next step is to shape the users. The
    principles for network traffic shaping and policing are the same as for standalone
    boxes, except that you can also implement policies on the Xen host. Let's look
    at how to limit both incoming and outgoing traffic for a particular interface—as
    if, say, you have a customer who's going over his bandwidth allotment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你能够快速检查流量，下一步就是塑造用户。网络流量整形和监控的原则与独立盒子相同，只是你还可以在Xen主机上实施策略。让我们看看如何限制特定接口的入站和出站流量——比如说，你有一个客户超出了他的带宽配额。
- en: Network Shaping Principles
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络整形原则
- en: The first thing to know about shaping is that it only works on outgoing traffic.
    Although it is possible to *police* incoming traffic, it isn't as effective. Fortunately,
    both directions look like outgoing traffic at some point in their passage through
    the dom0, as shown in [Figure 7-2](ch07s02.html#incoming_traffic_comes_from_the_internet
    "Figure 7-2. Incoming traffic comes from the Internet, goes through the virtual
    bridge, and gets shaped by a simple nonhierarchical filter. Outgoing traffic,
    on the other hand, needs to go through a system of filters that assign packets
    to classes in a hierarchical queuing discipline."). (When we refer to outgoing
    and incoming traffic in the following description, we mean from the perspective
    of the domU.)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于整形，首先要知道的是，它只适用于出站流量。尽管可以对入站流量进行监控，但效果并不理想。幸运的是，在它们通过dom0的过程中，两个方向的流量在某个时刻看起来都像是出站流量，如图[图7-2](ch07s02.html#incoming_traffic_comes_from_the_internet
    "图7-2. 入站流量来自互联网，通过虚拟桥接，并由简单的非层次化过滤器进行整形。另一方面，出站流量需要通过一系列过滤器，这些过滤器将数据包分配到层次队列纪律中的类别。")所示。（在以下描述中，当我们提到出站和入站流量时，我们是指从domU的角度来看。）
- en: '![Incoming traffic comes from the Internet, goes through the virtual bridge,
    and gets shaped by a simple nonhierarchical filter. Outgoing traffic, on the other
    hand, needs to go through a system of filters that assign packets to classes in
    a hierarchical queuing discipline.](httpatomoreillycomsourcenostarchimages333225.png.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![入站流量来自互联网，通过虚拟桥接，并由简单的非层次化过滤器进行整形。另一方面，出站流量需要通过一系列过滤器，这些过滤器将数据包分配到层次队列纪律中的类别。](httpatomoreillycomsourcenostarchimages333225.png.jpg)'
- en: Figure 7-2. Incoming traffic comes from the Internet, goes through the virtual
    bridge, and gets shaped by a simple nonhierarchical filter. Outgoing traffic,
    on the other hand, needs to go through a system of filters that assign packets
    to classes in a hierarchical queuing discipline.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7-2. 入站流量来自互联网，通过虚拟桥接，并由一个简单的非层次化过滤器进行整形。另一方面，出站流量需要通过一个将数据包分配到层次队列纪律类别的过滤系统。
- en: Shaping Incoming Traffic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整入站流量
- en: We'll start with incoming traffic because it's much simpler to limit than outgoing
    traffic. The easiest way to shape incoming traffic is probably the *token bucket
    filter* queuing discipline, which is a simple, effective, and lightweight way
    to slow down an interface.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从入站流量开始，因为它比出站流量更容易限制。调整入站流量的最简单方法可能是**令牌桶过滤器**队列纪律，这是一种简单、有效且轻量级的方法来降低接口速度。
- en: 'The token bucket filter, or TBF, takes its name from the metaphor of a bucket
    of tokens. Tokens stream into the bucket at a defined and constant rate. Each
    byte of data sent takes one token from the bucket and goes out immediately—when
    the bucket''s empty, data can only go as tokens come in. The bucket itself has
    a limited capacity, which guarantees that only a reasonable amount of data will
    be sent out at once. To use the TBF, we add a `qdisc` (*queuing discipline*) to
    perform the actual work of traffic limiting. To limit the virtual interface `osric`
    to 1 megabit per second, with bursts up to 2 megabits and maximum allowable latency
    of 50 milliseconds:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌桶过滤器，或 TBF，其名称来源于令牌桶的隐喻。令牌以定义和恒定的速率流入桶中。发送的每个数据字节从桶中取走一个令牌并立即出去——当桶为空时，数据只能以令牌流入的速度传输。桶本身具有有限的容量，这保证了只会一次性发送合理数量的数据。要使用
    TBF，我们需要添加一个 `qdisc`（队列纪律）来执行实际的流量限制工作。要将虚拟接口 `osric` 限制为每秒 1 兆比特，突发速率最高为 2 兆比特，最大允许延迟为
    50 毫秒：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This adds a `qdisc` to the device `osric`. The next arguments specify where
    to add it (`root`) and what sort of `qdisc` it is (`tbf`). Finally, we specify
    the rate, latency, burst rate, and amount that can go at burst rate. These parameters
    correspond to the token flow, amount of latency the packets are allowed to have
    (before the driver signals the operating system that its buffers are full), maximum
    rate at which the bucket can empty, and the size of the bucket.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在设备 `osric` 上添加一个 `qdisc`。下一个参数指定了添加的位置（`root`）以及它是什么类型的 `qdisc`（`tbf`）。最后，我们指定了速率、延迟、突发速率和可以以突发速率传输的量。这些参数对应于令牌流量、数据包允许的延迟量（在驱动程序向操作系统发出其缓冲区已满的信号之前），桶可以清空的最大速率以及桶的大小。
- en: Shaping Outgoing Traffic
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调整出站流量
- en: Having shaped incoming traffic, we can focus on limiting outgoing traffic. This
    is a bit more complex because the outgoing traffic for all domains goes through
    a single interface, so a single token bucket won't work. The policing filters
    might work, but they handle the problem by dropping packets, which is … bad. Instead,
    we're going to apply traffic shaping to the outgoing physical Ethernet device,
    peth0, with a *Hierarchical Token Bucket*, or HTB `qdisc`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整入站流量后，我们可以专注于限制出站流量。这稍微复杂一些，因为所有域的出站流量都通过单个接口，所以单个令牌桶不起作用。警察过滤器可能有效，但它们通过丢弃数据包来处理问题，这是……不好的。相反，我们将对出站物理以太网设备
    peth0 应用流量整形，使用**层次令牌桶**，或 HTB `qdisc`。
- en: The HTB discipline acts like the simple token bucket, but with a hierarchy of
    buckets, each with its own rate, and a system of filters to assign packets to
    buckets. Here's how to set it up.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: HTB 纪律类似于简单的令牌桶，但具有桶的层次结构，每个桶都有自己的速率，以及一个将数据包分配到桶的过滤系统。以下是设置方法。
- en: 'First, we have to make sure that the packets on Xen''s virtual bridge traverse
    `iptables`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须确保 Xen 虚拟桥上的数据包通过 `iptables`：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is so that we can mark packets according to which domU emitted them. There
    are other reasons, but that''s the important one in terms of our traffic-shaping
    setup. Next, for each domU, we add a rule to mark packets from the corresponding
    network interface:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就可以根据哪个 domU 发射的数据包来标记数据包。还有其他原因，但就我们的流量整形设置而言，这是重要的一个。接下来，对于每个 domU，我们添加一个规则来标记来自相应网络接口的数据包：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here the number 5 is an arbitrary mark—it's not important what the number is,
    as long as there's a useful mapping between number and domain. We're using the
    domain ID. We could also use `tc` filters directly that match on source IP address,
    but it feels more elegant to have everything keyed to the domain's physical network
    device. Note that we're using `physdev-in`—traffic that goes out from the domU
    comes in to the dom0, as [Figure 7-3](ch07s02.html#we_shape_traffic_coming_into_the_domu_as
    "Figure 7-3. We shape traffic coming into the domU as it comes into the dom0 from
    the physical device, and shape traffic leaving the domU as it enters the dom0
    on the virtual device.") shows.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里数字5是一个任意的标记——数字本身并不重要，只要数字与域之间有一个有用的映射即可。我们使用域ID。我们也可以直接使用匹配源IP地址的`tc`过滤器，但将所有内容与域的物理网络设备相关联感觉更优雅。请注意，我们使用`physdev-in`——从domU出去的流量进入dom0，正如[图7-3](ch07s02.html#we_shape_traffic_coming_into_the_domu_as
    "图7-3. 我们将物理设备进入dom0的流量形状为domU的流量，并将domU离开的流量形状为在虚拟设备上进入dom0的流量")所示。
- en: '![We shape traffic coming into the domU as it comes into the dom0 from the
    physical device, and shape traffic leaving the domU as it enters the dom0 on the
    virtual device.](httpatomoreillycomsourcenostarchimages333227.png.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![我们根据从物理设备进入dom0的流量形状domU的流量，并在虚拟设备上进入dom0时形状domU离开的流量](httpatomoreillycomsourcenostarchimages333227.png.jpg)'
- en: Figure 7-3. We shape traffic coming into the domU as it comes into the dom0
    from the physical device, and shape traffic leaving the domU as it enters the
    dom0 on the virtual device.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-3. 我们将物理设备进入dom0的流量形状为domU的流量，并将domU离开的流量形状为在虚拟设备上进入dom0的流量。
- en: 'Next we create a HTB `qdisc`. We won''t go over the HTB options in too much
    detail—see the documentation at [http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm](http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm)
    for more details:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们创建一个HTB `qdisc`。我们不会过多地介绍HTB选项——有关更多详细信息，请参阅[http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm](http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm)上的文档：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then we make some classes to put traffic into. Each class will get traffic from
    one domU. (As the HTB docs explain, we're also making a parent class so that they
    can share surplus bandwidth.)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一些类别来放置流量。每个类别将从一个domU接收流量。（如HTB文档所述，我们还在创建一个父类别，以便它们可以共享多余的带宽。）
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have a class for our domU's traffic, we need a filter that will
    assign packets to it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为domU的流量定义了一个类别，我们需要一个过滤器来将数据包分配给它。
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that we're matching on the "handle" that we set earlier using `iptables`.
    This assigns the packet to the 1:2 class, which we've previously limited to 1
    megabit per second.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在匹配之前使用`iptables`设置的“处理”方式。这会将数据包分配到1:2类别，我们之前将其限制为每秒1兆比特。
- en: At this point traffic to and from the target domU is essentially shaped, as
    demonstrated by [Figure 7-4](ch07s02.html#the_effect_of_the_shaping_filters "Figure 7-4. The
    effect of the shaping filters"). You can easily add commands like these to the
    end of your `vif` script, be it `vif-bridge`, `vif-route`, or a wrapper. We would
    also like to emphasize that this is only an example and that the Linux Advanced
    Routing and Traffic Control how-to at [http://lartc.org/](http://lartc.org/) is
    an excellent place to look for further documentation. The `tc` man page is also
    informative.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，目标domU的进出流量基本上已经被形状，如[图7-4](ch07s02.html#the_effect_of_the_shaping_filters
    "图7-4. 形状过滤器的影响")所示。你可以轻松地将这些命令添加到你的`vif`脚本末尾，无论是`vif-bridge`、`vif-route`还是包装器。我们还想强调，这只是一个示例，而且[http://lartc.org/](http://lartc.org/)上的Linux高级路由和流量控制指南是一个寻找进一步文档的优秀地方。`tc`手册页也是信息丰富的。
- en: '![The effect of the shaping filters](httpatomoreillycomsourcenostarchimages333229.png.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![形状过滤器的影响](httpatomoreillycomsourcenostarchimages333229.png.jpg)'
- en: Figure 7-4. The effect of the shaping filters
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图7-4. 形状过滤器的影响
- en: '* * *'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[39](#CHP-7-FNOTE-1)]) In HVM mode, the emulated QEMU devices are something
    of a risk, which is part of why we don't offer HVM domains.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[39](#CHP-7-FNOTE-1)]) 在HVM模式下，模拟的QEMU设备有一定的风险，这也是我们不提供HVM域的部分原因。
- en: ^([[40](#CHP-7-FNOTE-2)]) In this case, we're talking about bandwidth monitoring.
    You should also run some sort of IDS, such as Snort, to watch for outgoing abuse
    (we do) but there's nothing Xen-specific about that.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[40](#CHP-7-FNOTE-2)]) 在这种情况下，我们谈论的是带宽监控。你也应该运行某种类型的入侵检测系统（IDS），例如Snort，以监视外出滥用（我们确实这样做），但这与Xen没有特定关系。
- en: ^([[41](#CHP-7-FNOTE-3)]) We are using the words *port* and *interface* interchangeably
    here. This is a reasonable simplification in the context of interface counters
    on an SNMP-capable switch.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[41](#CHP-7-FNOTE-3)]) 在这里，我们使用“端口”和“接口”这两个词可以互换。在具有SNMP功能的交换机上的接口计数器上下文中，这是一种合理的简化。
- en: Storage in a Shared Hosting Environment
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享托管环境中的存储
- en: As with so much else in system administration, a bit of planning can save a
    lot of trouble. Figure out beforehand where you're going to store pristine filesystem
    images, where configuration files go, and where customer data will live.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就像系统管理中的许多其他事情一样，一点规划可以避免很多麻烦。事先想清楚你将存储原始文件系统镜像的位置，配置文件将放在哪里，客户数据将存放在哪里。
- en: For pristine images, there are a lot of conventions—some people use */diskimages*,
    some use */opt/xen, /var/xen* or similar, some use a subdirectory of */home*.
    Pick one and stick with it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于原始镜像，有很多约定——有些人使用`/diskimages`，有些人使用`/opt/xen, /var/xen`或类似路径，有些人使用`/home`的子目录。选择一个并坚持下去。
- en: Configuration files should, without exception, go in */etc/xen*. If you don't
    give *xm create* a full path, it'll look for the file in */etc/xen*. Don't disappoint
    it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件应该无一例外地放在`/etc/xen`中。如果你没有为`xm create`提供一个完整路径，它将在`/etc/xen`中查找文件。不要让它失望。
- en: As for customer data, we recommend that serious hosting providers use LVM. This
    allows greater flexibility and manageability than blktap-mapped files while maintaining
    good performance. [Chapter 4](ch04.html "Chapter 4. STORAGE WITH XEN") covers
    the details of working with LVM (or at least enough to get started), as well as
    many other available storage options and their advantages. Here we're confining
    ourselves to lessons that we've learned from our adventures in shared hosting.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 至于客户数据，我们建议严肃的托管提供商使用LVM。这比blktap-mapped文件提供了更大的灵活性和可管理性，同时保持了良好的性能。[第4章](ch04.html
    "第4章。使用XEN的存储")涵盖了与LVM（或至少足够开始）一起工作的细节，以及许多其他可用的存储选项及其优点。在这里，我们只关注我们从共享托管冒险中学到的经验教训。
- en: Regulating Disk Access with ionice
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`ionice`调节磁盘访问
- en: One common problem with VPS hosting is that customers—or your own housekeeping
    processes, like backups—will use enough I/O bandwidth to slow down everyone on
    the machine. Furthermore, I/O isn't really affected by the scheduler tweaks discussed
    earlier. A domain can request data, hand off the CPU, and save its credits until
    it's notified of the data's arrival.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: VPS托管的一个常见问题是客户——或者你自己的维护过程，如备份——会使用足够的I/O带宽来减慢机器上所有人的速度。此外，I/O实际上并不受前面讨论的调度器调整的影响。一个域可以请求数据，交出CPU，并保存其信用额度，直到它被通知数据到达。
- en: 'Although you can''t set hard limits on disk access rates as you can with the
    network QoS, you can use the `ionice` command to prioritize the different domains
    into subclasses, with a syntax like:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你不能像网络QoS那样设置磁盘访问速率的硬限制，但你可以使用`ionice`命令将不同的域优先级划分为子类，语法如下：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here `-n` is the knob you'll ordinarily want to twiddle. It can range from 0
    to 7, with lower numbers taking precedence.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`-n`是你通常会调整的旋钮。它可以从0到7不等，数字越小优先级越高。
- en: '*We recommend always specifying 2 for the class*. Other classes exist—3 is
    idle and 1 is realtime—but idle is extremely conservative, while realtime is so
    aggressive as to have a good chance of locking up the system. The within-class
    priority is aimed at proportional allocation, and is thus much more likely to
    be what you want.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们建议始终指定类为2*。其他类也存在——3是空闲，1是实时——但空闲非常保守，而实时则非常激进，有可能锁定系统。类内优先级旨在实现比例分配，因此更有可能是你想要的。'
- en: Let's look at `ionice` in action. Here we'll test `ionice` with two different
    domains, one with the highest normal priority, the other with the lowest.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`ionice`的实际应用。在这里，我们将使用两个不同的域来测试`ionice`，一个具有最高的正常优先级，另一个具有最低的优先级。
- en: 'First, `ionice` only works with the CFQ I/O scheduler. To check that you''re
    using the CFQ scheduler, run this command in the dom0:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`ionice`只与CFQ I/O调度器一起工作。为了确认你正在使用CFQ调度器，请在dom0中运行以下命令：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The word in brackets is the selected scheduler. If it's not `[cfq]`, reboot
    with the parameter `elevator =cfq`.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的词是选定的调度器。如果不是`[cfq]`，请使用参数`elevator =cfq`重新启动。
- en: Next we find the processes we want to `ionice`. Because we're using `tap:aio`
    devices in this example, the dom0 process is `tapdisk`. If we were using `phy:`
    devices, it'd be `[xvd <domain id> <device specifier>]`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们找到我们想要`ionice`的过程。由于在这个例子中我们使用`tap:aio`设备，dom0进程是`tapdisk`。如果我们使用`phy:`设备，它将是`[xvd
    <domain id> <device specifier>]`。
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we can `ionice` our domains. Note that the numbers of the `tapctrl` devices
    correspond to the order the domains were started in, not the domain ID.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对域进行`ionice`处理。请注意，`tapctrl`设备的数量对应于域启动的顺序，而不是域ID。
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To test `ionice`, let's run a couple of Bonnie++ processes and time them. (After
    Bonnie++ finishes, we `dd` a load file, just to make sure that conditions for
    the other domain remain unchanged.)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试`ionice`，让我们运行几个Bonnie++进程并计时。（在Bonnie++完成后，我们`dd`一个负载文件，只是为了确保其他域的条件保持不变。）
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the end, according to the wall clock, the domU with priority 0 took 3:32.33
    to finish, while the priority 7 domU needed 5:07.98\. As you can see, the `ionice`
    priorities provide an effective way to do proportional I/O allocation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，根据墙上的时钟，优先级为0的domU花费了3:32.33来完成，而优先级为7的domU需要5:07.98。如您所见，`ionice`优先级提供了一个有效的方法来进行比例I/O分配。
- en: The best way to apply `ionice` is probably to look at CPU allocations and convert
    them into priority classes. Domains with the highest CPU allocation get priority
    1, next highest priority 2, and so on. Processes in the dom0 should be ioniced
    as appropriate. This will ensure a reasonable priority, but not allow big domUs
    to take over the entirety of the I/O bandwidth.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 应用`ionice`的最佳方式可能是查看CPU分配并将它们转换为优先级类别。CPU分配最高的域获得优先级1，其次是优先级2，依此类推。dom0中的进程应根据适当的方式进行ionice处理。这将确保合理的优先级，但不会允许大domU占用全部I/O带宽。
- en: Backing Up DomUs
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 备份DomUs
- en: As a service provider, one rapidly learns that customers don't do their own
    backups. When a disk fails (not *if—when*), customers will expect you to have
    complete backups of their data, and they'll be very sad if you don't. So let's
    talk about backups.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为服务提供商，人们很快就会了解到客户不会自己进行备份。当磁盘失败（不是*如果——而是当*），客户会期望您有他们数据的完整备份，如果您没有，他们会非常难过。所以让我们来谈谈备份。
- en: 'Of course, you already have a good idea how to back up physical machines. There
    are two aspects to backing up Xen domains: First, there''s the domain''s virtual
    disk, which we want to back up just as we would a real machine''s disk. Second,
    there''s the domain''s running state, which can be saved and restored from the
    dom0\. Ordinarily, our use of *backup* refers purely to the disk, as it would
    with physical machines, but with the advantage that we can use domain snapshots
    to pause the domain long enough to get a clean disk image.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您已经对如何备份物理机器有了很好的了解。备份Xen域有两个方面：首先，是域的虚拟磁盘，我们希望像备份真实机器的磁盘一样备份它。其次，是域的运行状态，可以从dom0中保存和恢复。通常，我们所说的*备份*纯粹是指磁盘，就像物理机器一样，但有一个优势，即我们可以使用域快照来暂停域足够长的时间，以获取一个干净的磁盘镜像。
- en: We use `xm save` and LVM snapshots to back up both the domain's storage and
    running state. LVM snapshots aren't a good way of implementing full copy-on-write
    because they handle the "out of snapshot space" case poorly, but they're excellent
    if you want to preserve a filesystem state long enough to make a consistent backup.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`xm save`和LVM快照来备份域的存储和运行状态。LVM快照不是实现全写时复制的良好方式，因为它们处理“快照空间不足”的情况不佳，但如果您想保留足够长的文件系统状态以进行一致的备份，它们是非常好的。
- en: Our implementation copies the entire disk image using either a plain `cp` (in
    the case of file-backed domUs) or `dd` (for `phy:` devices). This is because we
    very much want to avoid mounting a possibly unclean filesystem in the dom0, which
    can cause the entire machine to panic. Besides, if we do a raw device backup,
    domU administrators will be able to use filesystems (such as ZFS on an OpenSolaris
    domU) that the dom0 cannot read.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现使用普通的`cp`（在基于文件的domU的情况下）或`dd`（对于`phy:`设备）来复制整个磁盘镜像。这是因为我们非常希望避免在dom0中挂载可能不干净的文件系统，这可能导致整个机器恐慌。此外，如果我们进行原始设备备份，domU管理员将能够使用dom0无法读取的文件系统（例如OpenSolaris
    domU上的ZFS）。
- en: 'An appropriate script to do as we''ve described might be:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个适合我们描述的适当脚本可能是：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Save it as, say, */usr/sbin/backup_domains.sh* and tell `cron` to execute the
    script at appropriate intervals.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将其保存为，比如说，`/usr/sbin/backup_domains.sh`，并告诉`cron`在适当的间隔执行脚本。
- en: This script works by saving each domain, copying file-based storage, and snapshotting
    LVs. When that's accomplished, it restores the domain, backs up the save file,
    and backs up the snapshots via `dd`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本通过保存每个域，复制基于文件的存储，并快照LVs来实现。当这些操作完成后，它将恢复域，备份保存文件，并通过`dd`备份快照。
- en: Note that users will see a brief hiccup in service while the domain is paused
    and snapshotted. We measured downtime of less than three minutes to get a consistent
    backup of a domain with a gigabyte of RAM—well within acceptable parameters for
    most applications. However, doing a bit-for-bit copy of an entire disk may also
    degrade performance somewhat.^([[42](#ftn.CHP-7-FNOTE-4)]) We suggest doing backups
    at off-peak hours.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当域暂停并快照时，用户将看到服务出现短暂的故障。我们测量了不到三分钟的停机时间，以获取一个具有千兆内存的域的一致备份——这对于大多数应用程序来说都在可接受的参数范围内。然而，对整个磁盘进行位对位复制可能会在一定程度上降低性能。[^([42](#ftn.CHP-7-FNOTE-4))]
    我们建议在非高峰时段进行备份。
- en: To view other scripts in use at prgmr.com, go to [http://book.xen.prgmr.com/](http://book.xen.prgmr.com/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看 prgmr.com 上使用的其他脚本，请访问 [http://book.xen.prgmr.com/](http://book.xen.prgmr.com/)。
- en: '* * *'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[42](#CHP-7-FNOTE-4)]) Humorous understatement.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ^([42](#CHP-7-FNOTE-4))) 幽默的夸张。
- en: Remote Access to the DomU
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 DomU 的远程访问
- en: 'The story on normal access for VPS users is deceptively simple: The Xen VM
    is exactly like a normal machine at the colocation facility. They can SSH into
    it (or, if you''re providing Windows, `rdesktop`). However, when problems come
    up, the user is going to need some way of accessing the machine at a lower level,
    as if they were sitting at their VPS''s console.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 VPS 用户来说，正常访问的故事表面上很简单：Xen 虚拟机与托管设施中的普通机器完全一样。他们可以 SSH 进入它（或者如果你提供 Windows，可以使用
    `rdesktop`）。然而，当出现问题时，用户将需要一种方式来以更低级别访问该机器，就像他们坐在 VPS 的控制台前一样。
- en: For that, we provide a console server that they can SSH into. The easiest thing
    to do is to use the dom0 as their console server and sharply limit their accounts.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们提供了一个他们可以 SSH 进入的控制台服务器。最简单的方法是将 dom0 作为他们的控制台服务器，并严格限制他们的账户。
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Analogously, we feel that any colocated machine should have a serial console
    attached to it*.^([[43](#ftn.CHP-7-FNOTE-5)]) *We discuss our reasoning and the
    specifics of using Xen with a serial console in [Chapter 14](ch14.html "Chapter 14. TIPS")*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*类似地，我们认为任何托管机器都应该连接一个串行控制台。[^([43](#ftn.CHP-7-FNOTE-5))] 我们在 [第 14 章](ch14.html
    "第 14 章。技巧") 中讨论了我们的理由和使用 Xen 与串行控制台的具体细节*。'
- en: An Emulated Serial Console
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟串行控制台
- en: Xen already provides basic serial console functionality via `xm`. You can access
    a guest's console by typing `xm console <domain>` within the dom0\. Issue commands,
    then type ctrl-] to exit from the serial console when you're done.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Xen 通过 `xm` 提供了基本的串行控制台功能。您可以在 dom0 内通过输入 `xm console <domain>` 访问一个虚拟机的控制台。输入命令后，完成操作时按
    ctrl-] 退出串行控制台。
- en: The problem with this approach is that `xm` has to run from the dom0 with effective
    UID 0\. While this is reasonable enough in an environment with trusted domU administrators,
    it's not a great idea when you're giving an account to anyone with $5\. Dealing
    with untrusted domU admins, as in a VPS hosting situation, requires some additional
    work to limit access using `ssh` and `sudo`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是 `xm` 必须以 dom0 的有效 UID 0 运行。虽然这在有信任的 domU 管理员的环境中是合理的，但当您给任何拥有 5 美元的人提供账户时，这并不是一个好主意。在
    VPS 托管环境中处理不受信任的 domU 管理员，需要做一些额外的工作来限制使用 `ssh` 和 `sudo` 的访问。
- en: 'First, configure `sudo`. Edit `/etc/sudoers` and append, for each user:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，配置 `sudo`。编辑 `/etc/sudoers` 并为每个用户追加：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, for each user, we create a *~/.ssh/authorized_keys* file like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为每个用户，我们创建一个类似于这样的 *~/.ssh/authorized_keys* 文件：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This line allows the user to log in with his key. Once he's logged in, `sshd`
    connects to the named domain console and automatically presents it to him, thus
    keeping domU administrators out of the dom0\. Also, note the options that start
    with `no`. They're important. We're not in the business of providing shell accounts.
    This is purely a console server—we want people to use their domUs rather than
    the dom0 for standard SSH stuff. These settings will allow users to access their
    domains' consoles via SSH in a way that keeps their access to the dom0 at a minimum.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这行允许用户使用其密钥登录。一旦登录，`sshd` 将连接到命名的域控制台并将其自动呈现给用户，从而将 domU 管理员排除在外。此外，请注意以 `no`
    开头的选项。它们很重要。我们不是提供 shell 账户的业务。这是一个纯控制台服务器——我们希望人们使用他们的 domU 而不是 dom0 来处理标准的 SSH
    事务。这些设置将允许用户通过 SSH 访问其域控制台，同时将他们对 dom0 的访问降到最低。
- en: A Menu for the Users
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为用户提供的菜单
- en: Of course, letting each user access his console is really just the beginning.
    By changing the `command` field in `authorized_keys` to a custom script, we can
    provide a menu with a startling array of features!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，让每个用户访问他的控制台只是开始。通过将 `authorized_keys` 中的 `command` 字段更改为自定义脚本，我们可以提供一个具有令人惊讶的功能系列的菜单！
- en: Here's a sample script that we call *xencontrol*. Put it somewhere in the filesystem—say
    `/usr/bin/xencontrol`—and then set the line in `authorized_keys` to call `xencontrol`
    rather than `xm console`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个我们称之为 *xencontrol* 的示例脚本。将其放置在文件系统中的某个位置——比如 `/usr/bin/xencontrol`——然后在
    `authorized_keys` 文件中设置一行来调用 `xencontrol` 而不是 `xm console`。
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When the user logs in via SSH, the SSH daemon runs this script in place of the
    user's login shell (which we recommend setting to `/bin/false` or its equivalent
    on your platform). The script then echoes some status information, an informative
    message, and a list of options. When the user enters a number, it runs the appropriate
    command (which we've allowed the user to run by configuring `sudo`).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户通过SSH登录时，SSH守护进程会代替用户的登录shell（我们建议将其设置为 `/bin/false` 或您平台上的等效项）运行此脚本。脚本随后会输出一些状态信息、一条信息性消息和一系列选项。当用户输入一个数字时，它会运行相应的命令（我们已经通过配置
    `sudo` 允许用户运行这些命令）。
- en: '* * *'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[43](#CHP-7-FNOTE-5)]) Our experience with other remote console tools has,
    overall, been unpleasant. Serial redirection systems work quite well. IP KVMs
    are barely preferable to toggling in the code on the front panel. On a good day.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[43](#CHP-7-FNOTE-5)]) 我们使用其他远程控制台工具的经验总体上并不愉快。串行重定向系统工作得相当好。IP KVM几乎比在前面面板上的代码切换更可取。在好日子里。
- en: PyGRUB, a Bootloader for DomUs
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyGRUB，一个DomUs的引导加载程序
- en: Up until now, the configurations that we've described, by and large, have specified
    the domU's boot configuration in the config file, using the `kernel, ramdisk`,
    and `extra` lines. However, there is an alternative method, which specifies a
    `bootloader` line in the config file and in turn uses that to load a kernel from
    the domU's filesystem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们描述的配置基本上都在配置文件中指定了domU的引导配置，使用 `kernel`、`ramdisk` 和 `extra` 行。然而，还有一种替代方法，即在配置文件中指定一个
    `bootloader` 行，然后使用它来从domU的文件系统加载内核。
- en: The bootloader most commonly used is PyGRUB, or Python GRUB. The best way to
    explain PyGRUB is probably to step back and examine the program it's based on,
    GRUB, the GRand Unified Bootloader. GRUB itself is a traditional bootloader—a
    program that sits in a location on the hard drive where the BIOS can load and
    execute it, which then itself loads and executes a kernel.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的引导加载程序是PyGRUB，或Python GRUB。解释PyGRUB的最佳方式可能是回顾它基于的程序，GRUB，即GRand Unified
    Bootloader。GRUB本身是一个传统的引导加载程序——一个程序，它位于硬盘上的某个位置，BIOS可以加载并执行它，然后它自己加载并执行内核。
- en: PyGRUB, therefore, is like GRUB for a domU. The Xen domain builder usually loads
    an OS kernel directly from the dom0 filesystem when the virtual machine is started
    (therefore acting like a bootloader itself). Instead, it can load PyGRUB, which
    then acts as a bootloader and loads the kernel from the domU filesystem.^([[44](#ftn.CHP-7-FNOTE-6)])
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PyGRUB就像domU的GRUB。Xen域构建器通常在虚拟机启动时直接从dom0文件系统加载操作系统内核（因此本身就像一个引导加载程序）。相反，它可以加载PyGRUB，然后充当引导加载程序并从domU文件系统加载内核。[^([44](#ftn.CHP-7-FNOTE-6))]
- en: PyGRUB is useful because it allows a more perfect separation between the administrative
    duties of the dom0 and the domU. When virtualizing the data center, you want to
    hand off virtual hardware to the customer. PyGRUB more effectively virtualizes
    the hardware. In particular, this means the customer can change his own kernel
    without the intervention of the dom0 administrator.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: PyGRUB很有用，因为它允许dom0和domU的管理职责之间有更完美的分离。在虚拟化数据中心时，您希望将虚拟硬件交给客户。PyGRUB更有效地虚拟化了硬件。特别是，这意味着客户可以更改自己的内核，而无需dom0管理员的干预。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*PyGRUB has been mentioned as a possible security risk because it reads an
    untrusted filesystem directly from the dom0\. PV-GRUB (see "PV-GRUB: A Safer Alternative
    to PyGRUB?" on [PV-GRUB: A SAFER ALTERNATIVE TO PYGRUB?](ch07s05.html#pv-grub_a_safer_alternative_to_pygrub
    "PV-GRUB: A SAFER ALTERNATIVE TO PYGRUB?")), which loads a trusted paravirtualized
    kernel from the dom0 then uses that to load and jump to the domU kernel, should
    improve this situation*.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*PyGRUB被提及为可能的安全风险，因为它直接从dom0读取未信任的文件系统。PV-GRUB（见“PV-GRUB：PyGRUB的一个更安全的替代方案？”[PV-GRUB：PyGRUB的一个更安全的替代方案？](ch07s05.html#pv-grub_a_safer_alternative_to_pygrub
    "PV-GRUB：PyGRUB的一个更安全的替代方案？"))，它从dom0加载一个可信的半虚拟化内核，然后使用它来加载和跳转到domU内核，应该会改善这种情况*。'
- en: 'PV-GRUB: A SAFER ALTERNATIVE TO PYGRUB?'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: PV-GRUB：PyGRUB的一个更安全的替代方案？
- en: PV-GRUB is an excellent reason to upgrade to Xen 3.3\. The problem with PyGRUB
    is that while it's a good simulation of a bootloader, it has to mount the domU
    partition in the dom0, and it interacts with the domU filesystem. This has led
    to at least one remote-execution exploit. PV-GRUB avoids the problem by loading
    an executable that is, quite literally, a paravirtualized version of the GRUB
    bootloader, which then runs entirely within the domU.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: PV-GRUB是升级到Xen 3.3的绝佳理由。PyGRUB的问题在于，虽然它是一个很好的引导加载程序的模拟，但它必须在dom0中挂载domU分区，并且与domU文件系统交互。这至少导致了至少一个远程执行漏洞。PV-GRUB通过加载一个实际上是GRUB引导加载程序的虚拟化版本的可执行文件来避免这个问题，然后它完全在domU中运行。
- en: This also has some other advantages. You can actually load the PV-GRUB binary
    from within the domU, meaning that you can load your first *menu.lst* from a read-only
    partition and have it fall through to a user partition, which then means that
    unlike my PyGRUB setup, users can never mess up their *menu.lst* to the point
    where they can't get into their rescue image.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这还有一些其他优点。您实际上可以在domU内部加载PV-GRUB二进制文件，这意味着您可以从只读分区加载您的第一个`menu.lst`文件，然后它将传递到用户分区，这意味着与我的PyGRUB设置不同，用户永远不可能将他们的`menu.lst`搞到无法进入救援镜像的程度。
- en: Note that Xen creates a domain in either 32- or 64-bit mode, and it can't switch
    later on. This means that a 64-bit PV-GRUB can't load 32-bit Linux kernels, and
    vice versa.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Xen可以在32位或64位模式下创建域，并且之后无法切换。这意味着64位的PV-GRUB无法加载32位Linux内核，反之亦然。
- en: Our PV-GRUB setup at `prgmr.com` starts with a normal `xm` config file, but
    with no bootloader and a `kernel= line` that points to PV-GRUB, instead of the
    domU kernel.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`prgmr.com`的PV-GRUB设置从正常的`xm`配置文件开始，但没有引导加载程序，并且有一个指向PV-GRUB的`kernel=`行，而不是domU内核。
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that we call the architecture-specific binary for PV-GRUB. The 32-bit (PAE)
    version is *pv-grub-x86_32*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们调用PV-GRUB的架构特定二进制文件。32位（PAE）版本是`pv-grub-x86_32`。
- en: 'This is enough to load a `regular menu.lst`, but what about this indestructible
    rescue image of which I spoke? Here''s how we do it on the new `prgmr.com` Xen
    3.3 servers. In the `xm` config file:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这足以加载一个常规的`menu.lst`，但关于我所说的那个坚不可摧的救援镜像怎么办？这是我们在新的`prgmr.com` Xen 3.3服务器上这样做的方法。在`xm`配置文件中：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, in */boot/grub/menu.lst* on the rescue disk:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在救援磁盘上的`/boot/grub/menu.lst`文件中：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first entry is the normal boot, with 64-bit PV-GRUB. The rest are various
    types of rescue and install boots. Note that we specify `(hd1)` for the rescue
    entries; in this case, the second disk is the rescue disk.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个条目是正常引导，使用64位的PV-GRUB。其余的是各种救援和安装引导。请注意，我们为救援条目指定了`(hd1)`；在这种情况下，第二个磁盘是救援磁盘。
- en: 'The normal boot loads PV-GRUB and the user''s */boot/grub/menu.lst* from `(hd0,0)`.
    Our default user-editable *menu.lst* looks like this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正常引导加载PV-GRUB和用户的`/boot/grub/menu.lst`文件，从`(hd0,0)`加载。我们的默认用户可编辑的`menu.lst`看起来像这样：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: PV-GRUB only runs on Xen 3.3 and above, and it seems that Red Hat has no plans
    to backport PV-GRUB to the version of Xen that is used by RHEL 5.*x*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: PV-GRUB仅在Xen 3.3及以上版本上运行，并且看起来Red Hat没有计划将PV-GRUB回滚到RHEL 5.*x*使用的Xen版本。
- en: Making PyGRUB Work
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使PyGRUB工作
- en: The domain's filesystem will need to include a */boot* directory with the appropriate
    files, just like a regular GRUB setup. We usually make a separate block device
    for */boot*, which we present to the domU as the first disk entry in its config
    file.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 域的文件系统需要包含一个包含适当文件的`/boot`目录，就像常规的GRUB设置一样。我们通常为`/boot`创建一个单独的块设备，并将其作为配置文件中的第一个磁盘条目提供给domU。
- en: 'To try PyGRUB, add a `bootloader=` line to the domU config file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试PyGRUB，请向domU配置文件中添加`bootloader=`行：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Of course, this being Xen, it may not be as simple as that. If you're using
    Debian, make sure that you have `libgrub, e2fslibs-dev`, and `reiserfslibs-dev`
    installed. (Red Hat Enterprise Linux and related distros use PyGRUB with their
    default Xen setup, and they include the necessary libraries with the Xen packages.)
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，由于这是Xen，可能不会这么简单。如果您使用Debian，请确保您已安装`libgrub, e2fslibs-dev`和`reiserfslibs-dev`。（Red
    Hat Enterprise Linux和相关发行版使用默认的Xen设置中的PyGRUB，并且它们将必要的库包含在Xen软件包中。）
- en: Even with these libraries installed, it may fail to work without some manual
    intervention. Older versions of PyGRUB expect the virtual disk to have a partition
    table rather than a raw filesystem. If you have trouble, this may be the culprit.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 即使安装了这些库，也可能在没有人工干预的情况下无法工作。较旧的PyGRUB版本期望虚拟磁盘有一个分区表而不是原始文件系统。如果您遇到问题，这可能就是原因。
- en: With modern versions of PyGRUB, it is unnecessary to have a partition table
    on the domU's virtual disk.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Self-Support with PyGRUB
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At prgmr.com, we give domU administrators the ability to repair and customize
    their own systems, which also saves us a lot of effort installing and supporting
    different distros. To accomplish this, we use PyGRUB and see to it that every
    customer has a bootable read-only rescue image they can boot into if their OS
    install goes awry. The domain config file for a customer who doesn't want us to
    do mirroring looks something like the following.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that we''re now exporting four disks to the virtual host: a */boot* partition
    on virtual sda, reserved for PyGRUB; two disks for user data, sdb and sdc; and
    a read-only CentOS install as sdd.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: A sufficiently technical user, with this setup and console access, needs almost
    no help from the dom0 administrator. He or she can change the operating system,
    boot a custom kernel, set up a software RAID, and boot the CentOS install to fix
    his setup if anything goes wrong.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the DomU for PyGRUB
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The only other important bit to make this work is a valid */grub/menu.lst*,
    which looks remarkably like the *menu.lst* in a regular Linux install. Our default
    looks like this and is stored on the disk exported as sda:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: /boot/grub/menu.lst *is frequently symlinked to either* /boot/grub/grub.conf
    *or* /etc/grub.conf. /boot/grub/menu.lst *is still the file that matters*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: As with native Linux, if you use a separate partition for */boot*, you'll need
    to either make a symlink at the root of */boot* that points boot back to `.` or
    make your kernel names relative to `/boot`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Here, the first and default entry is the CentOS distro kernel. The second entry
    is a generic Xen kernel, and the third choice is a read-only rescue image. Just
    like with native Linux, you can also specify devices by label rather than disk
    number.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: WORKING WITH PARTITIONS ON VIRTUAL DISKS
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In a standard configuration, partition 1 may be */boot*, with partition 2 as
    */*. In that case, partition 1 would have the configuration files and kernels
    in the same format as for normal GRUB.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s straightforward to create these partitions on an LVM device using `fdisk`.
    Doing so for a file is a bit harder. First, attach the file to a loop, using `losetup`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then create two partitions in the usual way, using your favorite partition
    editor:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, whether you''re using an LVM device or loop file, use `kpartx` to create
    device nodes from the partition table in that device:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Device nodes will be created under */dev/mapper* in the format *devnamep#*.
    Make a filesystem of your preferred type on the new partitions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Copy your filesystem image into */mnt*, make sure valid GRUB support files are
    in */mnt/boot* (just like a regular GRUB setup), and you are done.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: ^([[44](#CHP-7-FNOTE-6)]) This is an oversimplification. What actually happens
    is that PyGRUB copies a kernel from the domU filesystem, puts it in */tmp*, and
    then writes an appropriate domain config so that the domain builder can do its
    job. But the distinction is usually unimportant, so we've opted to approach PyGRUB
    as the bootloader it pretends to be.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[44](#CHP-7-FNOTE-6)]) 这是一个过于简化的说法。实际上发生的情况是 PyGRUB 从 domU 文件系统复制一个内核，将其放在
    */tmp* 目录下，然后写入适当的域配置，以便域构建者能够完成其工作。但这个区别通常并不重要，所以我们选择将 PyGRUB 视为它假装的引导加载程序。
- en: Wrap-Up
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter discussed things that we've learned from our years of relying on
    Xen. Mostly, that relates to how to partition and allocate resources between independent,
    uncooperative virtual machines, with a particular slant toward VPS hosting. We've
    described why you might host VPSs on Xen; specific allocation issues for CPU,
    disk, memory, and network access; backup methods; and letting customers perform
    self-service with scripts and PyGRUB.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了我们多年来依赖 Xen 所学到的内容。主要涉及如何在独立、不合作的虚拟机之间分区和分配资源，特别是针对 VPS 托管。我们描述了为什么你可能会在
    Xen 上托管 VPS；针对 CPU、磁盘、内存和网络访问的具体分配问题；备份方法；以及让客户通过脚本和 PyGRUB 进行自助服务。
- en: Note that there's some overlap between this chapter and some of the others.
    For example, we mention a bit about network configuration, but we go into far
    more detail on networking in [Chapter 5](ch05.html "Chapter 5. NETWORKING"), Networking.
    We describe `xm save` in the context of backups, but we talk a good deal more
    about it and how it relates to migration in [Chapter 9](ch09.html "Chapter 9. XEN
    MIGRATION"). Xen hosting's been a lot of fun. It hasn't made us rich, but it's
    presented a bunch of challenges and given us a chance to do some neat stuff.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章与其他章节之间有一些重叠。例如，我们提到了一些关于网络配置的内容，但在 [第五章](ch05.html "第五章。网络") 网络中，我们对此进行了更详细的讨论。我们在备份的上下文中描述了
    `xm save`，但在 [第九章](ch09.html "第九章。XEN 迁移") 中，我们对其进行了更多讨论，并探讨了它与迁移的关系。Xen 主机托管非常有趣。它并没有让我们变得富有，但它带来了一系列挑战，并给了我们机会做一些有趣的事情。
