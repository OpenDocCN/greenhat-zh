<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;HOSTING UNTRUSTED USERS UNDER XEN: LESSONS FROM THE TRENCHES"><div class="titlepage"><div><div><h1 class="title"><a id="hosting_untrusted_users_under_xen_lesson"/>Chapter 7. HOSTING UNTRUSTED USERS UNDER XEN: LESSONS FROM THE TRENCHES</h1></div></div></div><div class="informalfigure"><div class="mediaobject"><a id="I_mediaobject7_d1e7228"/><img src="httpatomoreillycomsourcenostarchimages333191.png.jpg" alt="image with no caption"/></div></div><p>Now that we've gone over the basics of Xen administration—storage, networking, provisioning, and management—let's look at applying these basics in practice. This chapter is mostly a case study of our <a id="idx-CHP-7-0484" class="indexterm"/>VPS hosting firm, prgmr.com, and the lessons we've learned from renting Xen instances to the public.</p><p>The most important lesson of public Xen hosting is that the users can't be trusted to cooperate with you or each other. Some people will always try to seize as much as they can. Our focus will be on preventing this tragedy of the commons.</p><div class="sect1" title="Advantages for the Users"><div class="titlepage"><div><div><h1 class="title"><a id="advantages_for_the_users"/>Advantages for the Users</h1></div></div></div><p>There's exactly one basic reason that a user would want to use a <a id="idx-CHP-7-0485" class="indexterm"/>Xen VPS rather than paying to colocate a box in your data center: it's cheap, especially for someone who's just interested in some basic services, rather than massive raw performance.</p><div class="sidebar"><a id="grid_computing_and_virtualization"/><p class="title">GRID COMPUTING AND VIRTUALIZATION</p><p>One term that you hear fairly often in connection with Xen is <span class="emphasis"><em>grid computing</em></span>. The basic idea behind grid computing is that you can quickly <a id="idx-CHP-7-0486" class="indexterm"/>and automatically provision and destroy nodes. Amazon's EC2 service is a good example of a grid computing platform that allows you to rent Linux servers by the hour.<a id="idx-CHP-7-0487" class="indexterm"/></p><p>Grid computing doesn't require virtualization, but the two concepts are fairly closely linked. One could design a system using physical machines and PXEboot for fast, easy, automated provisioning without using Xen, but a virtualization system would make the setup more lightweight, agile, and efficient.</p><p>There are several open source projects that are attempting to create a standard and open interface to provision "grid computing" resources. One such project is <a id="idx-CHP-7-0488" class="indexterm"/>Eucalyptus (<a class="ulink" href="http://www.eucalyptus.com/">http://www.eucalyptus.com/</a>). We feel that standard frameworks like this—that allow you to easily switch between grid computing providers—are essential if "the grid" is to survive.</p></div><p>Xen also gives users nearly all the advantages they'd get from colocating a box: their own publicly routed network interface, their own disk, root access, and so forth. With a 128MB VM, they can run DNS, light mail service, a web server, IRC, SSH, and so on. For lightweight services like these, the power of the box is much less important than its basic existence—just having something available and publicly accessible makes life more convenient.</p><p>You also have the basic <a id="idx-CHP-7-0489" class="indexterm"/>advantages of virtualization, namely, that <a id="idx-CHP-7-0490" class="indexterm"/>hosting one server with 32GB of RAM is a whole lot cheaper than hosting 32 servers with 1GB of RAM each (or even 4 servers with 8GB RAM each). In fact, the price of RAM being what it is, I would argue that it's difficult to even economically justify hosting a general-purpose server with less than 32GB of RAM.</p><p>The last important feature of Xen is that, relative to other virtualization systems, it's got a good combination of light weight, strong partitioning, and robust resource controls. Unlike some other virtualization options, it's consistent—a user can rely on getting exactly the amount of memory, disk space, and network bandwidth that he's signed up for and approximately as much CPU and disk bandwidth.</p></div></div>
<div class="sect1" title="Shared Resources and Protecting Them from the Users"><div class="titlepage"><div><div><h1 class="title"><a id="shared_resources_and_protecting_them_fro"/>Shared Resources and Protecting Them from the Users</h1></div></div></div><div class="epigraph"><p>Xen's design is congruent to good security.<a id="idx-CHP-7-0491" class="indexterm"/></p><div class="attribution"><span>—<span class="attribution">
<span class="author"><span class="firstname">Tavis</span> <span class="surname">Ormandy</span></span>
<em class="citation"><a class="ulink" href="http://taviso.decsystem.org/virtsec.pdf">http://taviso.decsystem.org/virtsec.pdf</a></em>
</span></span></div></div><p>It's a ringing endorsement, by security-boffin standards. By and large, with Xen, we're not worried about keeping people from breaking out of their virtual machines—Xen itself is supposed to provide an appropriate level of isolation. In paravirtualized mode, Xen doesn't expose hardware drivers to domUs, which eliminates one major attack vector.<sup>[<a id="CHP-7-FNOTE-1" href="#ftn.CHP-7-FNOTE-1" class="footnote">39</a>]</sup> For the most part, securing a dom0 is exactly like securing any other server, except in one area.</p><p>That area of possible concern is in the access controls for <a id="idx-CHP-7-0493" class="indexterm"/>shared resources, which are not entirely foolproof. The primary worry is that malicious users could gain more resources than they're entitled to, or in extreme cases cause denial-of-service attacks by exploiting flaws in Xen's accounting. In other words, we are in the business of enforcing performance isolation, rather than specifically trying to protect the dom0 from attacks via the domUs.</p><p>Most of the resource controls that we present here are aimed at users who aren't necessarily malicious—just, perhaps, exuberant.</p><div class="sect2" title="Tuning CPU Usage"><div class="titlepage"><div><div><h2 class="title"><a id="tuning_cpu_usage"/>Tuning CPU Usage</h2></div></div></div><p>The first shared resource of interest is the CPU. While memory and disk size are easy to tune—you can just specify memory in the config file, while disk size is determined by the size of the backing device—fine-grained CPU allocation requires you to adjust the scheduler.<a id="idx-CHP-7-0494" class="indexterm"/></p><div class="sect3" title="Scheduler Basics"><div class="titlepage"><div><div><h3 class="title"><a id="scheduler_basics"/>Scheduler Basics</h3></div></div></div><p>The Xen scheduler acts as a referee between the running domains. In some ways it's a lot like the <a id="idx-CHP-7-0495" class="indexterm"/>Linux scheduler: It can preempt processes as needed, it tries its best to ensure fair allocation, and it ensures that the CPU wastes as few cycles as possible. As the name suggests, Xen's scheduler schedules domains to run on the physical CPU. These domains, in turn, schedule and run processes from their internal run queues.<a id="idx-CHP-7-0496" class="indexterm"/></p><p>Because the dom0 is just another domain as far as Xen's concerned, it's subject to the same scheduling algorithm as the domUs. This can lead to trouble if it's not assigned a high enough weight because the dom0 has to be able to respond to I/O requests. We'll go into more detail on that topic a bit later, after we describe the general procedures for adjusting domain weights.</p><p>Xen can use a variety of scheduling algorithms, ranging from the simple to the baroque. Although Xen has shipped <a id="idx-CHP-7-0497" class="indexterm"/>with a number of schedulers in the past, we're going to concentrate on the <span class="emphasis"><em>credit scheduler</em></span>; it's the current default and recommended choice and the only one that the Xen team has indicated any interest in keeping.</p><p>The <code class="literal">xm dmesg</code> command will tell you, among other things, what scheduler Xen is using.<a id="idx-CHP-7-0498" class="indexterm"/></p><a id="I_programlisting7_d1e7378"/><pre class="programlisting"># xm dmesg | grep scheduler
(XEN) Using scheduler: SMP Credit Scheduler (credit)</pre><p>If you want to change the scheduler, you can set it as a boot parameter—to change to the SEDF scheduler, for example, append <code class="literal">sched=sedf</code> to the kernel line in GRUB. (That's the Xen kernel, not the dom0 Linux kernel loaded by the first <code class="literal">module</code> line.)</p></div><div class="sect3" title="VCPUs and Physical CPUs"><div class="titlepage"><div><div><h3 class="title"><a id="vcpus_and_physical_cpus"/>VCPUs and Physical CPUs</h3></div></div></div><p>For convenience, we consider each Xen domain to have one or more virtual CPUs (<a id="idx-CHP-7-0499" class="indexterm"/>VCPUs), which periodically run on the <a id="idx-CHP-7-0500" class="indexterm"/>physical CPUs. These are the entities that consume credits when run. To examine VCPUs, use <code class="literal">xm vcpu-list &lt;domain&gt;</code>
:</p><a id="I_programlisting7_d1e7408"/><pre class="programlisting"># xm vcpu-list horatio
Name                             ID VCPUs   CPU State   Time(s) CPU Affinity
horatio                          16     0     0   ---  140005.6 any cpu
horatio                          16     1     2   r--  139968.3 any cpu</pre><p>In this case, the domain has two VCPUs, 0 and 1. VCPU 1 is in the <span class="emphasis"><em>running</em></span> state on (physical) CPU 1. Note that Xen will try to spread VCPUs across CPUs as much as possible. Unless you've pinned them manually, VCPUs can occasionally switch CPUs, depending on which physical CPUs are available.</p><p>To specify the number of VCPUs for a domain, specify the <code class="literal">vcpus=</code> directive in the config file. You can also change the number of VCPUs while a domain is running using <code class="literal">xm vcpu-set</code>. However, note that you can decrease the number of VCPUs this way, but you can't increase the number of VCPUs beyond the initial count.</p><p>To set the CPU affinity, use <code class="literal">xm vcpu-pin &lt;domain&gt; &lt;vcpu&gt; &lt;pcpu&gt;</code>. For example, to switch the CPU assignment in the domain <span class="emphasis"><em>horatio</em></span>, so that VCPU0 runs on CPU2 and VCPU1 runs on CPU0:</p><a id="I_programlisting7_d1e7431"/><pre class="programlisting"># xm vcpu-pin horatio 0 2
# xm vcpu-pin horatio 1 0</pre><p>Equivalently, you can pin VCPUs in the domain config file (<span class="emphasis"><em>/etc/xen/horatio</em></span>, if you're using our standard naming convention) like this:</p><a id="I_programlisting7_d1e7438"/><pre class="programlisting">vcpus=2
cpus=[0,2]</pre><p>This gives the domain two VCPUs, pins the first VCPU to the first physical CPU, and pins the second VCPU to the third physical CPU.</p></div><div class="sect3" title="Credit Scheduler"><div class="titlepage"><div><div><h3 class="title"><a id="credit_scheduler"/>Credit Scheduler</h3></div></div></div><p>The Xen team designed the <a id="idx-CHP-7-0501" class="indexterm"/>credit scheduler to minimize wasted <a id="idx-CHP-7-0502" class="indexterm"/>CPU time. This makes it a <span class="emphasis"><em>work-conserving</em></span> scheduler, in that it tries to ensure that the CPU will always be working whenever there is work for it to do.<a id="idx-CHP-7-0503" class="indexterm"/></p><p>As a consequence, if there is more real CPU available than the domUs are demanding, all domUs get all the CPU they want. When there is contention—that is, when the domUs in aggregate want more CPU than actually exists—then the scheduler arbitrates fairly between the domains that want CPU.</p><p>Xen does its best to do a fair division, but the scheduling isn't perfect by any stretch of the imagination. In particular, cycles spent servicing I/O by domain 0 are not charged to the responsible domain, leading to situations where I/O-intensive clients get a disproportionate share of CPU usage. <a id="idx-CHP-7-0504" class="indexterm"/>Nonetheless, you can get pretty good allocation in nonpathological cases. (Also, in our experience, the CPU sits idle most of the time anyway.)</p><p>The <a id="idx-CHP-7-0505" class="indexterm"/>credit scheduler assigns each domain a <span class="emphasis"><em>weight</em></span> <a id="idx-CHP-7-0506" class="indexterm"/>and, optionally, a <span class="emphasis"><em>cap</em></span>. The weight indicates the relative CPU allocation of a domain—if the CPU is scarce, a domain with a weight of 512 will receive twice as much <a id="idx-CHP-7-0507" class="indexterm"/>CPU time as a domain with a weight of 256 (the default). The cap sets an absolute limit on the amount of CPU time a domain can use, expressed in hundredths of a CPU. Note that the CPU cap can exceed 100 on multiprocessor hosts.</p><p>The scheduler transforms the weight <a id="idx-CHP-7-0508" class="indexterm"/>into a <span class="emphasis"><em>credit</em></span> allocation for each VCPU, using a separate accounting thread. As a VCPU runs, it consumes credits. If a VCPU runs out of credits, it only runs when other, more thrifty <a id="idx-CHP-7-0509" class="indexterm"/>VCPUs have finished executing, as shown in <a class="xref" href="ch07s02.html#vcpus_wait_in_two_queues_one_for_vcpus_w" title="Figure 7-1. VCPUs wait in two queues: one for VCPUs with credits and the other for those that are over their allotment. Once the first queue is exhausted, the CPU will pull from the second.">Figure 7-1</a>. Periodically, the accounting thread goes through and gives everybody more credits.</p><div class="figure"><a id="vcpus_wait_in_two_queues_one_for_vcpus_w"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject7_d1e7513"/><img src="httpatomoreillycomsourcenostarchimages333223.png.jpg" alt="VCPUs wait in two queues: one for VCPUs with credits and the other for those that are over their allotment. Once the first queue is exhausted, the CPU will pull from the second."/></div></div><p class="title">Figure 7-1. VCPUs wait in two queues: one for VCPUs with credits and the other for those that are over their allotment. Once the first queue is exhausted, the CPU will pull from the second.</p></div><p>In this case, the details are probably less important than the practical application. Using the <code class="literal">xm sched-credit</code> commands, we can adjust CPU allocation on a per-domain basis. For example, here we'll increase a domain's CPU allocation. First, to list the weight and cap for the domain horatio:<a id="idx-CHP-7-0510" class="indexterm"/></p><a id="I_programlisting7_d1e7526"/><pre class="programlisting"># xm sched-credit -d horatio
{'cap': 0, 'weight': 256}</pre><p>Then, to modify the scheduler's parameters:</p><a id="I_programlisting7_d1e7530"/><pre class="programlisting"># xm sched-credit -d horatio -w 512
# xm sched-credit -d horatio
{'cap': 0, 'weight': 512}</pre><p>Of course, the value "512" <a id="idx-CHP-7-0511" class="indexterm"/>only has meaning relative to the other domains that are running on the machine. Make sure to set all the domains' <a id="idx-CHP-7-0512" class="indexterm"/>weights appropriately.</p><p>To set the cap for a domain:<a id="idx-CHP-7-0513" class="indexterm"/></p><a id="I_programlisting7_d1e7552"/><pre class="programlisting"># xm sched-<a id="idx-CHP-7-0514" class="indexterm"/>credit -d domain -c cap</pre></div></div><div class="sect2" title="Scheduling for Providers"><div class="titlepage"><div><div><h2 class="title"><a id="scheduling_for_providers"/>Scheduling for Providers</h2></div></div></div><p>We decided to divide the CPU along the same lines as the available RAM—it stands to reason that a user paying for half the RAM in a box will want more CPU than someone with a 64MB domain. Thus, in our setup, a customer with 25 percent of the RAM also has a minimum share of 25 percent of the CPU cycles.<a id="idx-CHP-7-0515" class="indexterm"/></p><p>The simple way to do this is to assign each CPU a weight equal to the number of megabytes of memory it has and leave the cap empty. The scheduler will then handle converting that into fair proportions. For example, our aforementioned user with half the RAM will get about as much <a id="idx-CHP-7-0516" class="indexterm"/>CPU time as the rest of the users put together.</p><p>Of course, that's the worst case; that is what the user will get in an environment of constant struggle for the CPU. Idle domains will automatically yield the CPU. If all domains but one are idle, that one can have the entire CPU to itself.</p><div class="note" title="Note"><h3 class="title"><a id="note-28"/>Note</h3><p><span class="emphasis"><em>It's essential to make sure that the dom0 has sufficient CPU to service I/O requests. You can handle this by dedicating a CPU to the dom0 or by giving the dom0 a very high weight—high enough to ensure that it never runs out of credits. At prgmr.com, we handle the problem by weighting each domU with its RAM amount and weighting the dom0 at 6000</em></span>.</p></div><p>This simple weight = memory formula becomes a bit more complex when dealing with multiprocessor systems because independent systems of CPU allocation come into play. A good rule would be to allocate VCPUs in proportion to memory (and therefore in proportion to weight). For example, a domain with half the RAM on a box with four cores (and hyperthreading turned off) should have at least two VCPUs. Another solution would be to give all domains as many VCPUs as physical processors in the box—this would allow all domains to burst to the full CPU capacity of the physical machine but might lead to increased overhead from context swaps.</p></div><div class="sect2" title="Controlling Network Resources"><div class="titlepage"><div><div><h2 class="title"><a id="controlling_network_resources"/>Controlling Network Resources</h2></div></div></div><p><a id="idx-CHP-7-0517" class="indexterm"/>Network resource controls are, frankly, essential to any kind of shared hosting operation. Among the many lessons that we've learned from Xen hosting has been that if you provide free bandwidth, some users will exploit it for all it's worth. This isn't a Xen-specific observation, but it's especially noticeable with the sort of cheap VPS hosting Xen lends itself to.</p><p>We prefer to use <code class="literal">network-bridge</code>, since that's the default. For a more thorough look at <code class="literal">network-bridge</code>, take a look at <a class="xref" href="ch05.html" title="Chapter 5. NETWORKING">Chapter 5</a>.<a id="I_indexterm7_d1e7599" class="indexterm"/><a id="I_indexterm7_d1e7602" class="indexterm"/><a id="I_indexterm7_d1e7607" class="indexterm"/></p><div class="sect3" title="Monitoring Network Usage"><div class="titlepage"><div><div><h3 class="title"><a id="monitoring_network_usage"/>Monitoring Network Usage</h3></div></div></div><p>Given that some users will consume as much bandwidth as possible, it's vital to have some way to monitor network traffic.<sup>[<a id="CHP-7-FNOTE-2" href="#ftn.CHP-7-FNOTE-2" class="footnote">40</a>]</sup></p><p>To monitor <a id="idx-CHP-7-0518" class="indexterm"/>network usage, we use <a id="idx-CHP-7-0519" class="indexterm"/>BandwidthD on a physical <a id="idx-CHP-7-0520" class="indexterm"/>SPAN port. It's a simple tool that counts bytes going through a switch—nothing Xen-specific here. We feel comfortable doing this because our provider doesn't allow anything but IP packets in or out, and our antispoof rules are good enough to protect us from users spoofing their IP on outgoing packets.</p><p>A similar approach would be to extend the <span class="emphasis"><em>dom0 is a switch</em></span> analogy and use SNMP monitoring software. As mentioned in <a class="xref" href="ch05.html" title="Chapter 5. NETWORKING">Chapter 5</a>, it's important to specify a <code class="literal">vifname</code> for each domain if you're doing this. In any case, we'll leave the particulars of bandwidth monitoring up to you.</p><div class="sidebar"><a id="arp_cache_poisoning"/><p class="title">ARP CACHE POISONING</p><p>If you use the default <code class="literal">network-bridge setup</code>, you are vulnerable <a id="idx-CHP-7-0521" class="indexterm"/>to ARP cache poisoning, just as on any layer 2 switch.<a id="idx-CHP-7-0522" class="indexterm"/></p><p>The idea is that the interface counters on a layer 2 switch—such as the virtual switch used by <code class="literal">network-bridge</code>—watch traffic as it passes through a particular port. Every time a switch sees an Ethernet frame or ARP is-at, it keeps track of what port and MAC it came from. If it gets a frame destined for a MAC address in its cache, it sends that frame down the proper port (and only the proper port). If the bridge sees a frame destined for a MAC that is not in the cache, it sends that frame to all ports.<sup>[<a id="CHP-7-FNOTE-3" href="#ftn.CHP-7-FNOTE-3" class="footnote">41</a>]</sup></p><p>Clever, no? In most cases this means that you almost never see Ethernet frames destined for other MAC addresses (other than broadcasts, etc.). However, this feature is designed purely as an optimization, not a security measure. As those of you with cable providers who do MAC address verification know quite well, it is fairly trivial to fake a MAC address. This means that a malicious user can fill the (limited in size) ARP cache with bogus MAC addresses, drive out the good data, and force all packets to go down all interfaces. At this point the switch becomes basically a hub, and the counters on all ports will show all traffic for any port.</p><p>There are two ways we have worked around the problem. You could use Xen's <code class="literal">network-route</code> networking model, which doesn't use a virtual bridge. The other approach is to ignore the interface counters and use something like BandwidthD, which bases its accounting on IP packets.</p></div><p>Once you can examine traffic quickly, the next step is to shape the users. The principles for network traffic shaping and policing are the same as for standalone boxes, except that you can also implement policies on the Xen host. Let's look at how to limit both incoming and outgoing traffic for a particular interface—as if, say, you have a customer who's going over his bandwidth allotment.</p></div><div class="sect3" title="Network Shaping Principles"><div class="titlepage"><div><div><h3 class="title"><a id="network_shaping_principles"/>Network Shaping Principles</h3></div></div></div><p>The first thing to know about shaping is that it only works on <a id="idx-CHP-7-0523" class="indexterm"/>outgoing traffic. Although it is possible to <span class="emphasis"><em>police</em></span> <a id="idx-CHP-7-0524" class="indexterm"/>incoming traffic, it isn't as effective. Fortunately, both directions look like outgoing traffic at some point in their passage through the dom0, as shown in <a class="xref" href="ch07s02.html#incoming_traffic_comes_from_the_internet" title="Figure 7-2. Incoming traffic comes from the Internet, goes through the virtual bridge, and gets shaped by a simple nonhierarchical filter. Outgoing traffic, on the other hand, needs to go through a system of filters that assign packets to classes in a hierarchical queuing discipline.">Figure 7-2</a>. (When we refer to outgoing and <a id="idx-CHP-7-0525" class="indexterm"/>incoming traffic in the following description, we mean from the perspective of the domU.)</p><div class="figure"><a id="incoming_traffic_comes_from_the_internet"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject7_d1e7713"/><img src="httpatomoreillycomsourcenostarchimages333225.png.jpg" alt="Incoming traffic comes from the Internet, goes through the virtual bridge, and gets shaped by a simple nonhierarchical filter. Outgoing traffic, on the other hand, needs to go through a system of filters that assign packets to classes in a hierarchical queuing discipline."/></div></div><p class="title">Figure 7-2. Incoming traffic comes from the Internet, goes through the virtual bridge, and gets shaped by a simple nonhierarchical filter. Outgoing traffic, on the other hand, needs to go through a system of filters that assign packets to classes in a hierarchical queuing discipline.</p></div></div><div class="sect3" title="Shaping Incoming Traffic"><div class="titlepage"><div><div><h3 class="title"><a id="shaping_incoming_traffic"/>Shaping Incoming Traffic</h3></div></div></div><p>We'll start with incoming traffic because it's much simpler to limit than outgoing traffic. The easiest way to shape incoming traffic is probably the <span class="emphasis"><em>token bucket filter</em></span> queuing discipline, which is a simple, effective, and lightweight way to slow down an interface.<a id="idx-CHP-7-0526" class="indexterm"/><a id="idx-CHP-7-0527" class="indexterm"/><a id="idx-CHP-7-0528" class="indexterm"/></p><p>The token bucket filter, or <a id="idx-CHP-7-0529" class="indexterm"/>TBF, takes its name from the metaphor of a bucket of tokens. Tokens stream into the bucket at a defined and constant rate. Each byte of data sent takes one token from the bucket and goes out immediately—when the bucket's empty, data can only go as tokens come in. The bucket itself has a limited capacity, which guarantees that only a reasonable amount of data will be sent out at once. To use the TBF, we add a <code class="literal">qdisc</code> (<span class="emphasis"><em>queuing discipline</em></span>) to perform the actual work of traffic limiting. To limit the virtual interface <code class="literal">osric</code> to 1 megabit per second, with bursts up to 2 megabits and maximum allowable latency of 50 milliseconds:<a id="idx-CHP-7-0530" class="indexterm"/></p><a id="I_programlisting7_d1e7755"/><pre class="programlisting"># tc qdisc add dev osric root tbf rate 1mbit latency 50ms peakrate 2mbit maxburst 40MB</pre><p>This adds a <code class="literal">qdisc</code> to the device <code class="literal">osric</code>. The next arguments specify where to add it (<code class="literal">root</code>) and what sort of <code class="literal">qdisc</code> it is (<code class="literal">tbf</code>). Finally, we specify the rate, latency, burst rate, and amount that can go at burst rate. These parameters correspond to the token flow, amount of latency the packets are allowed to have (before the driver signals the operating system that its buffers are full), maximum rate at which the bucket can empty, and the size of the bucket.</p></div><div class="sect3" title="Shaping Outgoing Traffic"><div class="titlepage"><div><div><h3 class="title"><a id="shaping_outgoing_traffic"/>Shaping Outgoing Traffic</h3></div></div></div><p>Having shaped <a id="idx-CHP-7-0531" class="indexterm"/>incoming traffic, we can focus on limiting <a id="idx-CHP-7-0532" class="indexterm"/>outgoing traffic. This is a bit more complex because the outgoing traffic for all domains goes through a single interface, so a single <a id="idx-CHP-7-0533" class="indexterm"/>token bucket won't work. The policing filters might work, but they handle the problem by dropping packets, which is … bad. Instead, we're going to apply traffic <a id="idx-CHP-7-0534" class="indexterm"/>shaping to the outgoing physical Ethernet device, peth0, with a <span class="emphasis"><em>Hierarchical Token Bucket</em></span>, or <a id="idx-CHP-7-0535" class="indexterm"/>HTB <code class="literal">qdisc</code>.<a id="idx-CHP-7-0536" class="indexterm"/><a id="idx-CHP-7-0537" class="indexterm"/></p><p>The HTB discipline acts like the simple token bucket, but with a hierarchy of buckets, each with its own rate, and a system of filters to assign packets to buckets. Here's how to set it up.</p><p>First, we have to make sure that the packets on Xen's virtual bridge traverse <code class="literal">iptables</code>:</p><a id="I_programlisting7_d1e7825"/><pre class="programlisting"># echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</pre><p>This is so that we can mark packets according to which domU emitted them. There are other reasons, but that's the important one in terms of our traffic-shaping setup. Next, for each domU, we add a rule to mark packets from the corresponding network interface:</p><a id="I_programlisting7_d1e7829"/><pre class="programlisting"># iptables -t mangle -A FORWARD -m physdev --physdev-in baldr -j MARK --set-mark 5</pre><p>Here the number 5 is an arbitrary mark—it's not important what the number is, as long as there's a useful mapping between number and domain. We're using the domain ID. We could also use <code class="literal">tc</code> filters directly that match on source IP address, but it feels more elegant to have everything keyed to the domain's physical network device. Note that we're using <code class="literal">physdev-in</code>—traffic that goes out from the domU comes in to the dom0, as <a class="xref" href="ch07s02.html#we_shape_traffic_coming_into_the_domu_as" title="Figure 7-3. We shape traffic coming into the domU as it comes into the dom0 from the physical device, and shape traffic leaving the domU as it enters the dom0 on the virtual device.">Figure 7-3</a> shows.</p><div class="figure"><a id="we_shape_traffic_coming_into_the_domu_as"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject7_d1e7844"/><img src="httpatomoreillycomsourcenostarchimages333227.png.jpg" alt="We shape traffic coming into the domU as it comes into the dom0 from the physical device, and shape traffic leaving the domU as it enters the dom0 on the virtual device."/></div></div><p class="title">Figure 7-3. We shape traffic coming into the domU as it comes into the dom0 from the physical device, and shape traffic leaving the domU as it enters the dom0 on the virtual device.</p></div><p>Next we create a <a id="idx-CHP-7-0538" class="indexterm"/><a id="idx-CHP-7-0539" class="indexterm"/>HTB <code class="literal">qdisc</code>. We won't go over the HTB options in too much detail—see the documentation at <a class="ulink" href="http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm">http://luxik.cdi.cz/~devik/qos/htb/manual/userg.htm</a> for more details:</p><a id="I_programlisting7_d1e7865"/><pre class="programlisting"># tc qdisc add dev peth0 root handle 1: htb default 12</pre><p>Then we make some classes to put <a id="idx-CHP-7-0540" class="indexterm"/>traffic into. Each class will get traffic from one domU. (As the HTB docs explain, we're also making a parent class so that they can share surplus bandwidth.)</p><a id="I_programlisting7_d1e7874"/><pre class="programlisting"># tc class add dev peth0 parent 1: classid 1:1 htb rate 100mbit
# tc class add dev peth0 parent 1:1 classid 1:2 htb rate 1mbit</pre><p>Now that we have a class for our domU's traffic, we need a filter that will assign packets to it.</p><a id="I_programlisting7_d1e7878"/><pre class="programlisting"># tc filter add dev peth0 protocol ip parent 1:0 prio 1 handle 5 fw flowid 1:2</pre><p>Note that we're matching on the "handle" that we set earlier <a id="idx-CHP-7-0541" class="indexterm"/>using <code class="literal">iptables</code>. This assigns the packet to the 1:2 class, which we've previously limited to 1 megabit per second.</p><p>At this point traffic to and from the target domU is essentially shaped, as demonstrated by <a class="xref" href="ch07s02.html#the_effect_of_the_shaping_filters" title="Figure 7-4. The effect of the shaping filters">Figure 7-4</a>. You can easily add commands like these to the end <a id="idx-CHP-7-0542" class="indexterm"/>of your <code class="literal">vif</code> script, be it <code class="literal">vif-bridge</code>, <code class="literal">vif-route</code>, or a wrapper. We would also like to emphasize that this is only an example and that the <a id="idx-CHP-7-0543" class="indexterm"/>Linux Advanced Routing and Traffic Control how-to at <a class="ulink" href="http://lartc.org/">http://lartc.org/</a> is an excellent place to look for further documentation. The <code class="literal">tc</code> man page is also informative.</p><div class="figure"><a id="the_effect_of_the_shaping_filters"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject7_d1e7923"/><img src="httpatomoreillycomsourcenostarchimages333229.png.jpg" alt="The effect of the shaping filters"/></div></div><p class="title">Figure 7-4. The effect of the shaping filters</p></div></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-7-FNOTE-1" href="#CHP-7-FNOTE-1" class="para">39</a>] </sup>In HVM mode, the emulated QEMU devices are something <a id="idx-CHP-7-0492" class="indexterm"/>of a risk, which is part of why we don't offer HVM domains.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-7-FNOTE-2" href="#CHP-7-FNOTE-2" class="para">40</a>] </sup>In this case, we're talking about bandwidth monitoring. You should also run some sort of IDS, such as Snort, to watch for outgoing abuse (we do) but there's nothing Xen-specific about that.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-7-FNOTE-3" href="#CHP-7-FNOTE-3" class="para">41</a>] </sup>We are using the words <span class="emphasis"><em>port</em></span> and <span class="emphasis"><em>interface</em></span> interchangeably here. This is a reasonable simplification in the context of interface counters on an SNMP-capable switch.</p></div></div></div>
<div class="sect1" title="Storage in a Shared Hosting Environment"><div class="titlepage"><div><div><h1 class="title"><a id="storage_in_a_shared_hosting_environment"/>Storage in a Shared Hosting Environment</h1></div></div></div><p>As with so much else in system administration, a bit of planning can save a lot of trouble. Figure out beforehand where you're going to store pristine filesystem images, where configuration files go, and where customer data will live.<a id="idx-CHP-7-0544" class="indexterm"/><a id="idx-CHP-7-0545" class="indexterm"/></p><p>For pristine images, there are a lot of conventions—some people use <span class="emphasis"><em>/diskimages</em></span>, some use <span class="emphasis"><em>/opt/xen, /var/xen</em></span> or similar, some use a subdirectory of <span class="emphasis"><em>/home</em></span>. Pick one and stick with it.<a id="I_indexterm7_d1e7952" class="indexterm"/><a id="I_indexterm7_d1e7955" class="indexterm"/><a id="I_indexterm7_d1e7958" class="indexterm"/></p><p>Configuration files should, <a id="idx-CHP-7-0546" class="indexterm"/><a id="idx-CHP-7-0547" class="indexterm"/>without exception, go in <span class="emphasis"><em>/etc/xen</em></span>. If you don't give <span class="emphasis"><em>xm create</em></span> a full path, it'll look <a id="idx-CHP-7-0548" class="indexterm"/>for the file in <span class="emphasis"><em>/etc/xen</em></span>. Don't disappoint it.</p><p>As for <a id="idx-CHP-7-0549" class="indexterm"/>customer data, we recommend that serious hosting providers use LVM. This allows greater flexibility and manageability than blktap-mapped files while maintaining good performance. <a class="xref" href="ch04.html" title="Chapter 4. STORAGE WITH XEN">Chapter 4</a> covers the details <a id="idx-CHP-7-0550" class="indexterm"/>of working with LVM (or at least enough to get started), as well as many other available storage options and their advantages. Here we're confining ourselves to lessons that we've learned from our adventures in <a id="idx-CHP-7-0551" class="indexterm"/>shared hosting.</p><div class="sect2" title="Regulating Disk Access with ionice"><div class="titlepage"><div><div><h2 class="title"><a id="regulating_disk_access_with_ionice"/>Regulating Disk Access with ionice</h2></div></div></div><p>One common problem with VPS hosting is that customers—or your own housekeeping processes, like backups—will use enough I/O bandwidth to slow down everyone on the machine. Furthermore, I/O isn't really affected by the scheduler tweaks discussed earlier. A domain can request data, hand <a id="idx-CHP-7-0552" class="indexterm"/>off the CPU, and save its credits until it's notified of the data's arrival.<a id="idx-CHP-7-0553" class="indexterm"/><a id="idx-CHP-7-0554" class="indexterm"/></p><p>Although you can't set hard limits on disk access rates as you can with the network QoS, you can use the <code class="literal">ionice</code> command to prioritize the different domains into subclasses, with a syntax like:</p><a id="I_programlisting7_d1e8031"/><pre class="programlisting"># ionice -p &lt;PID&gt; -c &lt;class&gt; -n &lt;priority within class&gt;</pre><p>Here <code class="literal">-n</code> is the knob you'll ordinarily want to twiddle. It can range from 0 to 7, with lower numbers taking precedence.</p><p><span class="emphasis"><em>We recommend always specifying 2 for the class</em></span>. Other classes exist—3 is idle and 1 is realtime—but idle is extremely conservative, while realtime is so aggressive as to have a good chance of locking up the system. The within-class priority is aimed at proportional allocation, and is thus much more likely to be what you want.</p><p>Let's look at <code class="literal">ionice</code> in action. Here we'll test <code class="literal">ionice</code> with two different domains, one with the highest normal priority, the other with the lowest.</p><p>First, <code class="literal">ionice</code> only works with the <a id="idx-CHP-7-0555" class="indexterm"/>CFQ I/O scheduler. To check that you're using the CFQ scheduler, run this command in the dom0:</p><a id="I_programlisting7_d1e8059"/><pre class="programlisting"># cat /sys/block/[sh]d[a-z]*/queue/scheduler
noop anticipatory deadline [cfq]
noop anticipatory deadline [cfq]</pre><p>The word in brackets is the selected scheduler. If it's not <code class="literal">[cfq]</code>, reboot with the parameter <code class="literal">elevator =cfq</code>.</p><p>Next we find the processes we want to <code class="literal">ionice</code>. Because we're using <code class="literal">tap:aio</code> devices in this example, the dom0 process is <code class="literal">tapdisk</code>. If we were using <code class="literal">phy:</code> devices, it'd be <code class="literal">[xvd &lt;domain id&gt; &lt;device specifier&gt;]</code>.</p><a id="I_programlisting7_d1e8087"/><pre class="programlisting"># ps aux | grep tapdisk
root      1054   0.5   0.0   13588   556   ?   Sl   05:45   0:10   tapdisk 
/dev/xen/tapctrlwrite1 /dev/xen/tapctrlread1
root      1172   0.6   0.0   13592   560   ?   Sl   05:45   0:10   tapdisk 
/dev/xen/tapctrlwrite2 /dev/xen/tapctrlread2</pre><p>Now we can <a id="idx-CHP-7-0556" class="indexterm"/><code class="literal">ionice</code> our domains. Note that the numbers of the <code class="literal">tapctrl</code> devices correspond to the order the domains were started in, not the domain ID.<a id="idx-CHP-7-0557" class="indexterm"/></p><a id="I_programlisting7_d1e8105"/><pre class="programlisting"># ionice -p 1054 -c 2 -n 7
# ionice -p 1172 -c 2 -n 0</pre><p>To test <code class="literal">ionice</code>, let's run a couple of <a id="idx-CHP-7-0558" class="indexterm"/>Bonnie++ processes and time them. (After Bonnie++ finishes, we <code class="literal">dd</code> a load file, just to make sure that conditions for the other domain remain unchanged.)</p><a id="I_programlisting7_d1e8119"/><pre class="programlisting">prio 7 domU tmp # /usr/bin/time -v  bonnie++  -u 1 &amp;&amp; dd if=/dev/urandom of=load
prio 0 domU tmp # /usr/bin/time -v  bonnie++  -u 1 &amp;&amp; dd if=/dev/urandom of=load</pre><p>In the end, according to the wall clock, the domU <a id="idx-CHP-7-0559" class="indexterm"/>with priority 0 took 3:32.33 to finish, while the priority 7 domU needed 5:07.98. As you can see, the <code class="literal">ionice</code> priorities provide an effective way to do proportional I/O allocation.</p><p>The best way to apply <code class="literal">ionice</code> is probably to look at CPU allocations and convert them into priority classes. Domains with the highest CPU allocation get priority 1, next highest priority 2, and so on. Processes in the dom0 should be ioniced as appropriate. This will ensure a reasonable priority, but not allow big domUs to take over the entirety of the I/O bandwidth.</p></div><div class="sect2" title="Backing Up DomUs"><div class="titlepage"><div><div><h2 class="title"><a id="backing_up_domus"/>Backing Up DomUs</h2></div></div></div><p>As a service provider, one rapidly learns that customers don't do their own backups. When a <a id="idx-CHP-7-0560" class="indexterm"/>disk fails (not <span class="emphasis"><em>if—when</em></span>), customers will expect you to have complete backups of their data, and they'll be very sad if you don't. So let's talk about backups.<a id="idx-CHP-7-0561" class="indexterm"/></p><p>Of course, you already have a good idea how to back up physical machines. There are two aspects to <a id="idx-CHP-7-0562" class="indexterm"/>backing up Xen domains: First, there's the domain's virtual disk, which we want to back up just as we would a real machine's disk. Second, there's the domain's running state, which can be saved and restored from the dom0. Ordinarily, our use of <span class="emphasis"><em>backup</em></span> refers purely to the disk, as it would with physical machines, but with the advantage that we can use domain snapshots to pause the domain long enough to get a clean disk image.</p><p>We use <code class="literal">xm save</code> and LVM snapshots to back up both the domain's storage and running state. LVM snapshots aren't a good way of implementing full copy-on-write because they handle the "out of snapshot space" case poorly, but they're excellent if you want to preserve a filesystem state long enough to make a consistent backup.</p><p>Our implementation copies the entire disk image <a id="idx-CHP-7-0563" class="indexterm"/>using either a plain <code class="literal">cp</code> (in the case of file-backed domUs) or <code class="literal">dd</code> (for <code class="literal">phy:</code> devices). This is because we very much want to avoid mounting a possibly unclean filesystem in the dom0, which can cause the entire machine to panic. Besides, if we do a raw device backup, domU administrators will be able to use filesystems (such as ZFS on an OpenSolaris domU) that the dom0 cannot read.</p><p>An appropriate script to do as we've described might be:<a id="idx-CHP-7-0564" class="indexterm"/></p><a id="I_programlisting7_d1e8192"/><pre class="programlisting">#!/usr/bin/perl
my @disks,@stores,@files,@lvs;

$domain=$ARGV[0];

my $destdir="/var/backup/xen/${domain}/";
system "mkdir -p $destdir";

open (FILE, "/etc/xen/$domain") ;
while (&lt;FILE&gt;) {
        if(m/^disk/) {
                s/.*\[\s+([^\]]+)\s*\].*/\1/;
                @disks = split(/[,]/);

                # discard elements without a :, since they can't be
                # backing store specifiers
                while($disks[$n]) {
                        $disks[$n] =~ s/['"]//g;
                        push(@stores,"$disks[$n]") if("$disks[$n]"=~ m/:/);
                        $n++;
                }
                $n=0;

                # split on : and take only the last field if the first
                # is a recognized device specifier.
                while($stores[$n]) {
                        @tmp = split(/:/, $stores[$n]);
                        if(($tmp[0] =~ m/file/i) || ($tmp[0] =~ m/tap/i)) {
                                push(@files, $tmp[$#tmp]);
                        }
                        elsif($tmp[0] =~ m/phy/i) {
                                push(@lvs, $tmp[$#tmp]);
                        }
                        $n++;
                }
        }
}
close FILE;

print "xm save $domain $destdir/${domain}.xmsave\n";
system ("xm save $domain $destdir/${domain}.xmsave");

foreach(@files) {
    print "copying $_";
        system("cp $_ ${destdir}") ;
}

foreach $lv (@lvs) {
        system("lvcreate --size 1024m --snapshot --name ${lv}_snap $lv");
}

system ("xm restore $destdir/${domain}.xmsave &amp;&amp; gzip $destdir/${domain}.xmsave");
foreach $lv (@lvs) {
    $lvfile=$lv;
    $lvfile=~s/\//_/g;
    print "<a id="idx-CHP-7-0565" class="indexterm"/>backing up $lv";
        system("dd if=${lv}_snap | gzip -c &gt; $destdir/${lvfile}.gz" ) ;
        system("lvremove ${lv}_snap" );
}</pre><p>Save it as, say, <span class="emphasis"><em>/usr/sbin/backup_domains.sh</em></span> and tell <code class="literal">cron</code> to execute the script at appropriate intervals.<a id="idx-CHP-7-0566" class="indexterm"/><a id="idx-CHP-7-0567" class="indexterm"/></p><p>This script works by <a id="idx-CHP-7-0568" class="indexterm"/>saving each domain, copying file-based storage, and snapshotting LVs. When that's accomplished, it restores the domain, backs up the save file, and backs up the snapshots via <code class="literal">dd</code>.</p><p>Note that users will see a brief hiccup in service while the domain is paused and snapshotted. We measured downtime of less than three minutes to get a consistent backup of a domain with a gigabyte of RAM—well within acceptable parameters for most applications. However, doing a bit-for-bit copy of an entire disk may also degrade performance somewhat.<sup>[<a id="CHP-7-FNOTE-4" href="#ftn.CHP-7-FNOTE-4" class="footnote">42</a>]</sup> We suggest doing backups at off-peak hours.</p><p>To view other <a id="idx-CHP-7-0569" class="indexterm"/>scripts in use at <a id="idx-CHP-7-0570" class="indexterm"/>prgmr.com, go to <a class="ulink" href="http://book.xen.prgmr.com/">http://book.xen.prgmr.com/</a>.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-7-FNOTE-4" href="#CHP-7-FNOTE-4" class="para">42</a>] </sup>Humorous understatement.</p></div></div></div>
<div class="sect1" title="Remote Access to the DomU"><div class="titlepage"><div><div><h1 class="title"><a id="remote_access_to_the_domu"/>Remote Access to the DomU</h1></div></div></div><p>The story on normal access for VPS users is deceptively simple: The Xen VM is exactly like a normal machine at the colocation facility. They can SSH into it (or, if you're providing Windows, <code class="literal">rdesktop</code>). However, when problems come up, the user is going to need some way of accessing the machine at a lower level, as if they were sitting at their VPS's console.<a id="idx-CHP-7-0571" class="indexterm"/></p><p>For that, we provide a console server that they can SSH into. The easiest thing to do is to use the dom0 as their console server and sharply limit their accounts.</p><div class="note" title="Note"><h3 class="title"><a id="note-29"/>Note</h3><p><span class="emphasis"><em>Analogously, we feel that any colocated machine should have a serial console attached to it</em></span>.<sup>[<a id="CHP-7-FNOTE-5" href="#ftn.CHP-7-FNOTE-5" class="footnote">43</a>]</sup> <span class="emphasis"><em>We discuss our reasoning and the specifics of using Xen with a serial console in <a class="xref" href="ch14.html" title="Chapter 14. TIPS">Chapter 14</a></em></span>.<a id="idx-CHP-7-0572" class="indexterm"/></p></div><div class="sect2" title="An Emulated Serial Console"><div class="titlepage"><div><div><h2 class="title"><a id="an_emulated_serial_console"/>An Emulated Serial Console</h2></div></div></div><p>Xen already provides basic serial console <a id="idx-CHP-7-0573" class="indexterm"/>functionality via <code class="literal">xm</code>. You can access a <a id="idx-CHP-7-0574" class="indexterm"/>guest's console by typing <code class="literal">xm console &lt;domain&gt;</code> within the dom0. Issue commands, then type ctrl-] to exit from the serial console when you're done.<a id="idx-CHP-7-0575" class="indexterm"/><a id="I_indexterm7_d1e8298" class="indexterm"/><a id="I_indexterm7_d1e8302" class="indexterm"/><a id="I_indexterm7_d1e8307" class="indexterm"/></p><p>The problem with this approach is that <code class="literal">xm</code> has to run from the dom0 with effective UID 0. While this is reasonable enough in an environment with trusted domU administrators, it's not a great idea when you're giving an account to anyone with $5. Dealing with untrusted domU admins, as in a VPS hosting situation, requires some additional work to limit access using <code class="literal">ssh</code> and <code class="literal">sudo</code>.</p><p>First, configure <code class="literal">sudo</code>. Edit <code class="literal">/etc/sudoers</code> and append, <a id="idx-CHP-7-0576" class="indexterm"/>for each user:</p><a id="I_programlisting7_d1e8337"/><pre class="programlisting">&lt;username&gt; ALL=NOPASSWD:/usr/sbin/xm console &lt;vm name&gt;</pre><p>Next, for each user, we create a <span class="emphasis"><em>~/.ssh/authorized_keys</em></span> file like this:</p><a id="I_programlisting7_d1e8344"/><pre class="programlisting">no-agent-forwarding,no-X11-forwarding,no-port-forwarding,command="sudo xm
console &lt;vm name&gt;" ssh-rsa &lt;key&gt; [comment]</pre><p>This line allows the user to log in with his key. Once he's logged in, <code class="literal">sshd</code> connects to the named domain console and automatically presents it to him, thus keeping domU administrators out of the dom0. Also, note the options that start with <code class="literal">no</code>. They're important. We're not in the business of providing shell accounts. This is purely a console server—we want people to use their domUs rather than the dom0 for standard SSH stuff. These settings will allow users to access their domains' consoles via SSH in a way that keeps their <a id="idx-CHP-7-0577" class="indexterm"/>access to the dom0 at a minimum.</p></div><div class="sect2" title="A Menu for the Users"><div class="titlepage"><div><div><h2 class="title"><a id="a_menu_for_the_users"/>A Menu for the Users</h2></div></div></div><p>Of course, letting each user access his console is really just the beginning. By changing the <code class="literal">command</code> field in <code class="literal">authorized_keys</code> to a custom script, we can provide a menu with a startling array of features!<a id="idx-CHP-7-0578" class="indexterm"/></p><p>Here's a sample script that we call <span class="emphasis"><em>xencontrol</em></span>. Put it somewhere in the filesystem—say <code class="literal">/usr/bin/xencontrol</code>—and then set the line in <code class="literal">authorized_keys</code> to call <code class="literal">xencontrol</code> rather than <code class="literal">xm console</code>.<a id="idx-CHP-7-0579" class="indexterm"/></p><a id="I_programlisting7_d1e8393"/><pre class="programlisting">#!/bin/bash
DOM="$1"
cat &lt;&lt; EOF
`sudo /usr/sbin/xm list $DOM`


Options for $DOM
1. console
2. create/start
3. shutdown
4. destroy/hard shutdown
5. reboot
6. exit
EOF
printf "&gt; "
read X
case "$X" in
*1*) sudo /usr/sbin/xm console "$DOM" ;;
*2*) sudo /usr/sbin/xm create -c "$DOM" ;;
*3*) sudo /usr/sbin/xm shutdown "$DOM" ;;
*4*) sudo /usr/sbin/xm destroy "$DOM" ;;
*5*) sudo /usr/sbin/xm reboot "$DOM" ;;
esac</pre><p>When the user logs in via SSH, the SSH daemon runs this <a id="idx-CHP-7-0580" class="indexterm"/>script in place <a id="idx-CHP-7-0581" class="indexterm"/>of the user's login shell (which we recommend setting to <code class="literal">/bin/false</code> or its equivalent on your platform). The script then echoes some status information, an informative message, and a list of options. When the user enters a number, it runs the appropriate command (which we've allowed the user to run by configuring <code class="literal">sudo</code>).</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-7-FNOTE-5" href="#CHP-7-FNOTE-5" class="para">43</a>] </sup>Our experience with other remote console tools has, overall, been unpleasant. Serial redirection systems work quite well. IP KVMs are barely preferable to toggling in the code on the front panel. On a good day.</p></div></div></div>
<div class="sect1" title="PyGRUB, a Bootloader for DomUs"><div class="titlepage"><div><div><h1 class="title"><a id="pygrub_a_bootloader_for_domus"/>PyGRUB, a Bootloader for DomUs</h1></div></div></div><p>Up until now, the configurations that we've described, by and large, have specified the domU's boot configuration in the config file, using the <code class="literal">kernel, ramdisk</code>, and <code class="literal">extra</code> lines. However, there is an alternative method, which specifies a <code class="literal">bootloader</code> line in the config file and in turn uses that to load a kernel from the domU's filesystem.<a id="idx-CHP-7-0582" class="indexterm"/><a id="idx-CHP-7-0583" class="indexterm"/></p><p>The bootloader most commonly used is <a id="idx-CHP-7-0584" class="indexterm"/>PyGRUB, or Python GRUB. The best way to explain PyGRUB is probably to step back and examine the program it's based on, GRUB, the GRand Unified Bootloader. GRUB itself is a traditional bootloader—a program that sits in a location on the hard drive where the BIOS can load and execute it, which then itself loads and executes a kernel.</p><p>PyGRUB, therefore, is like GRUB for a domU. The Xen domain builder usually loads an OS kernel directly from the dom0 filesystem when the virtual machine is started (therefore acting like a bootloader itself). Instead, it can load PyGRUB, which then acts as a bootloader and loads the kernel from the domU filesystem.<sup>[<a id="CHP-7-FNOTE-6" href="#ftn.CHP-7-FNOTE-6" class="footnote">44</a>]</sup></p><p>PyGRUB is useful because it allows a more perfect separation between the administrative duties of the dom0 and the domU. When virtualizing the data center, you want to hand off virtual hardware to the customer. PyGRUB more effectively virtualizes the hardware. In particular, this means the customer can change his own kernel without the intervention of the dom0 administrator.</p><div class="note" title="Note"><h3 class="title"><a id="note-30"/>Note</h3><p><span class="emphasis"><em>PyGRUB has been mentioned as a possible security risk because it reads an untrusted filesystem directly from the dom0. PV-GRUB (see "PV-GRUB: A Safer Alternative to PyGRUB?" on <a class="xref" href="ch07s05.html#pv-grub_a_safer_alternative_to_pygrub" title="PV-GRUB: A SAFER ALTERNATIVE TO PYGRUB?">PV-GRUB: A SAFER ALTERNATIVE TO PYGRUB?</a>), which loads a trusted paravirtualized kernel from the dom0 then uses that to load and jump to the domU kernel, should improve this situation</em></span>.<a id="idx-CHP-7-0585" class="indexterm"/></p></div><div class="sidebar"><a id="pv-grub_a_safer_alternative_to_pygrub"/><p class="title">PV-GRUB: A SAFER ALTERNATIVE TO PYGRUB?</p><p><a id="idx-CHP-7-0586" class="indexterm"/>PV-GRUB is an excellent <a id="idx-CHP-7-0587" class="indexterm"/>reason to upgrade to Xen 3.3. The problem with <a id="idx-CHP-7-0588" class="indexterm"/>PyGRUB is that while it's a good simulation <a id="idx-CHP-7-0589" class="indexterm"/>of a <a id="idx-CHP-7-0590" class="indexterm"/>bootloader, it has to mount the domU partition in the dom0, and it interacts with the domU filesystem. This has led to at least one remote-execution exploit. PV-GRUB avoids the problem by loading an executable that is, quite literally, a paravirtualized version of the GRUB bootloader, which then runs entirely within the domU.</p><p>This also has some other advantages. You can actually load the PV-GRUB binary from within the domU, meaning that you can load your first <em class="filename">menu.lst</em> from a read-only partition and have it fall through to a user partition, which then means that unlike my PyGRUB setup, users can never mess up their <em class="filename">menu.lst</em> to the point where they can't get into their rescue image.</p><p>Note that Xen creates a domain in either 32- or 64-bit mode, and it can't switch later on. This means that a 64-bit PV-GRUB can't load 32-bit Linux kernels, and vice versa.</p><p>Our PV-GRUB setup at <code class="literal">prgmr.com</code> starts with a normal <code class="literal">xm</code> config file, but with no bootloader and a <code class="literal">kernel= line</code> that points to PV-GRUB, instead of the domU kernel.</p><a id="I_programlisting7_d1e8512"/><pre class="programlisting">kernel = "/usr/lib/xen/boot/pv-grub-x86_64.gz"
extra = "(hd0,0)/boot/grub/menu.lst"
disk = ['phy:/dev/denmark/horatio,xvda,w','phy:/dev/denmark/rescue,xvde,r']</pre><p>Note that we call the architecture-specific binary for PV-GRUB. The 32-bit (PAE) version is <span class="emphasis"><em>pv-grub-x86_32</em></span>.</p><p>This is enough to load a <code class="literal">regular menu.lst</code>, but what about this indestructible rescue image of which I spoke? Here's how we do it on the new <code class="literal">prgmr.com</code> Xen 3.3 servers. In the <code class="literal">xm</code> config file:</p><a id="I_programlisting7_d1e8530"/><pre class="programlisting">kernel = "/usr/lib/xen/boot/pv-grub-x86_64.gz"
extra = "(hd1,0)/boot/grub/menu.lst"
disk = ['phy:/dev/denmark/horatio,xvda,w','phy:/dev/denmark/rescue,xvde,r']</pre><p>Then, in <span class="emphasis"><em>/boot/grub/menu.lst</em></span> on the rescue disk:</p><a id="I_programlisting7_d1e8537"/><pre class="programlisting">default=0
timeout=5

title Xen domain boot
        root (hd1)
        kernel /boot/pv-grub-x86_64.gz (hd0,0)/boot/grub/menu.lst

title CentOS-rescue (2.6.18-53.1.14.el5xen)
        root (hd1)
        kernel /boot/vmlinuz-2.6.18-53.1.14.el5xen ro root=LABEL=RESCUE
        initrd /boot/initrd-2.6.18-53.1.14.el5xen.img

title CentOS installer
        root (hd1)
        kernel /boot/centos-5.1-installer-vmlinuz
        initrd /boot/centos-5.1-installer-initrd.img

title NetBSD installer
        root (hd1)
        kernel  /boot/netbsd-INSTALL_XEN3_DOMU.gz</pre><p>The first entry is the normal boot, with 64-bit <a id="idx-CHP-7-0591" class="indexterm"/>PV-GRUB. The rest are various types of rescue and install boots. Note that we specify <code class="literal">(hd1)</code> <a id="idx-CHP-7-0592" class="indexterm"/>for the rescue entries; in this <a id="idx-CHP-7-0593" class="indexterm"/>case, the second disk is the rescue disk.</p><p>The normal boot loads PV-GRUB and the user's <span class="emphasis"><em>/boot/grub/menu.lst</em></span> from <code class="literal">(hd0,0)</code>. Our default user-editable <em class="filename">menu.lst</em> looks like this:</p><a id="I_programlisting7_d1e8574"/><pre class="programlisting">default=0
timeout=5

title CentOS (2.6.18-92.1.6.el5xen)
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.18-92.1.6.el5xen console=xvc0
root=LABEL=PRGMRDISK1 ro
        initrd /boot/initrd-2.6.18-92.1.6.el5xen.img</pre><p>PV-GRUB only runs on Xen 3.3 and above, and it seems that Red Hat has no plans to backport PV-GRUB to the version of Xen that is used by RHEL 5.<span class="emphasis"><em>x</em></span>.</p></div><div class="sect2" title="Making PyGRUB Work"><div class="titlepage"><div><div><h2 class="title"><a id="making_pygrub_work"/>Making PyGRUB Work</h2></div></div></div><p>The domain's filesystem will need to include a <span class="emphasis"><em>/boot</em></span> directory with the appropriate files, just like a regular GRUB setup. We usually make a separate block device for <span class="emphasis"><em>/boot</em></span>, which we present to the domU as the first disk entry in its config file.<a id="idx-CHP-7-0594" class="indexterm"/></p><p>To try PyGRUB, add a <code class="literal">bootloader=</code> line to the domU config file:<a id="idx-CHP-7-0595" class="indexterm"/></p><a id="I_programlisting7_d1e8603"/><pre class="programlisting">bootloader = "/usr/bin/pygrub"</pre><p>Of course, this being Xen, it may not be as simple as that. If you're <a id="idx-CHP-7-0596" class="indexterm"/>using Debian, make sure that you have <code class="literal">libgrub, e2fslibs-dev</code>, and <code class="literal">reiserfslibs-dev</code> installed. (Red Hat Enterprise Linux and related distros use PyGRUB with their default Xen setup, and they include the necessary libraries with the Xen packages.)</p><p>Even with these libraries installed, it may fail to work without some manual intervention. Older versions of PyGRUB expect the virtual disk to have a partition table rather than a raw filesystem. If you have trouble, this may be the culprit.</p><p>With modern versions of PyGRUB, it is unnecessary to have a partition table on the domU's virtual disk.</p><div class="sect3" title="Self-Support with PyGRUB"><div class="titlepage"><div><div><h3 class="title"><a id="self-support_with_pygrub"/>Self-Support with PyGRUB</h3></div></div></div><p>At prgmr.com, we give domU administrators the ability to repair and customize their own <a id="idx-CHP-7-0597" class="indexterm"/>systems, which also saves us a lot of effort installing and supporting different distros. To accomplish this, we use PyGRUB and see to it that every customer has a bootable read-only rescue image they can boot into if their OS install goes awry. The domain config file for a customer who doesn't want us to do <a id="idx-CHP-7-0598" class="indexterm"/>mirroring looks something like the following.<a id="idx-CHP-7-0599" class="indexterm"/></p><a id="I_programlisting7_d1e8641"/><pre class="programlisting">bootloader = "/usr/bin/pygrub"

memory = 512
name = "lsc"
vif = [ 'vifname=lsc,ip=38.99.2.47,mac=aa:00:00:50:20:2f,bridge=xenbr0' ]

disk = [
        'phy:/dev/verona/lsc_boot,sda,w',
        'phy:/dev/verona_left/lsc,sdb,w',
        'phy:/dev/verona_right/lsc,sdc,w',
        'file://var/images/centos_ro_rescue.img,sdd,r'
]</pre><p>Note that we're now exporting four disks to the virtual host: a <span class="emphasis"><em>/boot</em></span> partition on virtual sda, reserved <a id="idx-CHP-7-0600" class="indexterm"/>for <a id="idx-CHP-7-0601" class="indexterm"/>PyGRUB; two disks for user data, sdb and sdc; and a read-only CentOS install as sdd.</p><p>A sufficiently technical user, <a id="idx-CHP-7-0602" class="indexterm"/>with this setup and console access, needs almost no help from the dom0 administrator. He or she can change the operating system, boot a custom kernel, set up a software RAID, and boot the CentOS install to fix his setup if anything goes wrong.</p></div><div class="sect3" title="Setting Up the DomU for PyGRUB"><div class="titlepage"><div><div><h3 class="title"><a id="setting_up_the_domu_for_pygrub"/>Setting Up the DomU for PyGRUB</h3></div></div></div><p>The only other important bit to make this work is a valid <span class="emphasis"><em>/grub/menu.lst</em></span>, which looks remarkably like the <em class="filename">menu.lst</em> in a regular Linux install. Our default looks like this and is stored on the disk exported as sda:<a id="idx-CHP-7-0603" class="indexterm"/></p><a id="I_programlisting7_d1e8682"/><pre class="programlisting">default=0
timeout=15

title centos
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.18-53.1.6.el5xen console=xvc0 root=/dev/sdb ro
        initrd /boot/initrd-2.6.18-53.1.6.el5xen.XenU.img

title generic kernels
        root (hd0,0)
        kernel /boot/vmlinuz-2.6-xen root=/dev/sdb
        module /boot/initrd-2.6-xen

title rescue-disk
        root (hd0,0)
        kernel /boot/vmlinuz-2.6.18-53.1.6.el5xen console=xvc0 root=LABEL=RESCUE

ro
        initrd /boot/initrd-2.6.18-53.1.6.el5xen.XenU.img </pre><div class="note" title="Note"><h3 class="title"><a id="note-31"/>Note</h3><p>/boot/grub/menu.lst <span class="emphasis"><em>is frequently symlinked to either</em></span> /boot/grub/grub.conf <span class="emphasis"><em>or</em></span> /etc/grub.conf. /boot/grub/menu.lst <span class="emphasis"><em>is still the file that matters</em></span>.</p></div><p>As with native Linux, if you use a <a id="idx-CHP-7-0604" class="indexterm"/>separate partition for <span class="emphasis"><em>/boot</em></span>, you'll need to either make a symlink at the root of <span class="emphasis"><em>/boot</em></span> that points boot back to <code class="literal">.</code> or make your kernel names relative to <code class="literal">/boot</code>.</p><p>Here, the first and default entry is the CentOS distro kernel. The second entry is a generic Xen kernel, and the third choice is a read-only rescue image. Just like with native Linux, you can also specify devices by label rather than disk number.</p><div class="sidebar"><a id="working_with_partitions_on_virtual_disks"/><p class="title">WORKING WITH PARTITIONS ON VIRTUAL DISKS</p><p>In a standard configuration, partition 1 may be <span class="emphasis"><em>/boot</em></span>, with partition 2 as <span class="emphasis"><em>/</em></span>. In that case, partition 1 would have the configuration files and kernels in the same format as for normal GRUB.</p><p>It's straightforward to create these partitions on an LVM device using <code class="literal">fdisk</code>. Doing so for a file is a bit harder. First, attach the file to a loop, using <code class="literal">losetup</code>:</p><a id="I_programlisting7_d1e8737"/><pre class="programlisting"># losetup /dev/loop1 claudius.img</pre><p>Then create two partitions in the usual way, using your favorite partition editor:</p><a id="I_programlisting7_d1e8741"/><pre class="programlisting"># fdisk /dev/loop1</pre><p>Then, whether you're using an LVM device or loop file, use <code class="literal">kpartx</code> to create device nodes from the partition table in that device:</p><a id="I_programlisting7_d1e8748"/><pre class="programlisting"># kpartx -av /dev/loop1</pre><p>Device nodes will be created under <span class="emphasis"><em>/dev/mapper</em></span> in the format <span class="emphasis"><em>devnamep#</em></span>. Make a filesystem of your preferred type on the new partitions:</p><a id="I_programlisting7_d1e8758"/><pre class="programlisting"># mke2fs /dev/mapper/loop1p1
# mke2fs -j /dev/mapper/loop1p2

# mount /dev/mapper/loop1p2 /mnt
# mount /dev/mapper/loop1p1 /mnt/boot</pre><p>Copy your filesystem image into <span class="emphasis"><em>/mnt</em></span>, make sure valid GRUB support files are in <span class="emphasis"><em>/mnt/boot</em></span> (just like a regular GRUB setup), and you are done.</p></div></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-7-FNOTE-6" href="#CHP-7-FNOTE-6" class="para">44</a>] </sup>This is an oversimplification. What actually happens is that PyGRUB copies a kernel from the domU filesystem, puts it in <span class="emphasis"><em>/tmp</em></span>, and then writes an appropriate domain config so that the domain builder can do its job. But the distinction is usually unimportant, so we've opted to approach PyGRUB as the bootloader it pretends to be.</p></div></div></div>
<div class="sect1" title="Wrap-Up"><div class="titlepage"><div><div><h1 class="title"><a id="wrap-up"/>Wrap-Up</h1></div></div></div><p>This chapter discussed things that we've learned from our years of relying on Xen. Mostly, that relates to how to partition and allocate resources between independent, uncooperative virtual machines, with a particular slant toward VPS hosting. We've described why you might host VPSs on Xen; specific allocation issues for CPU, disk, memory, and network access; backup methods; and letting customers perform self-service with scripts and PyGRUB.</p><p>Note that there's some overlap between this chapter and some of the others. For example, we mention a bit about network configuration, but we go into far more detail on networking in <a class="xref" href="ch05.html" title="Chapter 5. NETWORKING">Chapter 5</a>, Networking. We describe <code class="literal">xm save</code> in the context of backups, but we talk a good deal more about it and how it relates to migration in <a class="xref" href="ch09.html" title="Chapter 9. XEN MIGRATION">Chapter 9</a>. Xen hosting's been a lot of fun. It hasn't made us rich, but it's presented a bunch of challenges and given us a chance to do some neat stuff.<a id="I_indexterm7_d1e8782" class="indexterm"/></p></div></body></html>