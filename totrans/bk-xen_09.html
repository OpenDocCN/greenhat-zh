<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;XEN MIGRATION"><div class="titlepage"><div><div><h1 class="title"><a id="xen_migration"/>Chapter 9. XEN MIGRATION</h1></div></div></div><div class="epigraph"><p><a id="idx-CHP-9-0682" class="indexterm"/><span class="emphasis"><em>In these situations the combination of virtualization and migration significantly improves manageability</em></span>.</p><div class="attribution"><span>—<span class="attribution">
<span class="author"><span class="firstname">Clark et al.,</span></span>
<em class="citation"><span class="emphasis"><em>"Live Migration of Virtual Machines"</em></span></em></span></span></div></div><div class="informalfigure"><div class="mediaobject"><a id="I_mediaobject9_d1e9890"/><img src="httpatomoreillycomsourcenostarchimages333191.png.jpg" alt="image with no caption"/></div></div><p>So let's review: Xen, in poetic terms, is an abstraction, built atop other abstractions, wrapped around still further abstractions. The goal of all this abstraction is to ensure that you, in your snug and secure domU, never even have to think about the messy, noisy, fallible hardware that actually sends electrical pulses out the network ports.</p><p>Of course, once in a while the hardware becomes, for reasons of its own, unable to run Xen. Perhaps it's overloaded, or maybe it needs some preventive maintenance. So long as you have advance warning, even this need not interrupt your virtual machine. One benefit of the sort of <span class="emphasis"><em>total hardware independence</em></span> offered by Xen is the ability to move an entire virtual machine instance to another machine and transparently resume operation—a process referred to as <span class="emphasis"><em>migration</em></span>.<a id="idx-CHP-9-0683" class="indexterm"/></p><p>Xen migration transfers the entire virtual machine—the in-memory state of the kernel, all <a id="idx-CHP-9-0684" class="indexterm"/>processes, and all application states. From the user's perspective, a live migration isn't even noticeable—at most, a few packets are dropped. This has the potential to make scheduled downtime a thing of the past. (Unscheduled downtime, like death and taxes, shows every sign of being inescapable.<sup>[<a id="CHP-9-FNOTE-1" href="#ftn.CHP-9-FNOTE-1" class="footnote">51</a>]</sup>)</p><p>Migration may be either <span class="emphasis"><em>live</em></span> or <span class="emphasis"><em>cold</em></span>,<sup>[<a id="CHP-9-FNOTE-2" href="#ftn.CHP-9-FNOTE-2" class="footnote">52</a>]</sup> with the distinction based on whether the instance is running at the time of migration. In a live migration, the domain continues to run during transfer, and downtime is kept to a minimum. In a cold migration, the virtual machine is paused, saved, and sent to another physical machine.</p><p>In either of these cases, the saved machine will expect its IP address and ARP cache to work on the new subnet. This is no surprise, considering that the in-memory state of the network stack persists unchanged. Attempts to initiate live migration between different layer 2 subnets will fail outright. Cold migration between different subnets will work, in that the VM will successfully transfer but will most likely need to have its networking reconfigured. We'll mention these characteristics again later in our discussion of live migration.</p><p>First, though, let's examine a basic, manual method for <a id="idx-CHP-9-0686" class="indexterm"/>moving a domain from one host to another.</p><div class="sect1" title="Migration for Troglodytes"><div class="titlepage"><div><div><h1 class="title"><a id="migration_for_troglodytes"/>Migration for Troglodytes</h1></div></div></div><p>The most basic, least elegant way to move a Xen instance from one physical machine to another is to stop it completely, move its backing storage, and re-create the domain on the remote host. This requires a full shutdown and reboot cycle for the VM. It isn't even "migration" in the formal Xen sense, but you may find it necessary if, for example, you need to change out the underlying block device or if certain machine-specific attributes change, for example, if you're moving a VM between different CPU architectures or from a machine that uses PAE to one that doesn't.<sup>[<a id="CHP-9-FNOTE-3" href="#ftn.CHP-9-FNOTE-3" class="footnote">53</a>]</sup></p><p>Begin by <a id="idx-CHP-9-0687" class="indexterm"/>shutting down the virtual machine normally, either from within the operating system or by doing an <code class="literal">xm shutdown</code> from the dom0. Copy its backing store, kernel image (if necessary), and config file over, and finally <code class="literal">xm create</code> the machine as usual on the new host.</p><p>It's primitive, but at least it's almost certain to work and doesn't require any sort of complex infrastructure. We mention it mostly for completeness; this is a way to move a Xen domain from one physical machine to another.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-9-FNOTE-1" href="#CHP-9-FNOTE-1" class="para">51</a>] </sup>Maybe not; see Project Kemari or Project Remus at <a class="ulink" href="http://www.osrg.net/kemari/">http://www.osrg.net/kemari/</a> and <a class="ulink" href="http://dsg.cs.ubc.ca/remus/">http://dsg.cs.ubc.ca/remus/</a> <a id="idx-CHP-9-0685" class="indexterm"/>for work being done on adding hardware redundancy to Xen.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-9-FNOTE-2" href="#CHP-9-FNOTE-2" class="para">52</a>] </sup>We also like the terms <span class="emphasis"><em>hot</em></span> and <span class="emphasis"><em>dead</em></span>, which are the less-commonly used parallels of the more common terms.</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-9-FNOTE-3" href="#CHP-9-FNOTE-3" class="para">53</a>] </sup>For example, NetBurst (Pentium 4 and friends) to Core (Core 2 et al.). Xen offers no ability to move a VM from, say, x86 to PPC.</p></div></div></div>
<div class="sect1" title="Migration with xm save and xm restore"><div class="titlepage"><div><div><h1 class="title"><a id="migration_with_xm_save_and_xm_restore"/>Migration with xm save and xm restore</h1></div></div></div><p>This "cowboy" method aside, all forms <a id="idx-CHP-9-0688" class="indexterm"/>of migration are based on the basic idea of saving the domain on one machine and restoring it on another. You can do this manually using the <code class="literal">xm save</code> and <code class="literal">xm restore</code> commands, simulating the automatic process.<a id="idx-CHP-9-0689" class="indexterm"/></p><p>The <a id="idx-CHP-9-0690" class="indexterm"/>Xen documentation likens the <code class="literal">xm save</code> and <code class="literal">restore</code> cycle to hibernation on a physical machine. When a machine hibernates, it enters a power-saving mode that saves the memory image to disk and physically powers off the machine. When the machine turns on again, the operating system loads the saved memory image from the disk and picks up where it left off. <code class="literal">xm save</code> behaves exactly the same way. Just like <a id="idx-CHP-9-0691" class="indexterm"/>with physical hibernation, the saved domain drops its network connections, takes some time to pause and resume, and consumes no CPU or memory until it is restored.</p><p>Even if you're not planning to do anything fancy involving migration, you may still find yourself saving machines when the physical Xen server reboots. Xen includes an init script to save domains automatically when the system shuts down and restore them on boot. To accommodate this, we suggest making sure that <span class="emphasis"><em>/var</em></span> is large enough to hold the complete contents of the server's memory (in addition to logs, DNS databases, etc.).</p><p>To save the machine, issue:</p><a id="I_programlisting9_d1e10034"/><pre class="programlisting"># xm save &lt;domain name or id&gt; &lt;savefile&gt;</pre><p>This command tells the domain to suspend itself; the domain releases its resources back to domain 0, detaches its interrupt handlers, and converts its physical memory mappings back to domain-virtual mappings (because the physical memory mappings will almost certainly change when the domain is restored).</p><div class="note" title="Note"><h3 class="title"><a id="note-39"/>Note</h3><p><span class="emphasis"><em>Those of you who maintain a constant burning focus on implementation will notice that this implies domU OS-level support for Xen. HVM save and restore—that is, when the guest can't be counted on to be Xen-aware—are done slightly differently. See <a class="xref" href="ch12.html" title="Chapter 12. HVM: BEYOND PARAVIRTUALIZATION">Chapter 12</a> for details</em></span>.</p></div><p>At this point, domain 0 takes over, stops the domU, and checkpoints the domain state to a file. During this process it makes sure that all memory page references are canonical (that is, domain virtual, because references to machine memory pages will almost certainly be invalid on restore). Then it writes the contents of pages to disk, reclaiming pages as it goes.</p><p>After this process is complete, the domain has stopped running. The entire contents of its memory are in a savefile approximately the size of its memory allocation, which you can restore at will. In the meantime, you can run other domains, reboot the physical machine, back up the domain's virtual disks, or do whatever else required you to take the domain offline in the first place.</p><div class="note" title="Note"><h3 class="title"><a id="note-40"/>Note</h3><p><span class="emphasis"><em>Although</em></span> <em class="replaceable"><code>xm save</code></em> <span class="emphasis"><em>ordinarily stops the domain while saving it, you can also invoke it with the</em></span> <em class="replaceable"><code>-c</code></em> <span class="emphasis"><em>option, for checkpoint. This tells</em></span> <em class="replaceable"><code>xm</code></em> <span class="emphasis"><em>to leave the domain running. It's a bit complex to set up, though, because you also need some way to snapshot the domain's storage during the save. This usually involves an external device migration script</em></span>.<a id="idx-CHP-9-0692" class="indexterm"/></p></div><p>When that's done, <a id="idx-CHP-9-0693" class="indexterm"/>restoring the domain is easy:</p><a id="I_programlisting9_d1e10087"/><pre class="programlisting"># <a id="idx-CHP-9-0694" class="indexterm"/>xm restore &lt;savefile&gt;</pre><p>Restoration operates much like saving in reverse; the hypervisor allocates memory for the domain, writes out pages from the savefile to the newly allocated memory, and translates shadow page table entries to point at the new physical addresses. When this is accomplished, the domain resumes execution, reinstates everything it removed when it suspended, and begins functioning as if nothing happened.</p><div class="note" title="Note"><h3 class="title"><a id="note-41"/>Note</h3><p><span class="emphasis"><em>The savefile remains intact; if something goes wrong with the restarted machine, you can restore the savefile and try again</em></span>.</p></div><p>This ability to save and restore on the local machine works as the backbone of the more complex forms of migration supported by <a id="idx-CHP-9-0695" class="indexterm"/>Xen.</p></div>
<div class="sect1" title="Cold Migration"><div class="titlepage"><div><div><h1 class="title"><a id="cold_migration"/>Cold Migration</h1></div></div></div><p>Before we get into Xen's automated migration, we'll give an outline of a manual <span class="emphasis"><em>cold migration</em></span> process that approximates the flow of live migration to get an idea of the steps involved.<a id="idx-CHP-9-0696" class="indexterm"/><a id="idx-CHP-9-0697" class="indexterm"/></p><p>In this case, migration begins by saving the domain. The administrator manually moves the save file and the domain's underlying storage over to the new machine and restores the domain state. Because the underlying block device is moved over manually, there's no need to have the same filesystem accessible from both machines, as would be necessary for live migration. All that matters is transporting the content of the Xen virtual disk.</p><p>Here are some steps to <a id="idx-CHP-9-0698" class="indexterm"/>cold migrate a Xen domain:</p><a id="I_programlisting9_d1e10135"/><pre class="programlisting"># xm save &lt;domain id&gt; &lt;savefile&gt;
# scp &lt;savefile&gt; &lt;target.domain.tld:/path/&gt;</pre><p>Perform the appropriate steps to copy the domain's storage to the target computer—<code class="literal">rsync, scp, dd</code> piped into <code class="literal">ssh</code>, whatever floats your boat. Whatever method you choose, ensure that it copies the disk in such a way that is bit-for-bit the same and has the same path on both physical machines. In particular, do not mount the domU filesystem on machine A and copy its files over to the new domU filesystem on machine B. This will cause the VM to crash upon restoration.</p><p>Finally, restart the domain on the new machine:</p><a id="I_programlisting9_d1e10147"/><pre class="programlisting"># xm restore &lt;savefile&gt;</pre><p>There's no need to copy the domain config file over to the new machine; the savefile contains all the configuration information necessary to start the machine. Conversely, this also means that you can't change the parameters of the machine between save and restore and expect that to have any effect at all.<sup>[<a id="CHP-9-FNOTE-4" href="#ftn.CHP-9-FNOTE-4" class="footnote">54</a>]</sup></p><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-9-FNOTE-4" href="#CHP-9-FNOTE-4" class="para">54</a>] </sup>To forestall the inevitable question, we did try using a hex editor on the savefile. The result was an immediate crash.</p></div></div></div>
<div class="sect1" title="Live Migration"><div class="titlepage"><div><div><h1 class="title"><a id="live_migration"/>Live Migration</h1></div></div></div><p><a id="idx-CHP-9-0699" class="indexterm"/>Cold migration has its place, but one of the absolute neatest features of Xen is the ability to move a domain from one physical machine to another transparently, that is, imperceptibly to the outside world. This feature is <span class="emphasis"><em>live migration</em></span>.<a id="idx-CHP-9-0700" class="indexterm"/></p><p>As with <a id="idx-CHP-9-0701" class="indexterm"/>cold migration, live migration transfers the domain's configuration as part of its state; it doesn't require the administrator to manually copy over a config file. Manual copying is, in fact, not required at all. Of course, you will still need the config file if you want to recreate the domain from scratch on the new machine.</p><p>Live migration has some extra prerequisites. It relies on the domain's storage being accessible from both machines and on the machines being on the same subnet. Finally, because the copy phase occurs automatically over the network, the machines must run a network service.</p><div class="sect2" title="How It Works"><div class="titlepage"><div><div><h2 class="title"><a id="how_it_works"/>How It Works</h2></div></div></div><p>We would really like to say that live migration works by magic. In reality, however, it works by the application of sufficiently advanced technology.</p><p>Live migration is based on the basic idea of <span class="emphasis"><em>save and restore</em></span> only in the most general sense. The machine doesn't hibernate until the very last phase of the migration, and it comes back out of its virtual hibernation almost immediately.</p><p>As shown in <a class="xref" href="ch09s04.html#overview_of_live_migration" title="Figure 9-1. Overview of live migration">Figure 9-1</a> Xen live migration begins by sending a request, or <span class="emphasis"><em>reservation</em></span>, to the target specifying the resources the migrating domain will need. If the target accepts the request, the source begins the <span class="emphasis"><em>iterative precopy</em></span> phase of migration. During this step, Xen copies pages of memory over a TCP connection to the destination host. While this is happening, pages that change are marked as dirty and then recopied. The machine iterates this until only very frequently changed pages remain, at which point it begins the <span class="emphasis"><em>stop and copy</em></span> phase. Now Xen stops the VM and copies over any pages that change too frequently to efficiently copy during the previous phase. In practice, our testing suggests that Xen usually reaches this point after four to eight iterations. Finally the VM starts executing on the new machine.<a id="idx-CHP-9-0702" class="indexterm"/><a id="idx-CHP-9-0703" class="indexterm"/><a id="idx-CHP-9-0704" class="indexterm"/></p><p>By default, Xen will iterate up to 29 times and stop if the number of dirty pages falls below a certain threshold. You can specify this threshold and the number of iterations at compile time, but the defaults should work fine.</p><div class="figure"><a id="overview_of_live_migration"/><div class="figure-contents"><div class="mediaobject"><a id="I_mediaobject9_d1e10220"/><img src="httpatomoreillycomsourcenostarchimages333231.png.jpg" alt="Overview of live migration"/></div></div><p class="title">Figure 9-1. Overview of live migration</p></div></div><div class="sect2" title="Making Xen Migration Work"><div class="titlepage"><div><div><h2 class="title"><a id="making_xen_migration_work"/>Making Xen Migration Work</h2></div></div></div><p>First, note that <a id="idx-CHP-9-0705" class="indexterm"/>migration won't work unless the domain is using some kind of network-accessible storage, as described later in this chapter. If you haven't got such a thing, set that up first and come back when it's done.</p><p>Second, <code class="literal">xend</code> has to be set up to listen for migration requests on both physical machines. Note that both machines need to listen; if only the target machine has the relocation <a id="idx-CHP-9-0706" class="indexterm"/>server running, the source machine won't be able to shut down its Xen instance at the correct time, and the restarted domain will reboot as if it hadn't shut down cleanly.<a id="idx-CHP-9-0707" class="indexterm"/></p><p>Enable the migration <a id="idx-CHP-9-0708" class="indexterm"/>server by uncommenting the following in <span class="emphasis"><em>/etc/xend-config.sxp</em></span>:<a id="idx-CHP-9-0709" class="indexterm"/></p><a id="I_programlisting9_d1e10262"/><pre class="programlisting">(xend-relocation-server yes)</pre><p>This will cause <code class="literal">xend</code> to listen for migration requests on port 8002, which can be changed with the <code class="literal">(xend-relocation-port)</code> directive. Note that this is somewhat of a security risk. You can mitigate this to some extent by adding lines like the following:</p><a id="I_programlisting9_d1e10272"/><pre class="programlisting">(xend-relocation-address 192.168.1.1)
(xend-relocation-hosts-allow '^localhost$' '^host.example.org$')</pre><p>The <code class="literal">xend-relocation-address</code> line confines <code class="literal">xend</code> to listen for migration requests on that address so that you can restrict migration to, for example, an internal subnet or a VPN. The second line specifies a list of hosts to allow migration from as a space-separated list of quoted regular expressions. Although the idea of migrating from the <code class="literal">localhost</code> seems odd, it does have some value for testing. Xen migration to and from <span class="emphasis"><em>other</em></span> hosts will operate fine without <code class="literal">localhost</code> in the allowed-hosts list, so feel free to remove it if desired.</p><p>On distributions that include a firewall, you'll have to open port 8002 (or another port that you've specified using the <code class="literal">xend-relocation-port</code> directive). Refer to your distro's documentation if necessary.</p><p>With <a id="idx-CHP-9-0710" class="indexterm"/>live migration, Xen can maintain <a id="idx-CHP-9-0711" class="indexterm"/>network connections while migrating so that clients don't have to reconnect. The domain, after migration, sends an unsolicited ARP (address request protocol) reply to advertise its new location. (Usually this will work. In some network configurations, depending on your switch configuration, it'll fail horribly. Test it first.) The migrating instance can only maintain its network connections if it's migrating to a machine on the same physical subnet because its IP address remains the same.</p><p>The commands are simple:</p><a id="I_programlisting9_d1e10313"/><pre class="programlisting"># xm migrate --live &lt;domain id&gt; &lt;destination machine&gt;</pre><p>The domain's name in <code class="literal">xm list</code> changes to <code class="literal">migrating-[domain]</code> while the VM copies itself over to the remote host. At this time it also shows up in the <code class="literal">xm list</code> output on the target machine. On our configuration, this copy and run phase took around 1 second per 10MB of domU memory, followed by about 6 seconds of service interruption.</p><div class="note" title="Note"><h3 class="title"><a id="note-42"/>Note</h3><p><span class="emphasis"><em>If you, for whatever reason, want the migration to take less total time (at the expense of greater downtime), you can eliminate the repeated incremental copies by simply removing the</em></span> <em class="replaceable"><code>--live</code></em> <span class="emphasis"><em>option</em></span>.</p></div><a id="I_programlisting9_d1e10337"/><pre class="programlisting"># xm migrate &lt;domain id&gt; &lt;destination machine&gt;</pre><p><span class="emphasis"><em>This automatically stops the domain, saves it as normal, sends it to the destination machine, and restores it. Just as with</em></span> <em class="replaceable"><code>--live</code></em>, <span class="emphasis"><em>the final product is a migrated domain</em></span>.</p><p>Here's a <a id="idx-CHP-9-0712" class="indexterm"/>domain list on the target machine while the migration is in process. Note that the memory usage goes up as the migrating domain transfers more data:</p><a id="I_programlisting9_d1e10357"/><pre class="programlisting">Name                                      ID Mem(MiB) VCPUs State   Time(s)
Domain-0                                   0     1024     8 r-----    169.2
orlando                                    3      307     0 -bp---      0.0</pre><p>About 30 seconds later, the domain's transferred a few hundred more MB:</p><a id="I_programlisting9_d1e10361"/><pre class="programlisting">Name                                      ID Mem(MiB) VCPUs State   Time(s)
Domain-0                                   0     1024     8 r-----    184.8
orlando                                    3      615     0 -bp---      0.0</pre><p>Another 30 seconds further on, the domain's completely transferred and running:</p><a id="I_programlisting9_d1e10365"/><pre class="programlisting">Name                                      ID Mem(MiB) VCPUs State   Time(s)
Domain-0                                   0     1024     8 r-----    216.0
orlando                                    3     1023     1 -b----      0.0</pre><p>We also pinged the domain as it was migrating. Note that response times go up dramatically while the domain moves its data:</p><a id="I_programlisting9_d1e10370"/><pre class="programlisting">PING  (69.12.128.195) 56(84) bytes of data.
64 bytes from 69.12.128.195: icmp_seq=1 ttl=56 time=15.8 ms
64 bytes from 69.12.128.195: icmp_seq=2 ttl=56 time=13.8 ms
64 bytes from 69.12.128.195: icmp_seq=3 ttl=56 time=53.0 ms
64 bytes from 69.12.128.195: icmp_seq=4 ttl=56 time=179 ms
64 bytes from 69.12.128.195: icmp_seq=5 ttl=56 time=155 ms
64 bytes from 69.12.128.195: icmp_seq=6 ttl=56 time=247 ms
64 bytes from 69.12.128.195: icmp_seq=7 ttl=56 time=239 ms</pre><p>After most of the domain's memory has been moved over, there's a brief hiccup as the domain stops, copies over its last few pages, and restarts on the destination host:</p><a id="I_programlisting9_d1e10374"/><pre class="programlisting">64 bytes from 69.12.128.195: icmp_seq=107 ttl=56 time=14.2 ms
64 bytes from 69.12.128.195: icmp_seq=108 ttl=56 time=13.0 ms
64 bytes from 69.12.128.195: icmp_seq=109 ttl=56 time=98.0 ms
64 bytes from 69.12.128.195: icmp_seq=110 ttl=56 time=15.4 ms
64 bytes from 69.12.128.195: icmp_seq=111 ttl=56 time=14.2 ms
--- 69.12.128.195 ping statistics ---
111 packets transmitted, 110 received, 0% packet loss, time 110197ms
rtt min/avg/max/mdev = 13.081/226.999/382.360/101.826 ms</pre><p>At this point the domain is completely migrated.</p><p>However, the migration tools don't make any guarantees that the migrated domain will actually run on the target machine. One common problem occurs when migrating from a newer CPU to an older one. Because instructions are enabled at boot time, it's quite possible for the migrated kernel to attempt to execute instructions that simply no longer exist.</p><p>For example, the <code class="literal">sfence</code> instruction is used to explicitly serialize out-<a id="idx-CHP-9-0713" class="indexterm"/>of-order memory writes; any writes issued before <code class="literal">sfence</code> must complete before writes after the fence. This instruction is part of SSE, so it isn't supported on all Xen-capable machines. A domain started on a machine that supports <code class="literal">sfence</code> will try to keep using it after migration, and it'll crash in short order. This may change in upcoming versions of Xen, but at present, all production Xen environments that we know of migrate only between homogeneous hardware.<a id="idx-CHP-9-0714" class="indexterm"/></p></div></div>
<div class="sect1" title="Migrating Storage"><div class="titlepage"><div><div><h1 class="title"><a id="migrating_storage"/>Migrating Storage</h1></div></div></div><p><a id="idx-CHP-9-0715" class="indexterm"/>Live migration only copies the RAM and processor state; ensuring that the migrated domain can access its disk is up to the administrator. As such, the storage issue boils down to a question of capabilities. The migrated domain will expect its disks to be exactly consistent and to retain the same device names on the new machine as on the old machine. In most cases, that means the domU, to be capable of migration, must pull its backing storage over the network. Two popular ways to attain this in the Xen world are <a id="idx-CHP-9-0716" class="indexterm"/>ATA over Ethernet (AoE), and iSCSI. We also discussed NFS in <a class="xref" href="ch04.html" title="Chapter 4. STORAGE WITH XEN">Chapter 4</a>. Finally, you could just throw a suitcase of money at NetApp.<a id="idx-CHP-9-0717" class="indexterm"/></p><p>There are a lot of options beyond these; you may also want to consider cLVM (with some kind of network storage enclosure) and DRBD.</p><p>With all of these storage methods, we'll discuss an approach that uses a storage server to export a block device to a dom0, which then makes the storage available to a domU.</p><p>Note that both iSCSI and AoE limit themselves to providing simple block devices. Neither allows multiple clients to share the same filesystem without filesystem-level support! This an important point. Attempts to export a single ext3 filesystem and run domUs out of file-backed VBDs on that filesystem will cause almost immediate corruption. Instead, configure your network storage technology to export a block device for each domU. However, the exported devices don't have to correspond to physical devices; we can as easily export files or LVM volumes.</p><div class="sect2" title="ATA over Ethernet"><div class="titlepage"><div><div><h2 class="title"><a id="ata_over_ethernet"/>ATA over Ethernet</h2></div></div></div><p>ATA over Ethernet is easy to set up, reasonably fast, and popular. It's not routable, but that doesn't really matter in the context of live migration because live migration always occurs within a layer 2 broadcast domain.<a id="idx-CHP-9-0718" class="indexterm"/></p><p>People use AoE to fill the same niche as a basic SAN setup: to make centralized storage available over the network. It exports block devices that can then be used like locally attached disks. For the purposes of this example, we'll export one block device via AoE for each domU.<a id="I_indexterm9_d1e10441" class="indexterm"/></p><p>Let's start by setting up the AoE server. This is the machine that exports disk devices to dom0s, which in their turn host domUs that rely on the devices for backing <a id="idx-CHP-9-0719" class="indexterm"/>storage. The first thing you'll need to do is make sure that you've got the kernel AoE driver, which is located in the kernel configuration at:</p><a id="I_programlisting9_d1e10454"/><pre class="programlisting">Device drivers ---&gt;
  Block Devices ---&gt;
    &lt;*&gt; <a id="idx-CHP-9-0720" class="indexterm"/>ATA over Ethernet support</pre><p>You can also make it a module (<span class="emphasis"><em>m</em></span>). If you go that route, load the module:</p><a id="I_programlisting9_d1e10467"/><pre class="programlisting"># modprobe aoe</pre><p>Either way, make sure that you can access the device nodes under <span class="emphasis"><em>/dev/etherd</em></span>. They should be created by udev. If they aren't, try installing the kernel source and running the <span class="emphasis"><em>Documentation/aoe/udev-install.sh</em></span> script that comes in the kernel source tree. This script will generate rules and place them in an appropriate location—in our case <span class="emphasis"><em>/etc/udev/rules.d/50-udev.rules</em></span>. You may need to tune these rules for your udev version. The configurations that we used on CentOS 5.3 were:</p><a id="I_programlisting9_d1e10480"/><pre class="programlisting">SUBSYSTEM=="aoe", KERNEL=="discover",   NAME="etherd/%k", GROUP="disk", MODE="0220"
SUBSYSTEM=="aoe", KERNEL=="err",        NAME="etherd/%k", GROUP="disk", MODE="0440"
SUBSYSTEM=="aoe", KERNEL=="interfaces", NAME="etherd/%k", GROUP="disk", MODE="0220"
SUBSYSTEM=="aoe", KERNEL=="revalidate", NAME="etherd/%k", GROUP="disk", MODE="0220"

# aoe block devices
KERNEL=="etherd*",       NAME="%k", GROUP="disk"</pre><p>AoE also requires some support software. The server package is called <a id="idx-CHP-9-0721" class="indexterm"/>vblade and can be obtained from <a class="ulink" href="http://aoetools.sourceforge.net/">http://aoetools.sourceforge.net/</a>. You'll also need the client tools aoetools on both the server and client machines, so make sure to get those.</p><p>First, run the <code class="literal">aoe-interfaces</code> command on the storage server to tell vblade what interfaces to export on:</p><a id="I_programlisting9_d1e10496"/><pre class="programlisting"># aoe-interfaces &lt;ifname&gt;</pre><p>vblade can export most forms of storage, including SCSI, MD, or LVM. Despite the name <a id="idx-CHP-9-0722" class="indexterm"/>ATA over Ethernet, it's not limited to <a id="idx-CHP-9-0723" class="indexterm"/>exporting ATA devices; it can export any seekable device file or any ordinary filesystem image. Just specify the filename on the command line. (This is yet another instance where UNIX's <span class="emphasis"><em>everything is a file</em></span> philosophy comes in handy.)</p><p>Although vblade has a configuration file, it's simple enough to specify the options on the command line. The syntax is:</p><a id="I_programlisting9_d1e10517"/><pre class="programlisting"># vblade &lt;shelf id&gt; &lt;slot id&gt; &lt;interface&gt; &lt;file to export&gt;</pre><p>So, for example, to export a file:</p><a id="I_programlisting9_d1e10521"/><pre class="programlisting"># dd if=/dev/zero of=/path/file.img bs=1024M count=1
# <a id="idx-CHP-9-0724" class="indexterm"/>vblade 0 0 &lt;ifname&gt; &lt;/path/file.img&gt; &amp;</pre><p>This exports <span class="emphasis"><em>/path/file.img</em></span> as <span class="emphasis"><em>/dev/etherd/e0.0</em></span>.</p><div class="note" title="Note"><h3 class="title"><a id="note-43"/>Note</h3><p><span class="emphasis"><em>For whatever reason, the new export is not visible from the server. The AoE maintainers note that this is not actually a bug because it was never a design goal</em></span>.</p></div><p>AoE may expect the device to have a partition table, or at least a valid partition signature. If necessary, you can partition it locally by making a partition that spans the entire disk:</p><a id="I_programlisting9_d1e10542"/><pre class="programlisting"># losetup /dev/loop0 test.img
# fdisk /dev/loop0</pre><p>When you've done that, make a filesystem and detach the loop:</p><a id="I_programlisting9_d1e10547"/><pre class="programlisting"># mkfs /dev/loop0
# losetup -d /dev/loop0</pre><p>Alternately, if you want multiple partitions on the device, <code class="literal">fdisk</code> the device and create multiple partitions as normal. The new partitions will show up on the client <a id="idx-CHP-9-0725" class="indexterm"/>with names like <span class="emphasis"><em>/dev/etherd/e0.0p1</em></span>. To access the devices from the AoE server, performing <code class="literal">kpartx -a</code> on an appropriately set up loop device should work.</p><p>Now that we've got a functional server, let's set up the client. Large chunks of the AoE client are implemented as a part of the kernel, so you'll need to make sure that AoE's included in the dom0 kernel just as with the <a id="idx-CHP-9-0726" class="indexterm"/>storage server. If it's a module, you'll mostly likely want to ensure it loads on boot. If you're using CentOS, you'll probably also need to fix your udev rules, again just as with the server.</p><p>Since we're using the dom0 to arbitrate the network storage, we don't need to include the AoE driver in the domU kernel. All Xen virtual disk devices are accessed via the domU <code class="literal">xenblk</code> driver, regardless of what technology they're using for storage.<sup>[<a id="CHP-9-FNOTE-5" href="#ftn.CHP-9-FNOTE-5" class="footnote">55</a>]</sup></p><p>Download <a id="idx-CHP-9-0727" class="indexterm"/>aoetools from your distro's package management system or <a id="idx-CHP-9-0728" class="indexterm"/><a class="ulink" href="http://aoetools.sourceforge.net/">http://aoetools.sourceforge.net/</a>. If necessary, build and install the package.</p><p>Once the aoetools package is installed, you can test the exported AoE device on the client by doing:</p><a id="I_programlisting9_d1e10597"/><pre class="programlisting"># aoe-discover
# aoe-stat
     e0.0         1.073GB   eth0 up
# mount /dev/etherd/e0.0 /mnt/aoe</pre><p>In this case, the device is 1GB (or thereabouts) in size, has been exported as slot 0 of shelf 0, and has been found on the client's eth0. If it mounts successfully, you're ready to go. You can unmount <span class="emphasis"><em>/mnt/aoe</em></span> and use <span class="emphasis"><em>/dev/etherd/e0.0</em></span> as an ordinary <code class="literal">phy:</code> device for domU storage. An appropriate domU config <code class="literal">disk=</code> line might be:</p><a id="I_programlisting9_d1e10613"/><pre class="programlisting">disk = [ phy:/dev/etherd/e0.0, xvda, w ]</pre><p>If you run into any problems, check <span class="emphasis"><em>/var/log/xen/xend.log</em></span>. The most common problems relate to the machine's inability to find devices—block devices or network devices. In that case, errors will show up in the log file. Make sure that the correct virtual disks and interfaces are configured.</p></div><div class="sect2" title="iSCSI"><div class="titlepage"><div><div><h2 class="title"><a id="iscsi"/>iSCSI</h2></div></div></div><p>AoE and <a id="idx-CHP-9-0729" class="indexterm"/>iSCSI share a lot of similarities from the administrator's perspective; they're both ways of exporting storage over a network without requiring special hardware. They both export block devices, rather than filesystems, meaning that only one machine can access an exported device at a time. ISCSI differs from AoE in that it's a routable protocol, based on TCP/IP. This makes it less efficient in both CPU and bandwidth, but more versatile, since iSCSI exports can traverse layer 2 networks.<a id="idx-CHP-9-0730" class="indexterm"/></p><p>iSCSI divides the world into <span class="emphasis"><em>targets</em></span> and <span class="emphasis"><em>initiators</em></span>. You might be more familiar with these as <span class="emphasis"><em>servers</em></span> and <span class="emphasis"><em>clients</em></span>, respectively. The servers function as targets for SCSI commands, which are initiated by the client machines. In most installations, the iSCSI targets will be dedicated devices, but if you need to set up an <a id="idx-CHP-9-0731" class="indexterm"/>iSCSI server for testing on a general-purpose server, here's how.<a id="idx-CHP-9-0732" class="indexterm"/></p><div class="sect3" title="Setting Up the iSCSI Server"><div class="titlepage"><div><div><h3 class="title"><a id="setting_up_the_iscsi_server"/>Setting Up the iSCSI Server</h3></div></div></div><p>For the target we recommend the <span class="emphasis"><em>iSCSI Enterprise Target</em></span> implementation (<a class="ulink" href="http://sourceforge.net/projects/iscsitarget/">http://sourceforge.net/projects/iscsitarget/</a>). Other software exists, but we're less familiar with it.<a id="idx-CHP-9-0733" class="indexterm"/></p><p>Your distro vendor most likely provides a package. On Debian it's iscsitarget. Red Hat and friends use the related <a id="idx-CHP-9-0734" class="indexterm"/>tgt package, which has somewhat different configuration. Although we don't cover the details of setting up tgt, there is an informative page at <a class="ulink" href="http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html">http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html</a>. For the rest of this section, we'll assume that you're using the iSCSI Enterprise Target.</p><p>If necessary, you can download and build the iSCSI target software manually. Download the target software from the website and save it somewhere appropriate (we dropped it onto our GNOME desktop for this example). Unpack it:<a id="I_indexterm9_d1e10683" class="indexterm"/><a id="I_indexterm9_d1e10688" class="indexterm"/><a id="I_indexterm9_d1e10693" class="indexterm"/></p><a id="I_programlisting9_d1e10698"/><pre class="programlisting"># tar xzvf Desktop/iscsitarget-0.4.16.tar.gz
# cd iscsitarget-0.4.16</pre><p>Most likely you'll be able to build all of the components—both the kernel module and userspace tools—via the usual <code class="literal">make</code> process. Ensure that you've installed the openSSL headers, probably as part of the openssl-devel package or similar:</p><a id="I_programlisting9_d1e10705"/><pre class="programlisting"># make
# <a id="idx-CHP-9-0735" class="indexterm"/>make install</pre><p><code class="literal">make install</code> will also copy the default config files into <span class="emphasis"><em>/etc</em></span>. Our next step is to edit them appropriately.</p><p>The main config file is <span class="emphasis"><em>/etc/ietd.conf</em></span>. It's liberally commented, and most of the values can safely be left at their defaults (for now). The bit that we're mostly concerned with is the Target section:</p><a id="I_programlisting9_d1e10723"/><pre class="programlisting">Target iqn.2001-04.com.prgmr:domU.orlando
       Lun 0 Path=/opt/xen/orlando.img,Type=fileio</pre><p>There are many other variables that we could tweak here, but the basic target definition is simple: the word <strong class="userinput"><code>Target</code></strong> followed by a conforming <span class="emphasis"><em>iSCSI Qualified Name</em></span> with a logical unit definition. Note the <code class="literal">Type=fileio</code>. In this example we're using plain files, but you'll most likely also want to use this value with whole disk exports and LVM volumes too.<a id="idx-CHP-9-0736" class="indexterm"/></p><p>The init script <span class="emphasis"><em>etc/iscsi_target</em></span> should have also been copied to the appropriate place. If you want <a id="idx-CHP-9-0737" class="indexterm"/>iSCSI to be enabled on boot, create appropriate start and kill links as well.<a id="idx-CHP-9-0738" class="indexterm"/></p><p>Now we can export our iSCSI devices:</p><a id="I_programlisting9_d1e10758"/><pre class="programlisting"># /etc/init.d/iscsi_target start</pre><p>To check that it's working:</p><a id="I_programlisting9_d1e10762"/><pre class="programlisting"># cat /proc/net/iet/volume
tid:1 name:iqn.2001-04.com.prgmr:domU.orlando
       lun:0 state:0 iotype:fileio iomode:wt path:/opt/xen/orlando</pre><p>You should see the export(s) that you've defined, along with some status information.</p></div><div class="sect3" title="iSCSI Client Setup"><div class="titlepage"><div><div><h3 class="title"><a id="iscsi_client_setup"/>iSCSI Client Setup</h3></div></div></div><p>For the initiator, a variety of clients exist. However, the best-supported package seems to be <a id="idx-CHP-9-0739" class="indexterm"/>Open-iSCSI, available at <a id="idx-CHP-9-0740" class="indexterm"/><a class="ulink" href="http://www.open-iscsi.org/">http://www.open-iscsi.org/</a>. Both Red Hat and Debian make a version available through their package manager, as iscsi-initiator-utils and open-iscsi, respectively. You can also download the package from the website and work through the very easy installation process.<a id="idx-CHP-9-0741" class="indexterm"/></p><p>When you have the iSCSI initiator installed, however you choose to do it, the next step is to say the appropriate incantations to instruct the machine to mount your iSCSI devices at boot.</p><p>The <a id="idx-CHP-9-0742" class="indexterm"/>iSCSI daemon, <code class="literal">iscsid</code>, uses a database to specify its devices. You can interact with this database with the <code class="literal">iscsiadm</code> command. <code class="literal">iscsiadm</code> also allows you to perform target discovery and login (here we've used the long option forms for clarity):<a id="idx-CHP-9-0743" class="indexterm"/><a id="idx-CHP-9-0744" class="indexterm"/><a id="idx-CHP-9-0745" class="indexterm"/></p><a id="I_programlisting9_d1e10816"/><pre class="programlisting"># iscsiadm --mode discovery --type sendtargets --portal 192.168.1.123
192.168.1.123:3260,1 iqn.2001-04.com.prgmr:domU.orlando</pre><p>Note that <span class="emphasis"><em>portal</em></span>, in iSCSI jargon, refers to the IP address via which the resource can be accessed. In this case it's the exporting host. <code class="literal">iscsiadm</code> tells us that there's one device being exported, <span class="emphasis"><em>iqn.2001-04.com.prgmr:domU.odin</em></span>. Now that we know about the node, we can update the <a id="idx-CHP-9-0746" class="indexterm"/>iSCSI database:</p><a id="I_programlisting9_d1e10833"/><pre class="programlisting"># iscsiadm -m node -T iqn.2001-04.com.prgmr:domU.orlando
-p 192.168.1.123:3260 -o update -n node.conn[0].startup -v automatic</pre><p>Here we use <code class="literal">iscsiadm</code> to update a node in the iSCSI database. We specify a target, a portal, and the operation we want to perform on the database node: <code class="literal">update</code>. We specify a node to update with the <code class="literal">-n</code> option and a new value with the <code class="literal">-v</code> option. Other operations we can perform via the <code class="literal">-o</code> option are <code class="literal">new</code>, <code class="literal">delete</code>, and <code class="literal">show</code>. See the Open-iSCSI documentation for more details.</p><p>Restart <code class="literal">iscsid</code> to propagate your changes. (This step may vary depending on your distro. Under Debian the script is <code class="literal">open-iscsi</code>; under Red Hat it's <code class="literal">iscsid</code>.)</p><a id="I_programlisting9_d1e10873"/><pre class="programlisting"># /etc/init.d/open-iscsi restart</pre><p>Note the new device in <code class="literal">dmesg</code>:</p><a id="I_programlisting9_d1e10881"/><pre class="programlisting">iscsi: registered transport (iser)
scsi3 : iSCSI Initiator over TCP/IP
Vendor: IET       Model: VIRTUAL-DISK      Rev: 0
Type:   Direct-Access                      ANSI SCSI revision: 04
SCSI device <a id="idx-CHP-9-0747" class="indexterm"/>sda: 8192000 512-byte hdwr sectors (4194 MB)
sda: Write Protect is off
sda: Mode Sense: 77 00 00 08
SCSI device sda: drive cache: write through
SCSI device sda: 8192000 512-byte hdwr sectors (4194 MB)</pre><p>Note that this is the first SCSI device on the dom0, and thus becomes <span class="emphasis"><em>/dev/sda</em></span>. Further iSCSI exports become <span class="emphasis"><em>sdb</em></span>, and so on. Of course, using local SCSI device nodes for network storage presents obvious management problems. We suggest mitigating this by using the devices under <span class="emphasis"><em>/dev/disk/by-path</em></span>. Here <span class="emphasis"><em>/dev/sda</em></span> becomes <span class="emphasis"><em>/dev/disk/by-path/ip-192.168.1.123:3260-iscsi-larry:domU.orlando</em></span>. Your device names, of course, will depend on the specifics of your <a id="idx-CHP-9-0748" class="indexterm"/>setup.</p><p>Now that you're equipped with the device, you can install a Xen instance on it, most likely with a <code class="literal">disk=</code> line similar to the following:</p><a id="I_programlisting9_d1e10918"/><pre class="programlisting">disk = [ 'phy:/dev/disk/by-path/ip-192.168.1.123:3260-iscsi-larry:domU.orlando ,xvda,rw' ]</pre><p>Since the domain is backed by shared iSCSI storage, you can then migrate the domain to any connected Xen dom0.</p></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-9-FNOTE-5" href="#CHP-9-FNOTE-5" class="para">55</a>] </sup>A natural extension would be to have the domU mount the network storage directly by including the driver and support software in the initrd. In that case, no local disk configuration would be necessary.</p></div></div></div>
<div class="sect1" title="Quo Peregrinatur Grex"><div class="titlepage"><div><div><h1 class="title"><a id="quo_peregrinatur_grex"/>Quo Peregrinatur Grex</h1></div></div></div><p>So that's migration. In this chapter we've described:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>How to manually move a domain from one host to another</p></li><li class="listitem"><p>Cold migration of a domain between hosts</p></li><li class="listitem"><p>Live migration between hosts on the same subnet</p></li><li class="listitem"><p>Shared storage for live migration</p></li></ul></div><p>Apply these suggestions, and find your manageability significantly improved!<a id="I_indexterm9_d1e10942" class="indexterm"/><a id="I_indexterm9_d1e10945" class="indexterm"/><a id="I_indexterm9_d1e10950" class="indexterm"/></p></div></body></html>