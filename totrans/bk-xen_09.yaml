- en: Chapter 9. XEN MIGRATION
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章。XEN 迁移
- en: '*In these situations the combination of virtualization and migration significantly
    improves manageability*.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这些情况下，虚拟化和迁移的结合显著提高了可管理性*。'
- en: — Clark et al., **"Live Migration of Virtual Machines"**![image with no caption](httpatomoreillycomsourcenostarchimages333191.png.jpg)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: — 克拉克等人，**《虚拟机的实时迁移》**![无标题图片](httpatomoreillycomsourcenostarchimages333191.png.jpg)
- en: 'So let''s review: Xen, in poetic terms, is an abstraction, built atop other
    abstractions, wrapped around still further abstractions. The goal of all this
    abstraction is to ensure that you, in your snug and secure domU, never even have
    to think about the messy, noisy, fallible hardware that actually sends electrical
    pulses out the network ports.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们回顾一下：用诗意的话说，Xen是一种抽象，建立在其他抽象之上，围绕更进一步的抽象。所有这些抽象的目标是确保你，在你的舒适和安全domU中，甚至不必考虑那些实际上发送电脉冲到网络端口的混乱、嘈杂、易出错硬件。
- en: Of course, once in a while the hardware becomes, for reasons of its own, unable
    to run Xen. Perhaps it's overloaded, or maybe it needs some preventive maintenance.
    So long as you have advance warning, even this need not interrupt your virtual
    machine. One benefit of the sort of *total hardware independence* offered by Xen
    is the ability to move an entire virtual machine instance to another machine and
    transparently resume operation—a process referred to as *migration*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，偶尔硬件会因自身原因无法运行Xen。可能是因为过载，或者可能需要一些预防性维护。只要你有提前警告，即使这种需求也不必打断你的虚拟机。Xen提供的这种*完全硬件独立性*的好处之一是将整个虚拟机实例移动到另一台机器上并透明地恢复操作——这个过程被称为*迁移*。
- en: Xen migration transfers the entire virtual machine—the in-memory state of the
    kernel, all processes, and all application states. From the user's perspective,
    a live migration isn't even noticeable—at most, a few packets are dropped. This
    has the potential to make scheduled downtime a thing of the past. (Unscheduled
    downtime, like death and taxes, shows every sign of being inescapable.^([[51](#ftn.CHP-9-FNOTE-1)]))
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Xen迁移将整个虚拟机——内核的内存状态、所有进程以及所有应用程序状态——转移到另一个位置。从用户的角度来看，实时迁移甚至不明显——最多只是丢失了几個数据包。这有可能使计划中的停机时间成为过去式。（非计划停机时间，就像死亡和税收一样，似乎是无法避免的。^([[51](#ftn.CHP-9-FNOTE-1)]))
- en: Migration may be either *live* or *cold*,^([[52](#ftn.CHP-9-FNOTE-2)]) with
    the distinction based on whether the instance is running at the time of migration.
    In a live migration, the domain continues to run during transfer, and downtime
    is kept to a minimum. In a cold migration, the virtual machine is paused, saved,
    and sent to another physical machine.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移可以是*实时*或*冷迁移*，^([[52](#ftn.CHP-9-FNOTE-2)]) 区别在于实例在迁移时是否正在运行。在实时迁移中，域在传输过程中继续运行，停机时间保持在最低。在冷迁移中，虚拟机被暂停、保存，并发送到另一台物理机器。
- en: In either of these cases, the saved machine will expect its IP address and ARP
    cache to work on the new subnet. This is no surprise, considering that the in-memory
    state of the network stack persists unchanged. Attempts to initiate live migration
    between different layer 2 subnets will fail outright. Cold migration between different
    subnets will work, in that the VM will successfully transfer but will most likely
    need to have its networking reconfigured. We'll mention these characteristics
    again later in our discussion of live migration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，保存的机器都期望其IP地址和ARP缓存在新子网上正常工作。考虑到网络堆栈的内存状态保持不变，这并不令人惊讶。尝试在不同层2子网之间启动实时迁移将直接失败。在不同子网之间进行冷迁移将工作，因为虚拟机将成功传输，但很可能需要重新配置其网络。我们将在讨论实时迁移时再次提及这些特性。
- en: First, though, let's examine a basic, manual method for moving a domain from
    one host to another.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们检查一种将域从一个主机移动到另一个主机的简单、手动方法。
- en: Migration for Troglodytes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移适用于穴居人
- en: The most basic, least elegant way to move a Xen instance from one physical machine
    to another is to stop it completely, move its backing storage, and re-create the
    domain on the remote host. This requires a full shutdown and reboot cycle for
    the VM. It isn't even "migration" in the formal Xen sense, but you may find it
    necessary if, for example, you need to change out the underlying block device
    or if certain machine-specific attributes change, for example, if you're moving
    a VM between different CPU architectures or from a machine that uses PAE to one
    that doesn't.^([[53](#ftn.CHP-9-FNOTE-3)])
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将Xen实例从一台物理机迁移到另一台物理机最基本、最不优雅的方法是完全停止它，移动其备份存储，并在远程主机上重新创建域。这需要虚拟机完全关闭和重启周期。这甚至不是正式Xen意义上的“迁移”，但如果你需要更换底层块设备或某些特定于机器的属性发生变化，例如，在将虚拟机在不同CPU架构之间迁移或在使用PAE的机器和不使用PAE的机器之间迁移时，你可能需要这样做.^([[53](#ftn.CHP-9-FNOTE-3)])
- en: Begin by shutting down the virtual machine normally, either from within the
    operating system or by doing an `xm shutdown` from the dom0\. Copy its backing
    store, kernel image (if necessary), and config file over, and finally `xm create`
    the machine as usual on the new host.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正常关闭虚拟机，无论是从操作系统内部还是通过在dom0上执行`xm shutdown`。复制其备份存储、内核镜像（如果需要）和配置文件，最后在新的主机上按常规使用`xm
    create`创建该机器。
- en: It's primitive, but at least it's almost certain to work and doesn't require
    any sort of complex infrastructure. We mention it mostly for completeness; this
    is a way to move a Xen domain from one physical machine to another.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这方法很简单，但至少它几乎肯定能行，而且不需要任何复杂的基础设施。我们主要提到它是为了完整性；这是一种将Xen域从一台物理机迁移到另一台物理机的方法。
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[51](#CHP-9-FNOTE-1)]) Maybe not; see Project Kemari or Project Remus at
    [http://www.osrg.net/kemari/](http://www.osrg.net/kemari/) and [http://dsg.cs.ubc.ca/remus/](http://dsg.cs.ubc.ca/remus/)
    for work being done on adding hardware redundancy to Xen.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[51](#CHP-9-FNOTE-1)]) 可能不是；请参阅[http://www.osrg.net/kemari/](http://www.osrg.net/kemari/)和[http://dsg.cs.ubc.ca/remus/](http://dsg.cs.ubc.ca/remus/)上的Project
    Kemari或Project Remus，了解正在进行的为Xen添加硬件冗余的工作。
- en: ^([[52](#CHP-9-FNOTE-2)]) We also like the terms *hot* and *dead*, which are
    the less-commonly used parallels of the more common terms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[52](#CHP-9-FNOTE-2)]) 我们还喜欢使用*hot*和*dead*这两个术语，它们是更常用术语的较少使用的对应词。
- en: ^([[53](#CHP-9-FNOTE-3)]) For example, NetBurst (Pentium 4 and friends) to Core
    (Core 2 et al.). Xen offers no ability to move a VM from, say, x86 to PPC.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[53](#CHP-9-FNOTE-3)]) 例如，从NetBurst（奔腾4及其同类）到Core（Core 2等）。Xen无法将虚拟机从x86迁移到PPC。
- en: Migration with xm save and xm restore
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用xm save和xm restore进行迁移
- en: This "cowboy" method aside, all forms of migration are based on the basic idea
    of saving the domain on one machine and restoring it on another. You can do this
    manually using the `xm save` and `xm restore` commands, simulating the automatic
    process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这种“牛仔”方法之外，所有形式的迁移都是基于在某一台机器上保存域并在另一台机器上恢复域的基本思想。你可以使用`xm save`和`xm restore`命令手动完成此操作，模拟自动过程。
- en: The Xen documentation likens the `xm save` and `restore` cycle to hibernation
    on a physical machine. When a machine hibernates, it enters a power-saving mode
    that saves the memory image to disk and physically powers off the machine. When
    the machine turns on again, the operating system loads the saved memory image
    from the disk and picks up where it left off. `xm save` behaves exactly the same
    way. Just like with physical hibernation, the saved domain drops its network connections,
    takes some time to pause and resume, and consumes no CPU or memory until it is
    restored.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Xen文档将`xm save`和`restore`周期比作物理机的休眠。当机器休眠时，它会进入节能模式，将内存镜像保存到磁盘，并物理关闭机器。当机器再次开启时，操作系统从磁盘加载保存的内存镜像，并从上次停止的地方继续。`xm
    save`的行为完全相同。就像物理休眠一样，保存的域断开网络连接，暂停和恢复需要一些时间，直到恢复之前不消耗CPU或内存。
- en: Even if you're not planning to do anything fancy involving migration, you may
    still find yourself saving machines when the physical Xen server reboots. Xen
    includes an init script to save domains automatically when the system shuts down
    and restore them on boot. To accommodate this, we suggest making sure that */var*
    is large enough to hold the complete contents of the server's memory (in addition
    to logs, DNS databases, etc.).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你并没有计划进行任何涉及迁移的复杂操作，你也可能会在物理Xen服务器重启时保存机器。Xen包括一个初始化脚本，在系统关闭时自动保存域，并在启动时恢复它们。为了适应这一点，我们建议确保*/var*足够大，可以容纳服务器内存的完整内容（包括日志、DNS数据库等）。
- en: 'To save the machine, issue:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存机器，请执行以下命令：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command tells the domain to suspend itself; the domain releases its resources
    back to domain 0, detaches its interrupt handlers, and converts its physical memory
    mappings back to domain-virtual mappings (because the physical memory mappings
    will almost certainly change when the domain is restored).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令指示域暂停自身；域将其资源释放回domain 0，断开其中断处理程序，并将其实际内存映射转换回域虚拟映射（因为当域恢复时，实际内存映射几乎肯定会发生变化）。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Those of you who maintain a constant burning focus on implementation will
    notice that this implies domU OS-level support for Xen. HVM save and restore—that
    is, when the guest can''t be counted on to be Xen-aware—are done slightly differently.
    See [Chapter 12](ch12.html "Chapter 12. HVM: BEYOND PARAVIRTUALIZATION") for details*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**那些始终关注实现细节的人会注意到，这意味着domU操作系统对Xen的支持。HVM保存和恢复——也就是说，当无法保证客户机是Xen感知的——执行方式略有不同。有关详细信息，请参阅[第12章](ch12.html
    "第12章。HVM：超越Para虚拟化")**。'
- en: At this point, domain 0 takes over, stops the domU, and checkpoints the domain
    state to a file. During this process it makes sure that all memory page references
    are canonical (that is, domain virtual, because references to machine memory pages
    will almost certainly be invalid on restore). Then it writes the contents of pages
    to disk, reclaiming pages as it goes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，domain 0接管，停止domU，并将域状态检查点到一个文件中。在这个过程中，它确保所有内存页面引用都是规范的（也就是说，域虚拟的，因为当域恢复时，对机器内存页面的引用几乎肯定是不有效的）。然后它将页面内容写入磁盘，在写入过程中回收页面。
- en: After this process is complete, the domain has stopped running. The entire contents
    of its memory are in a savefile approximately the size of its memory allocation,
    which you can restore at will. In the meantime, you can run other domains, reboot
    the physical machine, back up the domain's virtual disks, or do whatever else
    required you to take the domain offline in the first place.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程完成后，该域已停止运行。其内存的全部内容都保存在一个大小大约等于其内存分配的savefile中，您可以随时恢复。在此期间，您可以运行其他域，重启物理机器，备份域的虚拟磁盘，或执行任何其他需要您将域离线的操作。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Although* *`xm save`* *ordinarily stops the domain while saving it, you can
    also invoke it with the* *`-c`* *option, for checkpoint. This tells* *`xm`* *to
    leave the domain running. It''s a bit complex to set up, though, because you also
    need some way to snapshot the domain''s storage during the save. This usually
    involves an external device migration script*.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**尽管** **`xm save`** **通常在保存时停止域，但您也可以使用** **`-c`** **选项调用它，用于检查点。这告诉** **`xm`**
    **保持域运行。尽管如此，设置起来有些复杂，因为您还需要一种方法在保存期间快照域的存储。这通常涉及到一个外部设备迁移脚本**。'
- en: 'When that''s done, restoring the domain is easy:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当完成这些操作后，恢复域变得简单：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Restoration operates much like saving in reverse; the hypervisor allocates memory
    for the domain, writes out pages from the savefile to the newly allocated memory,
    and translates shadow page table entries to point at the new physical addresses.
    When this is accomplished, the domain resumes execution, reinstates everything
    it removed when it suspended, and begins functioning as if nothing happened.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复操作与保存相反；虚拟机管理程序为域分配内存，将保存文件中的页面写入到新分配的内存中，并将影子页面表条目转换为指向新物理地址。当完成这些操作后，域恢复执行，重新设置它在暂停时移除的所有内容，并开始像什么都没发生一样运行。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*The savefile remains intact; if something goes wrong with the restarted machine,
    you can restore the savefile and try again*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**保存文件保持完整；如果重启的机器出现错误，您可以恢复保存文件并再次尝试**。'
- en: This ability to save and restore on the local machine works as the backbone
    of the more complex forms of migration supported by Xen.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地机器上保存和恢复的能力是Xen支持的更复杂迁移形式的基础。
- en: Cold Migration
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冷迁移
- en: Before we get into Xen's automated migration, we'll give an outline of a manual
    *cold migration* process that approximates the flow of live migration to get an
    idea of the steps involved.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解Xen的自动迁移之前，我们将概述一个手动**冷迁移**过程，以了解涉及到的步骤，并近似模拟实时迁移的流程。
- en: In this case, migration begins by saving the domain. The administrator manually
    moves the save file and the domain's underlying storage over to the new machine
    and restores the domain state. Because the underlying block device is moved over
    manually, there's no need to have the same filesystem accessible from both machines,
    as would be necessary for live migration. All that matters is transporting the
    content of the Xen virtual disk.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，迁移开始于保存域。管理员手动移动保存文件和域的底层存储到新机器上，并恢复域状态。由于底层块设备是手动移动的，因此不需要两台机器都可以访问相同的文件系统，就像实时迁移那样。唯一重要的是传输Xen虚拟磁盘的内容。
- en: 'Here are some steps to cold migrate a Xen domain:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些将Xen域进行冷迁移的步骤：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Perform the appropriate steps to copy the domain's storage to the target computer—`rsync,
    scp, dd` piped into `ssh`, whatever floats your boat. Whatever method you choose,
    ensure that it copies the disk in such a way that is bit-for-bit the same and
    has the same path on both physical machines. In particular, do not mount the domU
    filesystem on machine A and copy its files over to the new domU filesystem on
    machine B. This will cause the VM to crash upon restoration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 执行适当的步骤将域的存储复制到目标计算机——`rsync, scp, dd` 通过 `ssh` 管道传输，无论您选择哪种方法。无论您选择哪种方法，确保它以位对位相同的方式复制磁盘，并且在两台物理机器上有相同的路径。特别是，不要在机器A上挂载domU文件系统，并将文件复制到机器B上的新domU文件系统。这将导致虚拟机在恢复时崩溃。
- en: 'Finally, restart the domain on the new machine:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在新机器上重启域：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There's no need to copy the domain config file over to the new machine; the
    savefile contains all the configuration information necessary to start the machine.
    Conversely, this also means that you can't change the parameters of the machine
    between save and restore and expect that to have any effect at all.^([[54](#ftn.CHP-9-FNOTE-4)])
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要将域名配置文件复制到新机器上；保存文件包含了启动机器所需的所有配置信息。相反，这也意味着您在保存和恢复之间无法更改机器的参数，并期望有任何效果。[^([54](#ftn.CHP-9-FNOTE-4))]
- en: '* * *'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[54](#CHP-9-FNOTE-4)]) To forestall the inevitable question, we did try using
    a hex editor on the savefile. The result was an immediate crash.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ^([54](#CHP-9-FNOTE-4))) 为了避免不可避免的问题，我们确实尝试使用十六进制编辑器在保存文件上操作。结果是立即崩溃。
- en: Live Migration
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时迁移
- en: Cold migration has its place, but one of the absolute neatest features of Xen
    is the ability to move a domain from one physical machine to another transparently,
    that is, imperceptibly to the outside world. This feature is *live migration*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 冷迁移有其位置，但Xen最干净利落的功能之一是能够将域从一个物理机透明地移动到另一个物理机，即对外部世界来说不可察觉。这个特性就是**实时迁移**。
- en: As with cold migration, live migration transfers the domain's configuration
    as part of its state; it doesn't require the administrator to manually copy over
    a config file. Manual copying is, in fact, not required at all. Of course, you
    will still need the config file if you want to recreate the domain from scratch
    on the new machine.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与冷迁移一样，实时迁移将域的配置作为其状态的一部分进行传输；它不需要管理员手动复制配置文件。实际上，根本不需要手动复制。当然，如果您想在新的机器上从头开始重新创建域，您仍然需要配置文件。
- en: Live migration has some extra prerequisites. It relies on the domain's storage
    being accessible from both machines and on the machines being on the same subnet.
    Finally, because the copy phase occurs automatically over the network, the machines
    must run a network service.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实时迁移有一些额外的先决条件。它依赖于域的存储可以从两台机器访问，并且机器位于同一子网中。最后，因为复制阶段是自动通过网络进行的，所以机器必须运行网络服务。
- en: How It Works
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: We would really like to say that live migration works by magic. In reality,
    however, it works by the application of sufficiently advanced technology.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真的很想说是通过魔法来实现实时迁移的。然而，实际上，它是通过应用足够先进的技术来实现的。
- en: Live migration is based on the basic idea of *save and restore* only in the
    most general sense. The machine doesn't hibernate until the very last phase of
    the migration, and it comes back out of its virtual hibernation almost immediately.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实时迁移只是在最一般的意义上基于“保存和恢复”的基本思想。机器直到迁移的最后阶段才休眠，并且几乎立即从虚拟休眠中恢复过来。
- en: As shown in [Figure 9-1](ch09s04.html#overview_of_live_migration "Figure 9-1. Overview
    of live migration") Xen live migration begins by sending a request, or *reservation*,
    to the target specifying the resources the migrating domain will need. If the
    target accepts the request, the source begins the *iterative precopy* phase of
    migration. During this step, Xen copies pages of memory over a TCP connection
    to the destination host. While this is happening, pages that change are marked
    as dirty and then recopied. The machine iterates this until only very frequently
    changed pages remain, at which point it begins the *stop and copy* phase. Now
    Xen stops the VM and copies over any pages that change too frequently to efficiently
    copy during the previous phase. In practice, our testing suggests that Xen usually
    reaches this point after four to eight iterations. Finally the VM starts executing
    on the new machine.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 9-1](ch09s04.html#overview_of_live_migration "图 9-1. 实时迁移概述") 所示，Xen 实时迁移首先向目标发送一个请求，或称为
    *预留*，指定迁移域所需的资源。如果目标接受请求，源开始迁移的 *迭代预复制* 阶段。在此步骤中，Xen 通过 TCP 连接将内存页复制到目标主机。在这个过程中，更改的页面被标记为脏，然后重新复制。机器迭代这个过程，直到只剩下非常频繁更改的页面，此时它开始
    *停止和复制* 阶段。现在 Xen 停止虚拟机，并复制那些在先前阶段更改过于频繁的页面，以有效地复制。实际上，我们的测试表明 Xen 通常在四到八次迭代后达到这一点。最后，虚拟机在新机器上开始执行。
- en: By default, Xen will iterate up to 29 times and stop if the number of dirty
    pages falls below a certain threshold. You can specify this threshold and the
    number of iterations at compile time, but the defaults should work fine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Xen 将迭代最多 29 次，如果脏页数低于某个阈值，则停止。您可以在编译时指定此阈值和迭代次数，但默认值应该可以正常工作。
- en: '![Overview of live migration](httpatomoreillycomsourcenostarchimages333231.png.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![实时迁移概述](httpatomoreillycomsourcenostarchimages333231.png.jpg)'
- en: Figure 9-1. Overview of live migration
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1. 实时迁移概述
- en: Making Xen Migration Work
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使 Xen 迁移工作
- en: First, note that migration won't work unless the domain is using some kind of
    network-accessible storage, as described later in this chapter. If you haven't
    got such a thing, set that up first and come back when it's done.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意，除非域名正在使用某种类型的网络可访问存储，否则迁移将不会工作，这一点在本章后面有描述。如果您没有这样的东西，请先设置好，完成后回来。
- en: Second, `xend` has to be set up to listen for migration requests on both physical
    machines. Note that both machines need to listen; if only the target machine has
    the relocation server running, the source machine won't be able to shut down its
    Xen instance at the correct time, and the restarted domain will reboot as if it
    hadn't shut down cleanly.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，`xend` 必须设置成在物理机器上监听迁移请求。请注意，两台机器都需要监听；如果只有目标机器运行了重定位服务器，源机器将无法在正确的时间关闭其
    Xen 实例，并且重启的域将像没有干净关闭一样重新启动。
- en: 'Enable the migration server by uncommenting the following in */etc/xend-config.sxp*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 */etc/xend-config.sxp* 中取消以下注释来启用迁移服务器：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will cause `xend` to listen for migration requests on port 8002, which
    can be changed with the `(xend-relocation-port)` directive. Note that this is
    somewhat of a security risk. You can mitigate this to some extent by adding lines
    like the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致 `xend` 在端口 8002 上监听迁移请求，这可以通过 `(xend-relocation-port)` 指令进行更改。请注意，这有一定的安全风险。您可以通过添加以下类似行在一定程度上减轻这种风险：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `xend-relocation-address` line confines `xend` to listen for migration requests
    on that address so that you can restrict migration to, for example, an internal
    subnet or a VPN. The second line specifies a list of hosts to allow migration
    from as a space-separated list of quoted regular expressions. Although the idea
    of migrating from the `localhost` seems odd, it does have some value for testing.
    Xen migration to and from *other* hosts will operate fine without `localhost`
    in the allowed-hosts list, so feel free to remove it if desired.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`xend-relocation-address` 行将 `xend` 限制在指定地址上监听迁移请求，这样您可以将迁移限制为例如内部子网或 VPN。第二行指定了一个允许迁移的主机列表，作为空格分隔的引号正则表达式列表。虽然从
    `localhost` 迁移的想法看起来有些奇怪，但它确实在测试中具有一定的价值。Xen 到 *其他* 主机的迁移在没有 `localhost` 在允许的主机列表中时也能正常工作，所以如果您想的话，可以随意删除它。'
- en: On distributions that include a firewall, you'll have to open port 8002 (or
    another port that you've specified using the `xend-relocation-port` directive).
    Refer to your distro's documentation if necessary.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含防火墙的分发版上，您必须打开端口 8002（或您使用 `xend-relocation-port` 指令指定的另一个端口）。如有必要，请参阅您分发版的文档。
- en: With live migration, Xen can maintain network connections while migrating so
    that clients don't have to reconnect. The domain, after migration, sends an unsolicited
    ARP (address request protocol) reply to advertise its new location. (Usually this
    will work. In some network configurations, depending on your switch configuration,
    it'll fail horribly. Test it first.) The migrating instance can only maintain
    its network connections if it's migrating to a machine on the same physical subnet
    because its IP address remains the same.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时迁移过程中，Xen可以在迁移的同时保持网络连接，这样客户端就不需要重新连接。迁移后的域名会发送一个未经请求的ARP（地址请求协议）回复来宣传其新位置。（通常这会成功。在某些网络配置中，这取决于您的交换机配置，可能会非常失败。请先测试。）迁移实例只有在迁移到同一物理子网上的机器时才能保持其网络连接，因为它的IP地址保持不变。
- en: 'The commands are simple:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 命令很简单：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The domain's name in `xm list` changes to `migrating-[domain]` while the VM
    copies itself over to the remote host. At this time it also shows up in the `xm
    list` output on the target machine. On our configuration, this copy and run phase
    took around 1 second per 10MB of domU memory, followed by about 6 seconds of service
    interruption.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟机将自己复制到远程主机时，`xm list`中的域名会变为`migrating-[domain]`。此时，它也会出现在目标机器的`xm list`输出中。在我们的配置中，这个复制和运行阶段每10MB
    domU内存大约需要1秒钟，然后大约有6秒钟的服务中断。
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*If you, for whatever reason, want the migration to take less total time (at
    the expense of greater downtime), you can eliminate the repeated incremental copies
    by simply removing the* *`--live`* *option*.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您出于任何原因希望迁移的总时间更短（以增加停机时间为代价），您可以通过简单地删除* *`--live`* *选项来消除重复的增量复制*。'
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*This automatically stops the domain, saves it as normal, sends it to the destination
    machine, and restores it. Just as with* *`--live`*, *the final product is a migrated
    domain*.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*这会自动停止域名，将其保存为正常状态，发送到目标机器，并恢复。就像* *`--live`* *一样，最终产品是一个迁移后的域名*。'
- en: 'Here''s a domain list on the target machine while the migration is in process.
    Note that the memory usage goes up as the migrating domain transfers more data:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移过程中，以下是目标机器上的域名列表。请注意，随着迁移域名传输更多数据，内存使用量会增加：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'About 30 seconds later, the domain''s transferred a few hundred more MB:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 大约30秒后，域名迁移了数百MB：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another 30 seconds further on, the domain''s completely transferred and running:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 再过30秒，域名已经完全迁移并运行：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We also pinged the domain as it was migrating. Note that response times go
    up dramatically while the domain moves its data:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在迁移过程中也对域名进行了ping测试。请注意，当域名移动数据时，响应时间会显著增加：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After most of the domain''s memory has been moved over, there''s a brief hiccup
    as the domain stops, copies over its last few pages, and restarts on the destination
    host:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在大部分域名内存迁移完成后，域名停止，复制最后几页，然后在目标主机上重新启动，这时会出现短暂的故障：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At this point the domain is completely migrated.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，域名已经完全迁移。
- en: However, the migration tools don't make any guarantees that the migrated domain
    will actually run on the target machine. One common problem occurs when migrating
    from a newer CPU to an older one. Because instructions are enabled at boot time,
    it's quite possible for the migrated kernel to attempt to execute instructions
    that simply no longer exist.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，迁移工具并不能保证迁移后的域名实际上能在目标机器上运行。当从较新的CPU迁移到较旧的CPU时，会出现一个常见问题。因为指令在启动时被启用，迁移后的内核尝试执行那些根本不存在的指令是完全可能的。
- en: For example, the `sfence` instruction is used to explicitly serialize out-of-order
    memory writes; any writes issued before `sfence` must complete before writes after
    the fence. This instruction is part of SSE, so it isn't supported on all Xen-capable
    machines. A domain started on a machine that supports `sfence` will try to keep
    using it after migration, and it'll crash in short order. This may change in upcoming
    versions of Xen, but at present, all production Xen environments that we know
    of migrate only between homogeneous hardware.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`sfence`指令用于显式序列化乱序内存写入；在`sfence`之前发出的任何写入必须在围栏之后的写入之前完成。这个指令是SSE的一部分，因此它并不支持所有支持Xen的机器。在一个支持`sfence`的机器上启动的域名在迁移后会尝试继续使用它，并且很快就会崩溃。这可能在Xen的后续版本中有所改变，但到目前为止，我们所知的所有生产Xen环境都只在同构硬件之间迁移。
- en: Migrating Storage
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移存储
- en: Live migration only copies the RAM and processor state; ensuring that the migrated
    domain can access its disk is up to the administrator. As such, the storage issue
    boils down to a question of capabilities. The migrated domain will expect its
    disks to be exactly consistent and to retain the same device names on the new
    machine as on the old machine. In most cases, that means the domU, to be capable
    of migration, must pull its backing storage over the network. Two popular ways
    to attain this in the Xen world are ATA over Ethernet (AoE), and iSCSI. We also
    discussed NFS in [Chapter 4](ch04.html "Chapter 4. STORAGE WITH XEN"). Finally,
    you could just throw a suitcase of money at NetApp.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实时迁移只复制RAM和处理器状态；确保迁移的域可以访问其磁盘取决于管理员。因此，存储问题归结为一个能力问题。迁移的域将期望其磁盘在新的机器上与旧的机器上完全一致，并且设备名称相同。在大多数情况下，这意味着domU要能够进行迁移，必须通过网络拉取其支持存储。在Xen世界中，有两种流行的方法可以实现这一点：通过以太网的ATA（AoE）和iSCSI。我们还在[第4章](ch04.html
    "第4章。XEN的存储")中讨论了NFS。最后，您也可以向NetApp投掷一箱钱。
- en: There are a lot of options beyond these; you may also want to consider cLVM
    (with some kind of network storage enclosure) and DRBD.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些选项之外，您还可以考虑使用cLVM（带有某种类型的网络存储封装）和DRBD。
- en: With all of these storage methods, we'll discuss an approach that uses a storage
    server to export a block device to a dom0, which then makes the storage available
    to a domU.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些存储方法中，我们将讨论一种使用存储服务器将块设备导出到dom0的方法，然后dom0将存储提供给domU。
- en: Note that both iSCSI and AoE limit themselves to providing simple block devices.
    Neither allows multiple clients to share the same filesystem without filesystem-level
    support! This an important point. Attempts to export a single ext3 filesystem
    and run domUs out of file-backed VBDs on that filesystem will cause almost immediate
    corruption. Instead, configure your network storage technology to export a block
    device for each domU. However, the exported devices don't have to correspond to
    physical devices; we can as easily export files or LVM volumes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，iSCSI和AoE都限制自己只提供简单的块设备。它们都不允许多个客户端在没有文件系统级别支持的情况下共享同一个文件系统！这是一个重要的观点。尝试导出单个ext3文件系统并在该文件系统上运行domUs，几乎会立即导致损坏。相反，配置您的网络存储技术为每个domU导出一个块设备。然而，导出的设备不必与物理设备相对应；我们同样可以导出文件或LVM卷。
- en: ATA over Ethernet
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过以太网的ATA
- en: ATA over Ethernet is easy to set up, reasonably fast, and popular. It's not
    routable, but that doesn't really matter in the context of live migration because
    live migration always occurs within a layer 2 broadcast domain.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以太网的ATA设置起来很简单，速度合理，并且很受欢迎。它不可路由，但在实时迁移的上下文中这并不重要，因为实时迁移总是在一个2层广播域内发生。
- en: 'People use AoE to fill the same niche as a basic SAN setup: to make centralized
    storage available over the network. It exports block devices that can then be
    used like locally attached disks. For the purposes of this example, we''ll export
    one block device via AoE for each domU.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 人们使用AoE来填补与基本SAN设置相同的空白：通过网络使集中式存储可用。它导出可以像本地连接的磁盘一样使用的块设备。为了本例的目的，我们将通过AoE为每个domU导出一个块设备。
- en: 'Let''s start by setting up the AoE server. This is the machine that exports
    disk devices to dom0s, which in their turn host domUs that rely on the devices
    for backing storage. The first thing you''ll need to do is make sure that you''ve
    got the kernel AoE driver, which is located in the kernel configuration at:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先设置AoE服务器。这是将磁盘设备导出到dom0s的机器，这些dom0s反过来托管依赖于这些设备的domUs。您首先需要做的是确保您有内核AoE驱动程序，该驱动程序位于内核配置中：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can also make it a module (*m*). If you go that route, load the module:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以将其作为一个模块（*m*）。如果您选择这条路，请加载该模块：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Either way, make sure that you can access the device nodes under */dev/etherd*.
    They should be created by udev. If they aren''t, try installing the kernel source
    and running the *Documentation/aoe/udev-install.sh* script that comes in the kernel
    source tree. This script will generate rules and place them in an appropriate
    location—in our case */etc/udev/rules.d/50-udev.rules*. You may need to tune these
    rules for your udev version. The configurations that we used on CentOS 5.3 were:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，请确保您可以访问 */dev/etherd* 下的设备节点。它们应由udev创建。如果不是这样，请尝试安装内核源代码并运行内核源代码树中提供的
    *Documentation/aoe/udev-install.sh* 脚本。此脚本将生成规则并将它们放置在适当的位置——在我们的案例中是 */etc/udev/rules.d/50-udev.rules*。您可能需要根据您的udev版本调整这些规则。我们在CentOS
    5.3上使用的配置是：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: AoE also requires some support software. The server package is called vblade
    and can be obtained from [http://aoetools.sourceforge.net/](http://aoetools.sourceforge.net/).
    You'll also need the client tools aoetools on both the server and client machines,
    so make sure to get those.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AoE还需要一些支持软件。服务器包名为vblade，可以从[http://aoetools.sourceforge.net/](http://aoetools.sourceforge.net/)获取。您还需要在服务器和客户端机器上安装客户端工具aoetools，所以请确保获取这些工具。
- en: 'First, run the `aoe-interfaces` command on the storage server to tell vblade
    what interfaces to export on:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在存储服务器上运行`aoe-interfaces`命令，告诉vblade要导出哪些接口：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: vblade can export most forms of storage, including SCSI, MD, or LVM. Despite
    the name ATA over Ethernet, it's not limited to exporting ATA devices; it can
    export any seekable device file or any ordinary filesystem image. Just specify
    the filename on the command line. (This is yet another instance where UNIX's *everything
    is a file* philosophy comes in handy.)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: vblade可以导出大多数存储形式，包括SCSI、MD或LVM。尽管名为以太网上的ATA，但它并不仅限于导出ATA设备；它可以导出任何可寻址的设备文件或任何普通文件系统镜像。只需在命令行上指定文件名即可。（这又是UNIX的“一切皆文件”哲学派上用场的一个例子。）
- en: 'Although vblade has a configuration file, it''s simple enough to specify the
    options on the command line. The syntax is:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然vblade有一个配置文件，但通过命令行指定选项很简单。语法是：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'So, for example, to export a file:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要导出文件：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This exports */path/file.img* as */dev/etherd/e0.0*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将*/path/file.img*导出为*/dev/etherd/e0.0*。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*For whatever reason, the new export is not visible from the server. The AoE
    maintainers note that this is not actually a bug because it was never a design
    goal*.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*无论什么原因，新的导出在服务器上不可见。AoE维护者指出，这实际上不是一个错误，因为它从未是设计目标*。'
- en: 'AoE may expect the device to have a partition table, or at least a valid partition
    signature. If necessary, you can partition it locally by making a partition that
    spans the entire disk:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: AoE可能期望设备有一个分区表，或者至少有一个有效的分区签名。如果需要，您可以通过创建跨越整个磁盘的分区来本地分区：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When you''ve done that, make a filesystem and detach the loop:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，创建一个文件系统并断开循环：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Alternately, if you want multiple partitions on the device, `fdisk` the device
    and create multiple partitions as normal. The new partitions will show up on the
    client with names like */dev/etherd/e0.0p1*. To access the devices from the AoE
    server, performing `kpartx -a` on an appropriately set up loop device should work.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您想在设备上创建多个分区，使用`fdisk`对设备进行分区，并创建多个分区，就像通常一样。新的分区将以类似*/dev/etherd/e0.0p1*的名称出现在客户端。要从AoE服务器访问设备，在适当配置的循环设备上执行`kpartx
    -a`应该可以工作。
- en: Now that we've got a functional server, let's set up the client. Large chunks
    of the AoE client are implemented as a part of the kernel, so you'll need to make
    sure that AoE's included in the dom0 kernel just as with the storage server. If
    it's a module, you'll mostly likely want to ensure it loads on boot. If you're
    using CentOS, you'll probably also need to fix your udev rules, again just as
    with the server.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个功能性的服务器，让我们设置客户端。AoE客户端的大部分功能都是作为内核的一部分实现的，所以您需要确保dom0内核中包含了AoE，就像存储服务器一样。如果是模块，您可能希望确保它在启动时加载。如果您使用CentOS，您可能还需要修复您的udev规则，就像服务器一样。
- en: Since we're using the dom0 to arbitrate the network storage, we don't need to
    include the AoE driver in the domU kernel. All Xen virtual disk devices are accessed
    via the domU `xenblk` driver, regardless of what technology they're using for
    storage.^([[55](#ftn.CHP-9-FNOTE-5)])
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用dom0来仲裁网络存储，因此不需要在domU内核中包含AoE驱动程序。所有Xen虚拟磁盘设备都通过domU的`xenblk`驱动程序访问，无论它们使用什么技术进行存储.^([[55](#ftn.CHP-9-FNOTE-5)])
- en: Download aoetools from your distro's package management system or [http://aoetools.sourceforge.net/](http://aoetools.sourceforge.net/).
    If necessary, build and install the package.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从您的发行版的包管理系统中下载aoetools或从[http://aoetools.sourceforge.net/](http://aoetools.sourceforge.net/)下载。如果需要，构建并安装该包。
- en: 'Once the aoetools package is installed, you can test the exported AoE device
    on the client by doing:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了aoetools包，您可以通过以下方式在客户端测试导出的AoE设备：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this case, the device is 1GB (or thereabouts) in size, has been exported
    as slot 0 of shelf 0, and has been found on the client''s eth0\. If it mounts
    successfully, you''re ready to go. You can unmount */mnt/aoe* and use */dev/etherd/e0.0*
    as an ordinary `phy:` device for domU storage. An appropriate domU config `disk=`
    line might be:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，设备大小为 1GB（或类似大小），已作为 0 号架 0 号槽位导出，并在客户端的 eth0 上找到。如果它成功挂载，您就可以开始了。您可以卸载
    */mnt/aoe* 并使用 */dev/etherd/e0.0* 作为普通 `phy:` 设备用于 domU 存储。适当的 domU 配置 `disk=`
    行可能如下：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you run into any problems, check */var/log/xen/xend.log*. The most common
    problems relate to the machine's inability to find devices—block devices or network
    devices. In that case, errors will show up in the log file. Make sure that the
    correct virtual disks and interfaces are configured.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到任何问题，请检查 */var/log/xen/xend.log*。最常见的问题与机器无法找到设备——块设备或网络设备有关。在这种情况下，错误将显示在日志文件中。确保已正确配置了正确的虚拟磁盘和接口。
- en: iSCSI
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: iSCSI
- en: AoE and iSCSI share a lot of similarities from the administrator's perspective;
    they're both ways of exporting storage over a network without requiring special
    hardware. They both export block devices, rather than filesystems, meaning that
    only one machine can access an exported device at a time. ISCSI differs from AoE
    in that it's a routable protocol, based on TCP/IP. This makes it less efficient
    in both CPU and bandwidth, but more versatile, since iSCSI exports can traverse
    layer 2 networks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: AoE 和 iSCSI 在管理员的角度上有很多相似之处；它们都是通过网络导出存储而不需要特殊硬件的方法。它们都导出块设备，而不是文件系统，这意味着一次只能有一台机器访问一个导出的设备。iSCSI
    与 AoE 的不同之处在于它是一个基于 TCP/IP 的可路由协议。这使得它在 CPU 和带宽方面效率较低，但更灵活，因为 iSCSI 导出可以穿越第 2
    层网络。
- en: iSCSI divides the world into *targets* and *initiators*. You might be more familiar
    with these as *servers* and *clients*, respectively. The servers function as targets
    for SCSI commands, which are initiated by the client machines. In most installations,
    the iSCSI targets will be dedicated devices, but if you need to set up an iSCSI
    server for testing on a general-purpose server, here's how.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI 将世界划分为 *目标* 和 *发起者*。您可能更熟悉它们作为 *服务器* 和 *客户端*。服务器作为 SCSI 命令的目标，这些命令由客户端机器发起。在大多数安装中，iSCSI
    目标将是专用设备，但如果您需要在通用服务器上为测试设置 iSCSI 服务器，以下是方法。
- en: Setting Up the iSCSI Server
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 iSCSI 服务器
- en: For the target we recommend the *iSCSI Enterprise Target* implementation ([http://sourceforge.net/projects/iscsitarget/](http://sourceforge.net/projects/iscsitarget/)).
    Other software exists, but we're less familiar with it.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标，我们推荐使用 *iSCSI 企业目标* 实现 ([http://sourceforge.net/projects/iscsitarget/](http://sourceforge.net/projects/iscsitarget/))。其他软件也存在，但我们对其不太熟悉。
- en: Your distro vendor most likely provides a package. On Debian it's iscsitarget.
    Red Hat and friends use the related tgt package, which has somewhat different
    configuration. Although we don't cover the details of setting up tgt, there is
    an informative page at [http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html](http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html).
    For the rest of this section, we'll assume that you're using the iSCSI Enterprise
    Target.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您的发行版供应商很可能提供了一个软件包。在 Debian 上是 iscsitarget。Red Hat 和其他使用相关的 tgt 软件包，其配置略有不同。尽管我们不涵盖设置
    tgt 的细节，但在 [http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html](http://www.cyberciti.biz/tips/howto-setup-linux-iscsi-target-sanwith-tgt.html)
    有一个信息丰富的页面。在本节的其余部分，我们假设您正在使用 iSCSI 企业目标。
- en: 'If necessary, you can download and build the iSCSI target software manually.
    Download the target software from the website and save it somewhere appropriate
    (we dropped it onto our GNOME desktop for this example). Unpack it:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以手动下载和构建 iSCSI 目标软件。从网站上下载目标软件并将其保存到适当的位置（我们在这个例子中将它拖到了我们的 GNOME 桌面上）。解压它：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Most likely you''ll be able to build all of the components—both the kernel
    module and userspace tools—via the usual `make` process. Ensure that you''ve installed
    the openSSL headers, probably as part of the openssl-devel package or similar:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能您可以通过常规的 `make` 进程构建所有组件——内核模块和用户空间工具。确保您已安装了 openSSL 头文件，可能是作为 openssl-devel
    软件包或类似软件包的一部分：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`make install` will also copy the default config files into */etc*. Our next
    step is to edit them appropriately.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`make install` 也会将默认配置文件复制到 */etc* 目录中。我们的下一步是适当地编辑它们。'
- en: 'The main config file is */etc/ietd.conf*. It''s liberally commented, and most
    of the values can safely be left at their defaults (for now). The bit that we''re
    mostly concerned with is the Target section:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 主要配置文件是 */etc/ietd.conf*。它有大量的注释，并且大多数值可以安全地保留在默认值（目前是这样）。我们主要关心的是目标部分：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'There are many other variables that we could tweak here, but the basic target
    definition is simple: the word **`Target`** followed by a conforming *iSCSI Qualified
    Name* with a logical unit definition. Note the `Type=fileio`. In this example
    we''re using plain files, but you''ll most likely also want to use this value
    with whole disk exports and LVM volumes too.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这里调整许多其他变量，但基本的目标定义很简单：单词 **`Target`** 后跟一个符合规范的 *iSCSI 合法名称* 以及一个逻辑单元定义。注意
    `Type=fileio`。在这个例子中，我们使用的是普通文件，但你很可能也想将此值用于整个磁盘导出和 LVM 卷。
- en: The init script *etc/iscsi_target* should have also been copied to the appropriate
    place. If you want iSCSI to be enabled on boot, create appropriate start and kill
    links as well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化脚本 *etc/iscsi_target* 也应该被复制到适当的位置。如果你想在启动时启用 iSCSI，还需要创建相应的启动和终止链接。
- en: 'Now we can export our iSCSI devices:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以导出我们的 iSCSI 设备：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To check that it''s working:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查它是否工作：
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You should see the export(s) that you've defined, along with some status information.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到你定义的导出（s），以及一些状态信息。
- en: iSCSI Client Setup
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: iSCSI 客户端设置
- en: For the initiator, a variety of clients exist. However, the best-supported package
    seems to be Open-iSCSI, available at [http://www.open-iscsi.org/](http://www.open-iscsi.org/).
    Both Red Hat and Debian make a version available through their package manager,
    as iscsi-initiator-utils and open-iscsi, respectively. You can also download the
    package from the website and work through the very easy installation process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于启动器，存在各种客户端。然而，似乎最好的支持包是 Open-iSCSI，可在 [http://www.open-iscsi.org/](http://www.open-iscsi.org/)
    找到。Red Hat 和 Debian 都通过它们的包管理器提供版本，分别是 iscsi-initiator-utils 和 open-iscsi。你也可以从网站上下载包，并完成非常简单的安装过程。
- en: When you have the iSCSI initiator installed, however you choose to do it, the
    next step is to say the appropriate incantations to instruct the machine to mount
    your iSCSI devices at boot.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当你安装了 iSCSI 启动器，无论你如何安装，下一步就是说出适当的咒语来指示机器在启动时挂载你的 iSCSI 设备。
- en: 'The iSCSI daemon, `iscsid`, uses a database to specify its devices. You can
    interact with this database with the `iscsiadm` command. `iscsiadm` also allows
    you to perform target discovery and login (here we''ve used the long option forms
    for clarity):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI 守护进程 `iscsid` 使用数据库来指定其设备。你可以使用 `iscsiadm` 命令与该数据库交互。`iscsiadm` 还允许你执行目标发现和登录（这里我们使用了长选项形式以提高清晰度）：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that *portal*, in iSCSI jargon, refers to the IP address via which the
    resource can be accessed. In this case it''s the exporting host. `iscsiadm` tells
    us that there''s one device being exported, *iqn.2001-04.com.prgmr:domU.odin*.
    Now that we know about the node, we can update the iSCSI database:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 iSCSI 术语中，*portal* 指的是可以通过它访问资源的 IP 地址。在这种情况下，它是导出主机。`iscsiadm` 告诉我们有一个设备正在导出，*iqn.2001-04.com.prgmr:domU.odin*。现在我们知道了节点，我们可以更新
    iSCSI 数据库：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here we use `iscsiadm` to update a node in the iSCSI database. We specify a
    target, a portal, and the operation we want to perform on the database node: `update`.
    We specify a node to update with the `-n` option and a new value with the `-v`
    option. Other operations we can perform via the `-o` option are `new`, `delete`,
    and `show`. See the Open-iSCSI documentation for more details.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用 `iscsiadm` 来更新 iSCSI 数据库中的一个节点。我们指定一个目标、一个 portal 以及我们想在数据库节点上执行的操作：`update`。我们使用
    `-n` 选项指定一个要更新的节点，并使用 `-v` 选项指定一个新值。我们还可以通过 `-o` 选项执行其他操作，如 `new`、`delete` 和 `show`。有关更多详细信息，请参阅
    Open-iSCSI 文档。
- en: Restart `iscsid` to propagate your changes. (This step may vary depending on
    your distro. Under Debian the script is `open-iscsi`; under Red Hat it's `iscsid`.)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动 `iscsid` 以传播你的更改。（此步骤可能因你的发行版而异。在 Debian 中脚本为 `open-iscsi`；在 Red Hat 中为
    `iscsid`。）
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note the new device in `dmesg`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `dmesg` 中的新设备：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that this is the first SCSI device on the dom0, and thus becomes */dev/sda*.
    Further iSCSI exports become *sdb*, and so on. Of course, using local SCSI device
    nodes for network storage presents obvious management problems. We suggest mitigating
    this by using the devices under */dev/disk/by-path*. Here */dev/sda* becomes */dev/disk/by-path/ip-192.168.1.123:3260-iscsi-larry:domU.orlando*.
    Your device names, of course, will depend on the specifics of your setup.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是 dom0 上的第一个 SCSI 设备，因此成为 */dev/sda*。进一步的 iSCSI 导出变为 *sdb*，依此类推。当然，使用本地
    SCSI 设备节点作为网络存储显然存在管理问题。我们建议通过使用 */dev/disk/by-path* 下的设备来减轻这个问题。在这里 */dev/sda*
    变为 */dev/disk/by-path/ip-192.168.1.123:3260-iscsi-larry:domU.orlando*。当然，你的设备名称将取决于你设置的特定细节。
- en: 'Now that you''re equipped with the device, you can install a Xen instance on
    it, most likely with a `disk=` line similar to the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经配备了设备，你可以在其上安装一个 Xen 实例，很可能有一个类似于以下 `disk=` 行：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Since the domain is backed by shared iSCSI storage, you can then migrate the
    domain to any connected Xen dom0.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于域由共享 iSCSI 存储支持，因此你可以将域迁移到任何连接的 Xen dom0。
- en: '* * *'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([[55](#CHP-9-FNOTE-5)]) A natural extension would be to have the domU mount
    the network storage directly by including the driver and support software in the
    initrd. In that case, no local disk configuration would be necessary.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ^([[55](#CHP-9-FNOTE-5)]) 一个自然的扩展是让 domU 通过在 initrd 中包含驱动程序和支持软件直接挂载网络存储。在这种情况下，不需要本地磁盘配置。
- en: Quo Peregrinatur Grex
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Quo Peregrinatur Grex
- en: 'So that''s migration. In this chapter we''ve described:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是迁移。在本章中，我们描述了：
- en: How to manually move a domain from one host to another
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何手动将域名从一个主机移动到另一个主机
- en: Cold migration of a domain between hosts
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机间域的冷迁移
- en: Live migration between hosts on the same subnet
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同一子网内主机的实时迁移
- en: Shared storage for live migration
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时迁移的共享存储
- en: Apply these suggestions, and find your manageability significantly improved!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些建议，你会发现你的可管理性显著提高！
