<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;PROFILING AND BENCHMARKING UNDER XEN"><div class="titlepage"><div><div><h1 class="title"><a id="profiling_and_benchmarking_under_xen"/>Chapter 10. PROFILING AND BENCHMARKING UNDER XEN</h1></div></div></div><div class="epigraph"><p><span class="emphasis"><em>Disraeli was pretty close: actually, there are Lies, Damn lies, Statistics, Benchmarks, and Delivery dates</em></span>.</p><div class="attribution"><span>—<span class="attribution"><span class="emphasis"><em>Anonymous, attributed to Usenet</em></span></span></span></div></div><div class="informalfigure"><div class="mediaobject"><a id="I_mediaobject10_d1e10967"/><img src="httpatomoreillycomsourcenostarchimages333191.png.jpg" alt="image with no caption"/></div></div><p>We've made a great fuss over how Xen, as a virtualization technology, offers better performance than competing technologies. However, when it comes to proofs and signs, we have been waving our hands and citing authorities. We apologize! In this chapter we will discuss how to measure Xen's performance for yourself, using a variety of tools.</p><p>We'll look closely at three general classes of performance monitoring, each of which you might use for a different reason. First, we have benchmarking Xen domU performance. If you are running a hosting service (or buying service from a hosting service), you need to see how the Xen image you are providing (or renting) stacks up to the competition. In this category, we have general-purpose <span class="emphasis"><em>synthetic benchmarks</em></span>.</p><p>Second, we want to be able to benchmark Xen versus other virtualization solutions (or bare hardware) <span class="emphasis"><em>for your workload</em></span> because Xen has both strengths and weaknesses compared to other virtualization packages. These <span class="emphasis"><em>application benchmarks</em></span> will help to determine whether Xen is the best match for your application.</p><p>Third, sometimes you have a performance problem in your Xen-related or kernel-related program, and you want to pinpoint the bits of code that are moving slowly. This category includes <span class="emphasis"><em>profiling tools</em></span>, such as OProfile. (Xen developers may also ask you for OProfile output when you ask about performance issues on the <span class="emphasis"><em>xen-devel</em></span> list.)</p><p>Although some of these techniques might come in handy while troubleshooting, we haven't really aimed our discussion here at solving problems—rather, we try to present an <a id="idx-CHP-10-0749" class="indexterm"/>overview of the tools for various forms of speed measurement. See <a class="xref" href="ch15.html" title="Chapter 15. TROUBLESHOOTING">Chapter 15</a> for more specific troubleshooting suggestions.</p><div class="sect1" title="A Benchmarking Overview"><div class="titlepage"><div><div><h1 class="title"><a id="a_benchmarking_overview"/>A Benchmarking Overview</h1></div></div></div><p>We've seen that the performance of a paravirtualized Xen domain running most workloads approximates that of the native machine. However, there are cases where this isn't true or where this fuzzy simulacrum of the truth isn't precise enough. In these cases, we move from prescientific assertion to direct experimentation—that is, using benchmarking tools and simulators to find actual, rather than theoretical, performance numbers.</p><p>As we're sure you know, generalized benchmarking is, if not a "hard problem,"<sup>[<a id="CHP-10-FNOTE-1" href="#ftn.CHP-10-FNOTE-1" class="footnote">56</a>]</sup> at least quite difficult. If your load is I/O bound, testing the CPU will tell you nothing you need to know. If your load is IPC-bound or blocking on certain threads, testing the disk and the CPU will tell you little. Ultimately, the best results come from benchmarks that use as close to real-world load as possible.</p><p>The very best way to test, for example, the performance of a server that serves an HTTP web application would be to sniff live <a id="idx-CHP-10-0750" class="indexterm"/>traffic hitting your current HTTP server, and then replay that data against the new server, speeding up or slowing down the replay to see if you have more or less capacity than before.</p><p>This, of course, is rather difficult both to do and to generalize. Most people go at least one step into "easier" and "more general." In the previous example, you might pick a particularly heavy page (or a random sampling of pages) and test the server with a generalized HTTP tester, such as Siege. This usually still gives you pretty good results, is a lot easier, and has fewer privacy concerns than running the aforementioned live data.</p><p>There are times, however, when a general benchmark, for all its inadequacies, is the best tool. For example, if you are trying to compare two virtual private server providers, a standard, generalized test might be more readily available than a real-world, specific test. Let's start by examining a few of the synthetic benchmarks that we've used.</p><div class="sect2" title="UnixBench"><div class="titlepage"><div><div><h2 class="title"><a id="unixbench"/>UnixBench</h2></div></div></div><p>One classic <a id="idx-CHP-10-0751" class="indexterm"/>benchmarking tool is the public domain <a id="idx-CHP-10-0752" class="indexterm"/>UnixBench released by <span class="emphasis"><em>BYTE</em></span> magazine in 1990, available from <a class="ulink" href="http://www.tux.org/pub/tux/niemi/unixbench/">http://www.tux.org/pub/tux/niemi/unixbench/</a><a id="idx-CHP-10-0753" class="indexterm"/>. The tool was last updated in 1999, so it is rather old. However, it seems to be quite popular for benchmarking <a id="idx-CHP-10-0754" class="indexterm"/>VPS providers—by comparing one provider's UnixBench number to another, you can get a rough idea of the capacity of VM they're providing.</p><p>UnixBench is easy to install—download the source, untar it, build it, and run it.</p><a id="I_programlisting10_d1e11059"/><pre class="programlisting"># tar zxvf unixbench-4.1.0.tgz
# cd unixbench-4.1.0
# make
# ./Run</pre><p>(That last command is a literal "Run"—it's a script that cycles through the various tests, in order, and outputs results.)</p><p>You may get some warnings, or even errors, about the <code class="literal">-fforce-mem</code> option that UnixBench uses, depending on your compiler version. If you edit the Makefile to remove all instances of <code class="literal">-fforce-mem</code>, UnixBench should build successfully.</p><p>We recommend benchmarking the Xen instance in single-user mode if possible. Here's some example output:</p><a id="I_programlisting10_d1e11073"/><pre class="programlisting">                INDEX VALUES           
TEST                                        BASELINE     RESULT      INDEX

Dhrystone 2 using register variables        116700.0  1988287.6      170.4
Double-Precision Whetstone                      55.0      641.4      116.6
Execl Throughput                                43.0     1619.6      376.7
File Copy 1024 bufsize 2000 maxblocks         3960.0   169784.0      428.7
File Copy 256 bufsize 500 maxblocks           1655.0    53117.0      320.9
File Copy 4096 bufsize 8000 maxblocks         5800.0   397207.0      684.8
Pipe Throughput                              12440.0   233517.3      187.7
Pipe-based Context Switching                  4000.0    75988.8      190.0
Process Creation                               126.0     6241.4      495.3
Shell Scripts (8 concurrent)                     6.0      173.6      289.3
System Call Overhead                         15000.0   184753.6      123.2
                                                                 =========
     FINAL SCORE...............................     264.5</pre><p>Armed with a UnixBench number, you at least have some basis for comparison between different VPS providers. It's not going to tell you much about the specific performance you're going to get, but it has the advantage that it is a widely published, readily available benchmark.</p><p>Other tools, such as netperf and Bonnie++, can give you more detailed performance information.</p></div><div class="sect2" title="Analyzing Network Performance"><div class="titlepage"><div><div><h2 class="title"><a id="analyzing_network_performance"/>Analyzing Network Performance</h2></div></div></div><p>One popular tool for measuring low-level <a id="idx-CHP-10-0755" class="indexterm"/>network performance is <a id="idx-CHP-10-0756" class="indexterm"/>netperf. This tool supports a variety of <a id="idx-CHP-10-0757" class="indexterm"/>performance measurements, with a focus on measuring the efficiency of the network implementation. It's also been used in Xen-related papers. For one example, see "The Price of Safety: Evaluating IOMMU Performance" by Muli <a id="idx-CHP-10-0758" class="indexterm"/>Ben-Yehuda et al.<sup>[<a id="CHP-10-FNOTE-2" href="#ftn.CHP-10-FNOTE-2" class="footnote">57</a>]</sup></p><p>First, download netperf from <a class="ulink" href="http://netperf.org/netperf/DownloadNetperf.html">http://netperf.org/netperf/DownloadNetperf.html</a>. We picked up version 2.4.4.</p><a id="I_programlisting10_d1e11109"/><pre class="programlisting"># wget ftp://ftp.netperf.org/netperf/netperf-2.4.4.tar.bz2</pre><p>Untar it and enter the netperf directory.</p><a id="I_programlisting10_d1e11113"/><pre class="programlisting"># tar xjvf netperf-2.4.4.tar.bz2
# cd netperf-2.4.</pre><p>Configure, build, and install netperf. (Note that these directions are a bit at variance with the documentation; the documentation claims that <span class="emphasis"><em>/opt/netperf</em></span> is the hard-coded install prefix, whereas it seems to install in <span class="emphasis"><em>/usr/local</em></span> for me. Also, the manual seems to predate netperf's use of Autoconf.)</p><a id="I_programlisting10_d1e11123"/><pre class="programlisting"># ./configure
# make
# su
# make install</pre><p>netperf works by running the client, <code class="literal">netperf</code>, on the machine being benchmarked. <code class="literal">netperf</code> connects to a <code class="literal">netserver</code> daemon and tests the rate at which it can send and receive data. So, to use <code class="literal">netperf</code>, we first need to set up <code class="literal">netserver</code>.</p><p>In the standard service configuration, <code class="literal">netserver</code> would run under <code class="literal">inetd</code>; however, <code class="literal">inetd</code> is obsolete. Many distros don't even include it by default. Besides, you probably don't want to leave the benchmark server running all the time. Instead of configuring <code class="literal">inetd</code>, therefore, run <code class="literal">netserver</code> in standalone mode:</p><a id="I_programlisting10_d1e11159"/><pre class="programlisting"># /usr/local/bin/netserver
Starting netserver at port 12865
Starting netserver at hostname 0.0.0.0 port 12865 and family AF_UNSPEC</pre><p>Now we can run the <code class="literal">netperf</code> client with no arguments to perform a 10-second test with the local daemon.</p><a id="I_programlisting10_d1e11167"/><pre class="programlisting"># netperf
TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to localhost (127.0.0.1)
port 0 AF_INET
Recv   Send    Send
Socket Socket  Message  Elapsed
Size   Size    Size     Time     Throughput
bytes  bytes   bytes    secs.    10^6bits/sec
87380  16384   16384    10.01    10516.33</pre><p>Okay, looks good. Now we'll test from the dom0 to this domU. To do that, we install the <a id="idx-CHP-10-0759" class="indexterm"/>netperf binaries as described previously and run <code class="literal">netperf</code> with the <code class="literal">-H</code> option to specify a target host (in this case, .74 is the domU we're <a id="idx-CHP-10-0760" class="indexterm"/>testing against):</p><a id="I_programlisting10_d1e11187"/><pre class="programlisting"># netperf -H 216.218.223.74,ipv4
TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.0.2.74
(192.0.2.74) port 0 AF_INET
Recv   Send    Send
Socket Socket  Message  Elapsed
Size   Size    Size     Time     Throughput
bytes  bytes   bytes    secs.    10^6bits/sec
 87380  16384  16384    10.00     638.59</pre><p>Cool. Not as fast, obviously, but we expected that. Now from another physical machine to our test domU:</p><a id="I_programlisting10_d1e11191"/><pre class="programlisting"># netperf -H  192.0.2.66
TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.0.2.66
(192.0.2.66) port 0 AF_INET
Recv   Send    Send
Socket Socket  Message  Elapsed
Size   Size    Size     Time     Throughput
bytes  bytes   bytes    secs.    10^6bits/sec
 87380  16384  16384    10.25      87.72</pre><p>Ouch. Well, so how much of that is Xen, and how much is the <a id="idx-CHP-10-0761" class="indexterm"/>network we're going through? To find out, we'll run the <code class="literal">netserver</code> daemon on the dom0 hosting the test domU and connect to that:<a id="idx-CHP-10-0762" class="indexterm"/></p><a id="I_programlisting10_d1e11205"/><pre class="programlisting"># netperf -H  192.0.2.74
TCP STREAM TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.0.2.74
(192.0.2.74) port 0 AF_INET
Recv   Send    Send
Socket Socket  Message  Elapsed
Size   Size    Size     Time     Throughput
bytes  bytes   bytes    secs.    10^6bits/sec
 87380  16384  16384    10.12      93.66</pre><p>It could be worse, I guess. The moral of the story? <code class="literal">xennet</code> introduces a noticeable but reasonable <a id="idx-CHP-10-0763" class="indexterm"/>overhead. Also, netperf can be a useful tool for discovering the actual bandwidth you've got available. In this case the machines are connected via a 100Mbit connection, and netperf lists an actual throughput of 93.66Mbits/second.</p></div><div class="sect2" title="Measuring Disk Performance with Bonnie++"><div class="titlepage"><div><div><h2 class="title"><a id="measuring_disk_performance_with_bonnie"/>Measuring Disk Performance with Bonnie++</h2></div></div></div><p>One of the major factors in a machine's overall performance is its disk subsystem. By exercising its hard drives, we can get a useful metric to compare Xen providers or Xen instances with, say, VMware guests.<a id="idx-CHP-10-0764" class="indexterm"/></p><p>We, like virtually everyone else on the planet, use <a id="idx-CHP-10-0765" class="indexterm"/>Bonnie++ to measure <a id="idx-CHP-10-0766" class="indexterm"/>disk performance. <a id="idx-CHP-10-0767" class="indexterm"/>Bonnie++ attempts to measure both random and sequential <a id="idx-CHP-10-0768" class="indexterm"/>disk performance and does a good job simulating real-world loads. This is especially important in the Xen context because of the degree to which domains are partitioned—although domains share resources, there's no way for them to coordinate resource use.</p><p>One illustration of this point is that if multiple domains are trying to access a platter simultaneously, what looks like sequential access from the viewpoint of one VM becomes random accesses to the disk. This makes things like seek time and the robustness of your tagged queuing system much more important. To test the effect of these optimizations on domU performance, you'll probably want a tool like Bonnie++.</p><p>The Bonnie++ author maintains a home page at <a class="ulink" href="http://www.coker.com.au/bonnie++/">http://www.coker.com.au/bonnie++/</a>. Download the source package, build it, and install it:</p><a id="I_programlisting10_d1e11256"/><pre class="programlisting"># wget http://www.coker.com.au/bonnie++/bonnie++-1.03c.tgz
# cd bonnie++-1.03c
# make
# make install</pre><p>At this point you can simply invoke Bonnie++ with a command such as:</p><a id="I_programlisting10_d1e11260"/><pre class="programlisting"># /usr/local/sbin/bonnie++</pre><p>This command will run some tests, printing status information as it goes along, and eventually generate output like this:</p><a id="I_programlisting10_d1e11264"/><pre class="programlisting">Version  1.03       ------Sequential Output------ --Sequential Input- --Random-
                    -Per Chr- --Block-- -Rewrite- -Per Chr- --Block-- --Seeks--
Machine        Size K/sec %CP K/sec %CP K/sec %CP K/sec %CP K/sec %CP  /sec %CP
alastor       2512M 20736  76 55093  14 21112   5 26385  87 55658   6 194.9   0
...........     ------Sequential Create------ --------Random Create--------
	     -Create-- --Read--- -Delete-- -Create-- --Read--- -Delete--
	     files  /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP  /sec %CP
	     256 35990  89 227885  85 16877  28 34146  84 334227 99  5716  10</pre><p>Note that some tests may simply output a row of pluses. This indicates that the machine finished them in less than 500 ms. Make the workload more difficult. For example, you might specify something like:</p><a id="I_programlisting10_d1e11269"/><pre class="programlisting"># /usr/local/sbin/bonnie++ -d . -s 2512 -n 256</pre><p>This specifies writing 2512MB files for I/O performance tests. (This is the default file size, which is twice the RAM size on this particular machine. This is important to ensure that we're not just exercising RAM rather than <a id="idx-CHP-10-0769" class="indexterm"/>disk.) It also tells Bonnie++ to create 256*1024 files in its file creation tests.</p><p>We also recommend reading Bonnie++'s online manual, which includes a fair amount of pithy benchmarking wisdom, detailing why the author chose to include the tests that he did, and what meanings the different numbers have.</p></div></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-10-FNOTE-1" href="#CHP-10-FNOTE-1" class="para">56</a>] </sup>The phrase "hard problem" is usually used as dry and bleak humor. Classic "hard problems" include natural language and strong AI. See also: "interesting."</p></div><div class="footnote"><p><sup>[<a id="ftn.CHP-10-FNOTE-2" href="#CHP-10-FNOTE-2" class="para">57</a>] </sup>See <a class="ulink" href="http://ols.108.redhat.com/2007/Reprints/ben-yehuda-Reprint.pdf">http://ols.108.redhat.com/2007/Reprints/ben-yehuda-Reprint.pdf</a>.</p></div></div></div>
<div class="sect1" title="Application Benchmarks"><div class="titlepage"><div><div><h1 class="title"><a id="application_benchmarks"/>Application Benchmarks</h1></div></div></div><p>Of course, the purpose of a server is to run applications—we're not really interested in how many times per second the VM can do absolutely nothing. For testing application performance, we use the applications that we're planning to put on the machine, and then throw load at them.</p><p>Since this is necessarily application-specific, we can't give you too many pointers on specifics. There are good test suites available for many popular libraries. For example, we've had customers benchmark their Xen instances with the popular web framework <a id="idx-CHP-10-0770" class="indexterm"/>Django.<sup>[<a id="CHP-10-FNOTE-3" href="#ftn.CHP-10-FNOTE-3" class="footnote">58</a>]</sup></p><div class="sect2" title="httperf: A Load Generator for HTTP Servers"><div class="titlepage"><div><div><h2 class="title"><a id="httperf_a_load_generator_for_http_server"/>httperf: A Load Generator for HTTP Servers</h2></div></div></div><p>Having tested the effectiveness of your domain's network interface, you may want to discover how well the domain performs when serving applications through that interface. Because of Xen's server-oriented heritage, one popular means of testing its performance in HTTP-based real-world applications is <code class="literal">httperf</code>. The tool generates HTTP requests and summarizes performance statistics. It supports HTTP/1.1 and SSL protocols and offers a variety of workload generators. You may find <code class="literal">httperf</code> useful if, for example, you're trying to figure out how many users your web server can handle before it goes casters-up.<a id="idx-CHP-10-0771" class="indexterm"/><a id="idx-CHP-10-0772" class="indexterm"/><a id="idx-CHP-10-0773" class="indexterm"/></p><p>First, install <code class="literal">httperf</code> on a machine other than the one you're testing—it can be another domU, but we usually prefer to install it on something completely separate. This "load" machine should also be as close to the target machine as possible—preferably connected to the same Ethernet switch.</p><p>You can get <code class="literal">httperf</code> through your distro's package-management mechanism or from <a class="ulink" href="http://www.hpl.hp.com/research/linux/httperf/">http://www.hpl.hp.com/research/linux/httperf/</a>.</p><p>If you've downloaded the source code, build it <a id="idx-CHP-10-0774" class="indexterm"/>using the standard method. <code class="literal">httperf</code> 's documentation recommends using a separate build directory rather than building directly in the source tree. Thus, from the <code class="literal">httperf</code> source directory:</p><a id="I_programlisting10_d1e11344"/><pre class="programlisting"># mkdir build
# cd build
# ../configure
# make
# make install</pre><p>Next, run appropriate tests. What we usually do is run <code class="literal">httperf</code> with a command similar to this:<a id="idx-CHP-10-0775" class="indexterm"/></p><a id="I_programlisting10_d1e11354"/><pre class="programlisting"># <a id="idx-CHP-10-0776" class="indexterm"/>httperf --server 192.168.1.80 --uri /index.html --num-conns 6000
--rate 1500</pre><p>In this case we're just demanding a static HTML page, so the request rate is obscenely high; usually we would use a much smaller number in tests of real-world database-backed websites.</p><p><code class="literal">httperf</code> will then give you some statistics. The important numbers, in our experience, are the connection rate, the request rate, and the reply rate. All of these should be close to the rate specified on the command line. If they start to decline from that number, that indicates that the server has reached its capacity.</p><p>However, <code class="literal">httperf</code> isn't limited to repeated requests for a single file. We prefer to use <code class="literal">httperf</code> in session mode by specifying the <code class="literal">--wsesslog</code> <a id="idx-CHP-10-0777" class="indexterm"/>workload generator. This gives a closer approximation to the actual load on the web server. You can create a session file from your web server logs with a bit of Perl, winding up with a simple formatted list of URLs:<a id="idx-CHP-10-0778" class="indexterm"/></p><a id="I_programlisting10_d1e11389"/><pre class="programlisting">/newsv3/
....../style/crimson.css
....../style/ash.css
....../style/azure.css
....../images/news.feeds.anime/sites/ann-xs.gif
....../images/news.feeds.anime/sites/annpr-xs.gif
....../images/news.feeds.anime/sites/aod-xs.gif
....../images/news.feeds.anime/sites/an-xs.gif
....../images/news.feeds.anime/header-lite.gif
/index.shtml
....../style/sable.css
....../images/banners/igloo.gif
....../images/temp_banner.gif
....../images/faye_header2.jpg
....../images/faye-birthday.jpg
....../images/giant_arrow.gif
....../images/faye_header.jpg
/news/
/events/
....../events/events.css
....../events/summergathering2007/coverimage.jpg

<em class="replaceable"><code>(and so forth.)</code></em></pre><p>This session file lists files for <code class="literal">httperf</code> to request, with indentations to define bursts; a group of lines that begin with whitespace is a burst. When run, <code class="literal">httperf</code> will request the first burst, wait a certain amount of time, then move to the next burst. Equipped with this session file, we can use <code class="literal">httperf</code> to simulate a user:</p><a id="I_programlisting10_d1e11404"/><pre class="programlisting"># httperf --hog --server 192.168.1.80 <a id="idx-CHP-10-0779" class="indexterm"/>--wsesslog=40,10,urls.txt --rate=1</pre><p>This will start 40 sessions at the rate of one per second. The new parameter, <code class="literal">--wsesslog</code>, takes the input of <em class="filename">urls.txt</em> and runs through it in bursts, pausing 10 seconds between bursts to simulate the user thinking.</p><p>Again, throw this at your server, increasing the rate until the server can't meet demand. When the server fails, congratulations! You've got a benchmark.</p></div><div class="sect2" title="Another Application Benchmark: POV-Ray"><div class="titlepage"><div><div><h2 class="title"><a id="another_application_benchmark_pov-ray"/>Another Application Benchmark: POV-Ray</h2></div></div></div><p>Of course, depending on your application, <code class="literal">httperf</code> may not be a suitable workload. Let's say that you've decided to use Xen to render scenes with popular open source raytracer <a id="idx-CHP-10-0780" class="indexterm"/>POV-Ray. (If nothing else, it's a good way to soak up spare CPU cycles.)<a id="idx-CHP-10-0781" class="indexterm"/></p><p>The POV-Ray benchmark is easy to run. Just give the <code class="literal">-benchmark</code> option on the command line:</p><a id="I_programlisting10_d1e11442"/><pre class="programlisting"># povray -benchmark</pre><p>This renders a standard scene and gives a large number of statistics, ending with an overall summary and rendering time. A domU with a 2.8 GHz Pentium 4 and 256MB of memory gave us the following output:</p><a id="I_programlisting10_d1e11446"/><pre class="programlisting">Smallest Alloc:                   9 bytes
Largest  Alloc:             1440008 bytes
Peak memory used:           5516100 bytes
Total Scene Processing Times
  Parse Time:    0 hours  0 minutes  2 seconds (2 seconds)
  Photon Time:   0 hours  0 minutes 53 seconds (53 seconds)
  Render Time:   0 hours 43 minutes 26 seconds (2606 seconds)
  Total Time:    0 hours 44 minutes 21 seconds (2661 seconds)</pre><p>Now you've got a single number that you can easily compare between various setups running POV-Ray, be they Xen instances, VMware boxes, or physical servers.</p></div><div class="sect2" title="Tuning Xen for Optimum Benchmarking"><div class="titlepage"><div><div><h2 class="title"><a id="tuning_xen_for_optimum_benchmarking"/>Tuning Xen for Optimum Benchmarking</h2></div></div></div><p>Most system administration work involves comparing results at the machine level—analyzing the performance of a Xen VM relative to another machine, virtual or not. However, with virtualization, there are some performance knobs that aren't obvious but can make a huge difference in the final benchmark results.<a id="idx-CHP-10-0782" class="indexterm"/><a id="I_indexterm10_d1e11460" class="indexterm"/><a id="I_indexterm10_d1e11463" class="indexterm"/></p><p>First, Xen allocates CPU dynamically and attempts to keep the CPU busy as much as possible. That is, if dom2 isn't using all <a id="idx-CHP-10-0783" class="indexterm"/>of its allocated CPU, dom3 can pick up the extra. Although this is usually a good thing, it can make CPU benchmark data misleading. While testing, you can avoid this problem by specifying the <code class="literal">cap</code> parameter to the scheduler. <a id="idx-CHP-10-0784" class="indexterm"/>For example, to ensure that domain ID 1 can get no more than 50 percent of one CPU:</p><a id="I_programlisting10_d1e11485"/><pre class="programlisting"># xm sched-credit -d 1 -c 50</pre><p>Second, guests in HVM mode absolutely must use paravirtualized drivers for acceptable performance. This point is driven home in a XenSource analysis of benchmark results published by VMware, in which XenSource points out that, in VMware's benchmarks, "XenSource's Xen Tools for Windows, which optimize the I/O path, were not installed. The VMware benchmarks should thus be disregarded in their entirety."</p><p>Also, shared resources (like disk I/O) are difficult to account, can interact with dom0 CPU demand, and can be affected by other domUs. For example, although paravirtualized Xen can deliver excellent network performance, it requires more CPU cycles to do so than a nonvirtualized machine. This may affect the capacity of your machine.</p><p>This is a difficult issue to address, and we can't really offer a magic bullet. One point to note is that the dom0 will likely use more CPU than an intuitive estimate would suggest; it's very important to weight the dom0's CPU allocation heavily, or perhaps even devote a core exclusively to the dom0 on boxes with four or more cores.</p><p>For benchmarking, we also recommend minimizing <a id="idx-CHP-10-0785" class="indexterm"/>error by benchmarking with a reasonably loaded machine. If you're expecting to run a dozen domUs, then they should all be performing some reasonable synthetic task while benchmarking to get an appreciation for the real-world performance of the VM.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-10-FNOTE-3" href="#CHP-10-FNOTE-3" class="para">58</a>] </sup><a class="ulink" href="http://journal.uggedal.com/vps-comparison-between-slicehost-and-prgmr">http://journal.uggedal.com/vps-comparison-between-slicehost-and-prgmr</a> uses Django among other tools.</p></div></div></div>
<div class="sect1" title="Profiling with Xen"><div class="titlepage"><div><div><h1 class="title"><a id="profiling_with_xen"/>Profiling with Xen</h1></div></div></div><p>Of course, there is one way of seeing shared resource use more precisely. We can <span class="emphasis"><em>profile</em></span> the VM as it runs our application workload to get a clear idea of what it's doing and—with a Xen-aware profiler—how other domains are interfering with us.</p><p>Profiling refers to the practice of examining a specific application to see what it spends time doing. In particular, it can tell you whether an app is CPU or I/O limited, whether particular functions are inefficient, or whether performance problems are occurring outside of the app entirely, perhaps in the kernel.</p><p>Here, we'll discuss a sample setup with Xen and OProfile, using the kernel compile as a standard workload (and one that most Xen admins are likely to be familiar with).</p><div class="sect2" title="Xenoprof"><div class="titlepage"><div><div><h2 class="title"><a id="xenoprof"/>Xenoprof</h2></div></div></div><p><a id="idx-CHP-10-0786" class="indexterm"/>OProfile is probably the most popular profiling package for Linux.<sup>[<a id="CHP-10-FNOTE-4" href="#ftn.CHP-10-FNOTE-4" class="footnote">59</a>]</sup> The kernel includes <a id="idx-CHP-10-0787" class="indexterm"/>OProfile support, and the user-space tools come with virtually every distro we know. If you have a performance problem with a particular program and want to see precisely what's causing it, <a id="idx-CHP-10-0788" class="indexterm"/>OProfile is the tool for the job.</p><p>OProfile works by incrementing a counter whenever the program being profiled performs a particular action. For example, it can keep count of the number of cache misses or the number of instructions executed. When the counter reaches a certain value, it instructs the OProfile daemon to sample the counter, using a non-maskable interrupt to ensure prompt handling of the sampling request.</p><p><a id="idx-CHP-10-0789" class="indexterm"/>Xenoprofile, or <a id="idx-CHP-10-0790" class="indexterm"/>Xenoprof, is a version of OProfile that has been extended to work as a system-wide profiling tool under Xen, using hypercalls to enable domains to access hardware performance counters. It supports analysis of complete Xen instances and accounts for time spent in the hypervisor or within another domU.</p></div><div class="sect2" title="Getting OProfile"><div class="titlepage"><div><div><h2 class="title"><a id="getting_oprofile"/>Getting OProfile</h2></div></div></div><p>As of recent versions, Xen includes support for OProfile versions up to 0.9.2 (0.9.3 will require you to apply a patch to the Xen kernel). For now, it would probably be best to use the packaged version to minimize the tedious effort of recompilation.</p><p>If you're using a recent version of Debian, Ubuntu, CentOS, or Red Hat, you're in luck; the version of OProfile that they ship is already set up to work with Xen. Other distro kernels, if they ship with Xen, will likely also incorporate OProfile's Xen support.</p><div class="sect3" title="Building OProfile"><div class="titlepage"><div><div><h3 class="title"><a id="building_oprofile"/>Building OProfile</h3></div></div></div><p>If you're not so lucky as to have Xen profiling support already, you'll have to download and build OProfile, for which we'll give very brief directions just for completeness.</p><p>The first thing to do is to download the OProfile source from <a class="ulink" href="http://oprofile.sourceforge.net/">http://oprofile.sourceforge.net/</a>. We used version 0.9.4.</p><p>First, untar OProfile, like so:</p><a id="I_programlisting10_d1e11568"/><pre class="programlisting"># wget http://prdownloads.sourceforge.net/oprofile/oprofile-0.9.4.tar.gz
# tar xzvf oprofile-0.9.4.tar.gz
# cd oprofile-0.9.4</pre><p>Then configure <a id="idx-CHP-10-0791" class="indexterm"/>and build <a id="idx-CHP-10-0792" class="indexterm"/>OProfile:</p><a id="I_programlisting10_d1e11584"/><pre class="programlisting"># ./configure --with-kernel-support
# make
# make install</pre><p>Finally, do a bit of Linux kernel configuration if your kernel isn't correctly configured already. (You can check by issuing <code class="literal">gzip -d -i /proc/config.gz | grep PROFILE</code>.) In our case that returns:</p><a id="I_programlisting10_d1e11591"/><pre class="programlisting">CONFIG_PROFILING=y
CONFIG_<a id="idx-CHP-10-0793" class="indexterm"/>OPROFILE=m</pre><div class="note" title="Note"><h3 class="title"><a id="note-44"/>Note</h3><p>/proc/config.gz <span class="emphasis"><em>is an optional feature that may not exist. If it doesn't, you'll have to find your configuration some other way. On Fedora 8, for example, you can check for profiling support by looking at the kernel config file shipped with the distro</em></span>:</p></div><a id="I_programlisting10_d1e11605"/><pre class="programlisting"># cat  /boot/config-2.6.23.1-42.fc8 | grep PROFILE</pre><p>If your kernel isn't set up for profiling, rebuild it with profiling support. Then install and boot from the new kernel (a step that we won't detail at length here).</p></div><div class="sect3" title="OProfile Quickstart"><div class="titlepage"><div><div><h3 class="title"><a id="oprofile_quickstart"/>OProfile Quickstart</h3></div></div></div><p>To make sure OProfile works, you can profile a standard workload in domain 0. (We chose the kernel compile because it's a familiar task to most sysadmins, although we're compiling it out of the Xen source tree.)</p><p>Begin by telling OProfile to clear its <a id="idx-CHP-10-0794" class="indexterm"/>sample buffers:</p><a id="I_programlisting10_d1e11623"/><pre class="programlisting"># opcontrol --reset</pre><p>Now configure OProfile.</p><a id="I_programlisting10_d1e11627"/><pre class="programlisting"># opcontrol --setup --vmlinux=/usr/lib/debug/lib/modules/vmlinux
--separate=library --event=CPU_CLK_UNHALTED:750000:0x1:1:1</pre><p>The first three arguments are the command (setup for profiling), kernel image, and an option to create separate output files for libraries used. The final switch, <code class="literal">event</code>, describes the event that we're instructing OProfile to monitor.</p><p>The precise event that you'll want to sample varies depending on your processor type (and on what you're trying to measure). For this run, to get an overall approximation of CPU usage, we used <code class="literal">CPU_CLK_UNHALTED</code> on an Intel Core 2 machine. On a Pentium 4, the equivalent measure would be <code class="literal">GLOBAL_POWER_EVENTS</code>. The remaining arguments indicate the size of the counter, the unit mask (in this case, 0x1), and that we want both the kernel and userspace code.</p><div class="sidebar"><a id="installing_an_uncompressed_kernel_on_red"/><p class="title">INSTALLING AN UNCOMPRESSED KERNEL ON RED HAT–DERIVED DISTROS</p><p>One issue that you may run into with <a id="idx-CHP-10-0795" class="indexterm"/>OProfile and kdump, as with any tool that digs into the kernel's innards, is that these tools expect to find an uncompressed kernel with debugging symbols for maximum benefit. This is simple to provide if you've built the kernel yourself, but with a distro kernel it can be more difficult.</p><p>Under <a id="idx-CHP-10-0796" class="indexterm"/>Red Hat and others, these kernels (and other software built for debugging) are in special <code class="literal">-debuginfo</code> RPM packages. These packages aren't in the standard <code class="literal">yum</code> repositories, but you can get them from Red Hat's FTP site. For Red Hat Enterprise Linux 5, for example, that'd be <a class="ulink" href="ftp://ftp.redhat.com/pub/redhat/linux/enterprise/5Server/en/os/i386/Debuginfo">ftp://ftp.redhat.com/pub/redhat/linux/enterprise/5Server/en/os/i386/Debuginfo</a>.</p><p>For the default kernel, you'll want the packages:</p><a id="I_programlisting10_d1e11669"/><pre class="programlisting">kernel-debuginfo-common-`uname -r`.`uname -m`.rpm
kernel-PAE-debuginfo-`uname -r`.`uname -m`.rpm</pre><p>Download these and install them using RPM.</p><a id="I_programlisting10_d1e11673"/><pre class="programlisting"># rpm -ivh *.rpm</pre></div><p>To start collecting samples, run:</p><a id="I_programlisting10_d1e11677"/><pre class="programlisting"># opcontrol --start</pre><p>Then run the experiment that you want to profile, in this case a kernel compile.</p><a id="I_programlisting10_d1e11682"/><pre class="programlisting"># /usr/bin/time -v make bzImage</pre><p>Then stop the profiler.</p><a id="I_programlisting10_d1e11686"/><pre class="programlisting"># opcontrol --shutdown</pre><p>Now that we have samples, we can extract meaningful and useful information from the mass of raw data via the standard postprofiling tools. The main analysis command is <code class="literal">opreport</code>. To get a basic overview of the processes that consumed CPU, we could run:</p><a id="I_programlisting10_d1e11693"/><pre class="programlisting"># opreport -t 2
CPU: Core 2, speed 2400.08 MHz (estimated)
Counted CPU_CLK_UNHALTED events (Clock cycles when not halted) with a unit mask
of 0x01 (Unhalted bus cycles) count 750000
CPU_CLK_UNHALT...|
  samples|      %|
------------------
   370812 90.0945 cc1
        CPU_CLK_UNHALT...|
          samples|      %|
        ------------------
           332713 89.7255 cc1
            37858 10.2095 libc-2.5.so
              241  0.0650 ld-2.5.so
    11364  2.7611 genksyms
        CPU_CLK_UNHALT...|
          samples|      %|
        ------------------
             8159 71.7969 genksyms
             3178 27.9655 libc-2.5.so
               27  0.2376 ld-2.5.so</pre><p>This tells us which processes accounted for CPU usage during the compile, with a threshold of 2 percent (indicated by the <code class="literal">-t 2</code> option.) This isn't terribly interesting, however. We can get more granularity using the <code class="literal">--symbols</code> option with <code class="literal">opreport</code>, which gives a best guess as to what functions accounted for the CPU usage. Try it.</p><p>You might be interested in other events, such as cache misses. To get a list of possible counters customized for your hardware, issue:</p><a id="I_programlisting10_d1e11708"/><pre class="programlisting"># ophelp</pre></div></div><div class="sect2" title="Profiling Multiple Domains in Concert"><div class="titlepage"><div><div><h2 class="title"><a id="profiling_multiple_domains_in_concert"/>Profiling Multiple Domains in Concert</h2></div></div></div><p>So far, all this has covered standard use of <a id="idx-CHP-10-0797" class="indexterm"/>OProfile, without touching on the Xen-specific features. But one of the most useful features of <a id="idx-CHP-10-0798" class="indexterm"/>OProfile, in the Xen context, is the ability to profile entire domains against each other, analyzing how different scheduling parameters, disk allocations, drivers, and code paths interact to affect performance.<a id="idx-CHP-10-0799" class="indexterm"/><a id="idx-CHP-10-0800" class="indexterm"/></p><p>When <a id="idx-CHP-10-0801" class="indexterm"/>profiling multiple domains, dom0 still coordinates the session. It's not currently possible to simply profile in a domU without dom0's involvement—domUs don't have direct access to the CPU performance counters.</p><div class="sect3" title="Active vs. Passive Profiling"><div class="titlepage"><div><div><h3 class="title"><a id="active_vs_passive_profiling"/>Active vs. Passive Profiling</h3></div></div></div><p>Xenoprofile supports both active and passive modes for domain profiling.<a id="idx-CHP-10-0802" class="indexterm"/></p><p>When profiling in passive mode, the results indicate which domain is running at sample time but don't delve more deeply into what's being executed. It's useful to get a quick look at which domains are using the system.</p><p>In active mode, each domU runs its own instance of OProfile, which samples events within its virtual machine. Active mode allows better granularity than passive mode, but is more inconvenient. Only paravirtualized domains can run in active mode.</p></div><div class="sect3" title="Active Profiling"><div class="titlepage"><div><div><h3 class="title"><a id="active_profiling"/>Active Profiling</h3></div></div></div><p>Active profiling is substantially more interesting. For this example, we'll use three domains: dom0, to control the profiler, and domUs 1 and 3 as active domains.<a id="I_indexterm10_d1e11764" class="indexterm"/></p><a id="I_programlisting10_d1e11769"/><pre class="programlisting">0 # opcontrol --reset
1 # opcontrol --reset
3 # opcontrol --reset</pre><p>First, set up the daemon in dom0 with some initial parameters:</p><a id="I_programlisting10_d1e11773"/><pre class="programlisting">0 # opcontrol --start-daemon --event=GLOBAL_POWER_EVENTS:1000000:1:1
   --xen=/boot/xen-syms-3.0-unstable
   --vmlinux=/boot/vmlinux-syms-2.6.18-xen0 --<a id="idx-CHP-10-0803" class="indexterm"/>active-domains=1,3</pre><p>This introduces the <code class="literal">--xen</code> option, which gives the path to the uncompressed Xen kernel image, and the <code class="literal">--active-domains</code> option, which lists the domains to profile <a id="idx-CHP-10-0804" class="indexterm"/>in active mode. The <code class="literal">:1 s</code> at the end of the event option tells OProfile to count events in both userspace and kernel space.</p><div class="note" title="Note"><h3 class="title"><a id="note-45"/>Note</h3><p><span class="emphasis"><em>Specify domains by numeric ID. OProfile won't interpret names</em></span>.</p></div><p>Next, start OProfile in the active domUs. The daemon must already be running in dom0, otherwise the domU won't have permission to access the performance counters.</p><a id="I_programlisting10_d1e11805"/><pre class="programlisting">1 # opcontrol --reset
1 # opcontrol --start</pre><p>Run the same commands in domain 3. Finally, begin sampling in domain 0:</p><a id="I_programlisting10_d1e11809"/><pre class="programlisting">0 # opcontrol --start</pre><p>Now we can run commands in the domains of interest. Let's continue to use the kernel compile as our test workload, but this time complicate matters by running a disk-intensive benchmark in another domain.</p><a id="I_programlisting10_d1e11814"/><pre class="programlisting">1 # time make bzImage
3 # time bonnie++</pre><p>When the kernel compile and Bonnie++ have finished, we stop OProfile:</p><a id="I_programlisting10_d1e11818"/><pre class="programlisting">0 # opcontrol --stop

0 # opcontrol --shutdown
1 # opcontrol --shutdown
3 # opcontrol --shutdown</pre><p>Now each domU will have its own set of samples, which we can view with <code class="literal">opreport</code>. Taken together, these reports form a complete picture of the various domains' activity. We might suggest playing with the CPU allocations and seeing how that influences OProfile's results.</p></div></div><div class="sect2" title="An OProfile Example"><div class="titlepage"><div><div><h2 class="title"><a id="an_oprofile_example"/>An OProfile Example</h2></div></div></div><p>Now let's try applying <a id="idx-CHP-10-0805" class="indexterm"/>OProfile to an actual problem. Here's the scenario: We've moved to a setup that uses <a id="idx-CHP-10-0806" class="indexterm"/>LVM mirroring on a pair of 1 TB SATA disks. The hardware is a quad-core Intel QX6600, with 8GB memory and an ICH7 SATA controller, <a id="idx-CHP-10-0807" class="indexterm"/>using the AHCI driver. We've devoted 512MB of memory to the dom0.</p><p>We noted that the performance of mirrored logical volumes accessed through <code class="literal">xenblk</code> was about one-tenth that of nonmirrored LVs, or of LVs mirrored with the <code class="literal">--corelog</code> option. Mirrored LVs with and without <code class="literal">–corelog</code> performed fine when accessed normally within the dom0, but performance dropped when accessed via <code class="literal">xm block-attach</code>. This was, to our minds, ridiculous.</p><p>First, we created two logical volumes in the volume group <span class="emphasis"><em>test</em></span>: one with mirroring and a mirror log, and one with the <code class="literal">--corelog</code> option.</p><a id="I_programlisting10_d1e11868"/><pre class="programlisting"># lvcreate -m 1 -L 2G -n test_mirror test
# lvcreate -m 1 --corelog -L 2G -n test_core test</pre><p>Then we made filesystems and mounted them:</p><a id="I_programlisting10_d1e11872"/><pre class="programlisting"># mke2fs -j /dev/test/test*
# mkdir -p /mnt/test/mirror
# mkdir -p /mnt/test/core
# mount /dev/test/test_mirror /mnt/test/mirror</pre><p>Next we started <a id="idx-CHP-10-0808" class="indexterm"/>OProfile, using the <code class="literal">--xen</code> option to give the path to our uncompessed Xen kernel image. After a few test runs profiling various events, it became clear that our problem related to excessive amounts of time spent waiting for I/O. Thus, we instruct the profiler to count <code class="literal">BUS_IO_WAIT</code> events, which indicate when the processor is stuck waiting for input:</p><a id="I_programlisting10_d1e11888"/><pre class="programlisting"># opcontrol --start --event=BUS_IO_WAIT:500:0xc0
--xen=/usr/lib/debug/boot/xen-syms-2.6.18-53.1.14.el5.debug
--vmlinux=/usr/lib/debug/lib/modules/2.6.18-53.1.14.el5xen/vmlinux
--separate=all</pre><p>Then we ran Bonnie++ on each device in sequence, stopping OProfile and saving the output each time.<a id="I_indexterm10_d1e11892" class="indexterm"/></p><a id="I_programlisting10_d1e11897"/><pre class="programlisting"># bonnie++ -d /mnt/test/mirror
# opcontrol --stop
# opcontrol --save=mirrorlog
# opcontrol --reset</pre><p>The LV with the corelog displayed negligible iowait, as expected. However, the other experienced quite a bit, as you can see in this output from our test of the LV in question:</p><a id="I_programlisting10_d1e11902"/><pre class="programlisting"># opreport -t 1 --symbols session:iowait_mirror
warning: /ahci could not be found.
CPU: Core 2, speed 2400.08 MHz (estimated)
Counted BUS_IO_WAIT events (IO requests waiting in the bus queue) with a unit mask of 0xc0 (All
cores) count 500
Processes with a thread ID of 0
Processes with a thread ID of 463
Processes with a thread ID of 14185
samples %       samples  %      samples %  app name                         symbol name
32      91.4286 15      93.7500 0      0  xen-syms-2.6.18-53.1.14.el5.debug pit_read_counter
1       2.8571  0       0       0      0  ahci                              (no symbols)
1       2.8571  0       0       0      0  vmlinux                           bio_put
1       2.8571  0       0       0      0  vmlinux                           hypercall_page</pre><p>Here we see that the Xen kernel is experiencing a large number of <code class="literal">BUS_IO_WAIT</code> events in the <code class="literal">pit_read_counter</code> function, suggesting that this function is probably our culprit. A bit of searching for that function name reveals that it's been taken out of recent versions of Xen, so we decide to take the easy way out and upgrade. Problem solved—but now we have some idea why.</p><p>Used properly, profiling can be an excellent way to track down performance bottlenecks. However, it's not any sort of magic bullet. The sheer amount of data that profiling generates can be seductive, and sorting through the profiler's output may take far more time than it's worth.</p></div><div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.CHP-10-FNOTE-4" href="#CHP-10-FNOTE-4" class="para">59</a>] </sup>Excluding top(1), of course.</p></div></div></div>
<div class="sect1" title="Conclusion"><div class="titlepage"><div><div><h1 class="title"><a id="conclusion"/>Conclusion</h1></div></div></div><p>So that's a sysadmin's primer on performance measurement with Xen. In this chapter, we've described tools to measure performance, ranging from the general to the specific, from the hardware focused to the application oriented. We've also briefly discussed the Xen-oriented features of OProfile, which aim to extend the profiler to multiple domUs and the hypervisor itself.</p></div></body></html>